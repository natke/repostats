[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1530488987",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1530488987",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1530488987,
        "node_id": "IC_kwDOCVq1mM5bOWib",
        "user": {
            "login": "wangyems",
            "id": 52801275,
            "node_id": "MDQ6VXNlcjUyODAxMjc1",
            "avatar_url": "https://avatars.githubusercontent.com/u/52801275?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wangyems",
            "html_url": "https://github.com/wangyems",
            "followers_url": "https://api.github.com/users/wangyems/followers",
            "following_url": "https://api.github.com/users/wangyems/following{/other_user}",
            "gists_url": "https://api.github.com/users/wangyems/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wangyems/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wangyems/subscriptions",
            "organizations_url": "https://api.github.com/users/wangyems/orgs",
            "repos_url": "https://api.github.com/users/wangyems/repos",
            "events_url": "https://api.github.com/users/wangyems/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wangyems/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-01T22:51:38Z",
        "updated_at": "2023-05-01T22:51:38Z",
        "author_association": "MEMBER",
        "body": "how about running FP32 model with CUDA EP?\r\nIf FP32 is good, then you can try mixed precision conversion by specifying op_block_list. [code example](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/onnx_model.py#L598)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1530488987/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1531991114",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1531991114",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1531991114,
        "node_id": "IC_kwDOCVq1mM5bUFRK",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-02T18:57:09Z",
        "updated_at": "2023-05-02T20:49:27Z",
        "author_association": "MEMBER",
        "body": "CPU will use fp32 to run the model so it is fine. It seems  SimplifiedLayerNormalization has issue in FP16 based on dumping node outputs. You can put it to op_block_list.\r\n```\r\nSimplifiedLayerNormalization node: SimplifiedLayerNormalization_token_210\r\nInput 0 Name: /model/block.6/layer.1/Add_output_0\r\n Shape: {1,256,512}\r\nOrtMemoryInfo:[name:Cuda id:0 OrtMemType:0 OrtAllocatorType:1 Device:[DeviceType:1 MemoryType:0 DeviceId:0]]\r\n-23.15625, 106.1875, -46.09375, ... , -136, -44.75, 5416\r\n-23.1875, 106.1875, -46.125, ... , -136, -44.8125, 5416\r\n-23.15625, 106.1875, -46.125, ... , -136, -44.75, 5416\r\n...\r\n-23.125, 106.1875, -46.125, ... , -136, -44.75, 5416\r\n-23.15625, 106.125, -46.125, ... , -136, -44.75, 5416\r\n-23.21875, 106.1875, -46.09375, ... , -136, -44.75, 5416\r\n\r\nInput 1 Name: model.block.7.layer.0.layer_norm.weight\r\n Shape: {512}\r\nOrtMemoryInfo:[name:Cuda id:0 OrtMemType:0 OrtAllocatorType:1 Device:[DeviceType:1 MemoryType:0 DeviceId:0]]\r\n0.22058105, 0.18444824, 0.1887207, ... , 0.17089844, 0.18896484, 0.098571777\r\n\r\nPlacement: CUDAExecutionProvider\r\n-----------\r\nOutput 0 Name: /model/block.7/layer.0/layer_norm/Mul_1_output_0\r\n Shape: {1,256,512}\r\nOrtMemoryInfo:[name:Cuda id:0 OrtMemType:0 OrtAllocatorType:1 Device:[DeviceType:1 MemoryType:0 DeviceId:0]]\r\n-0, 0, -0, ... , -0, -0, 0\r\n-0, 0, -0, ... , -0, -0, 0\r\n-0, 0, -0, ... , -0, -0, 0\r\n...\r\n-0, 0, -0, ... , -0, -0, 0\r\n-0, 0, -0, ... , -0, -0, 0\r\n-0, 0, -0, ... , -0, -0, 0\r\n\r\nMin=-0,Max=-0,Zero=131072\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1531991114/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1532101031",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1532101031",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1532101031,
        "node_id": "IC_kwDOCVq1mM5bUgGn",
        "user": {
            "login": "omera-deci",
            "id": 118735753,
            "node_id": "U_kgDOBxPDiQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/118735753?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/omera-deci",
            "html_url": "https://github.com/omera-deci",
            "followers_url": "https://api.github.com/users/omera-deci/followers",
            "following_url": "https://api.github.com/users/omera-deci/following{/other_user}",
            "gists_url": "https://api.github.com/users/omera-deci/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/omera-deci/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/omera-deci/subscriptions",
            "organizations_url": "https://api.github.com/users/omera-deci/orgs",
            "repos_url": "https://api.github.com/users/omera-deci/repos",
            "events_url": "https://api.github.com/users/omera-deci/events{/privacy}",
            "received_events_url": "https://api.github.com/users/omera-deci/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-02T20:24:13Z",
        "updated_at": "2023-05-02T20:26:18Z",
        "author_association": "NONE",
        "body": "> SimplifiedLayerNormalization\r\n\r\nIs this an actual onnx op? Or some cuda kernel that results from fusion? I can't find this op in my graph or in https://github.com/onnx/onnx/blob/main/docs/Operators.md.\r\n\r\nFollowing @wangyems 's advice, I was able to convert to fp16 and run inference with CUDA EP using the following op_block_list:\r\n```python\r\nFP16_BAD_OPS = [\r\n    \"Add\",\r\n    \"MatMul\",\r\n    \"Mul\",\r\n    \"Pow\",\r\n    \"ReduceMean\",\r\n    \"Sqrt\",\r\n]\r\n```\r\nRemoving any of these ops from the list results in a nan or all-zero output (uploaded a new model with these ops blocked to the google drive). However, I'm still getting all zeros from the TRT EP even with these ops blocked.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1532101031/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1532155845",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1532155845",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1532155845,
        "node_id": "IC_kwDOCVq1mM5bUtfF",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-02T21:09:43Z",
        "updated_at": "2023-05-02T21:23:58Z",
        "author_association": "MEMBER",
        "body": "The op is from fusion, You need run fusion before converting to fp16.\r\n\r\nBTW, we have scripts can help export T5 to fp16, or use in beam search: \r\nhttps://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/models/t5/convert_to_onnx.py\r\nhttps://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_generation.py\r\n\r\nFor example,\r\n```\r\npython -m onnxruntime.transformers.models.t5.convert_to_onnx -m t5-small -o -p fp16 --use_gpu --separate_encoder_and_decoder_init\r\n```\r\n\r\nThis is the op_block_list we used: https://github.com/microsoft/onnxruntime/blob/abdd4f518a144035fee3b369996d8416a024bdaa/onnxruntime/python/tools/transformers/models/t5/t5_helper.py#L153-L157",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1532155845/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1532451366",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1532451366",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1532451366,
        "node_id": "IC_kwDOCVq1mM5bV1om",
        "user": {
            "login": "omera-deci",
            "id": 118735753,
            "node_id": "U_kgDOBxPDiQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/118735753?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/omera-deci",
            "html_url": "https://github.com/omera-deci",
            "followers_url": "https://api.github.com/users/omera-deci/followers",
            "following_url": "https://api.github.com/users/omera-deci/following{/other_user}",
            "gists_url": "https://api.github.com/users/omera-deci/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/omera-deci/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/omera-deci/subscriptions",
            "organizations_url": "https://api.github.com/users/omera-deci/orgs",
            "repos_url": "https://api.github.com/users/omera-deci/repos",
            "events_url": "https://api.github.com/users/omera-deci/events{/privacy}",
            "received_events_url": "https://api.github.com/users/omera-deci/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-03T04:52:19Z",
        "updated_at": "2023-05-03T04:52:19Z",
        "author_association": "NONE",
        "body": "Thanks @tianleiwu ! Will definitely take a look. Do you have any clue about what might be wrong with the TRT EP?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1532451366/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1535231166",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1535231166",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1535231166,
        "node_id": "IC_kwDOCVq1mM5bgcS-",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-04T18:35:58Z",
        "updated_at": "2023-05-04T18:36:53Z",
        "author_association": "MEMBER",
        "body": "@omera-deci, For TRT, you need use FP32 raw onnx models. TRT will change it to fp16 internally.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1535231166/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1537331653",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1537331653",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1537331653,
        "node_id": "IC_kwDOCVq1mM5bodHF",
        "user": {
            "login": "omera-deci",
            "id": 118735753,
            "node_id": "U_kgDOBxPDiQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/118735753?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/omera-deci",
            "html_url": "https://github.com/omera-deci",
            "followers_url": "https://api.github.com/users/omera-deci/followers",
            "following_url": "https://api.github.com/users/omera-deci/following{/other_user}",
            "gists_url": "https://api.github.com/users/omera-deci/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/omera-deci/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/omera-deci/subscriptions",
            "organizations_url": "https://api.github.com/users/omera-deci/orgs",
            "repos_url": "https://api.github.com/users/omera-deci/repos",
            "events_url": "https://api.github.com/users/omera-deci/events{/privacy}",
            "received_events_url": "https://api.github.com/users/omera-deci/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-07T06:31:22Z",
        "updated_at": "2023-05-07T06:31:22Z",
        "author_association": "NONE",
        "body": "@tianleiwu I just tried to give the TRT EP the fp32 model. If I don't enable fp16 everything works smoothly, but once I enable fp16 the output is all zeros again. I've uploaded the fp32 model to the drive as well as a new script to reproduce. I guess some layers are overflowing in trt as well - anyway I can block their conversion the same way I did with onnx?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1537331653/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1537514865",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15752#issuecomment-1537514865",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15752",
        "id": 1537514865,
        "node_id": "IC_kwDOCVq1mM5bpJ1x",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-07T18:44:55Z",
        "updated_at": "2023-05-07T18:59:58Z",
        "author_association": "MEMBER",
        "body": "@omera-deci, you can follow https://github.com/NVIDIA/TensorRT/blob/release/8.6/demo/HuggingFace/T5 to export onnx for T5 and run it in TRT EP. I did not see special setting so export onnx might be the key. You can run those scripts and get the onnx models to run in TRT EP.\r\n\r\nYou will need build from source to support TRT 8.6, and use some new features (like trt_layer_norm_fp32_fallback and explicit input profiles). See the following doc for detail:\r\nhttps://github.com/microsoft/onnxruntime/blob/fd080caf62db1b41463955286c49d6a582c6a45a/docs/execution-providers/TensorRT-ExecutionProvider.md\r\n@chilo-ms for comments of fp16 in TRT EP",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1537514865/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]