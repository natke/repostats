[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241572013",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12905#issuecomment-1241572013",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12905",
        "id": 1241572013,
        "node_id": "IC_kwDOCVq1mM5KAOKt",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-09T06:52:45Z",
        "updated_at": "2022-09-09T06:52:45Z",
        "author_association": "MEMBER",
        "body": "Not sure what you mean by 'ukernel'. Can you elaborate or maybe provide a specific example?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241572013/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241669892",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12905#issuecomment-1241669892",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12905",
        "id": 1241669892,
        "node_id": "IC_kwDOCVq1mM5KAmEE",
        "user": {
            "login": "JasonMaojinsong",
            "id": 95834241,
            "node_id": "U_kgDOBbZQgQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/95834241?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JasonMaojinsong",
            "html_url": "https://github.com/JasonMaojinsong",
            "followers_url": "https://api.github.com/users/JasonMaojinsong/followers",
            "following_url": "https://api.github.com/users/JasonMaojinsong/following{/other_user}",
            "gists_url": "https://api.github.com/users/JasonMaojinsong/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JasonMaojinsong/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JasonMaojinsong/subscriptions",
            "organizations_url": "https://api.github.com/users/JasonMaojinsong/orgs",
            "repos_url": "https://api.github.com/users/JasonMaojinsong/repos",
            "events_url": "https://api.github.com/users/JasonMaojinsong/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JasonMaojinsong/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-09T08:30:34Z",
        "updated_at": "2022-09-09T08:30:34Z",
        "author_association": "NONE",
        "body": "@skottmckay ,  sorry, i didn't describe it clearly,  \r\nfor example, I am running resnet50 which mapping one conv operation (qlinearconv) to micro kernel of XNNPACK : \r\nxnn_qs8_igemm_minmax_fp32_ukernel_4x16c8__avx512skx (),  \r\n\r\nlooks the CreateXnnpackKernel() on conv.cc of xnnpack EP setup this \"ukernel\",  which calls xnn_create_convolution2d_nhwc_qs8() of XNNPACK library internally.   \r\n\r\nas there are lots of kernels in xnnpack qs8-igemm,  https://github.com/google/XNNPACK/tree/master/src/qs8-igemm/gen\r\nxnn_qs8_igemm_minmax_fp32_ukernel_4x16c8__avx512skx () is determined by XNNPACK internally or any options in onnxruntime XNNPACK EP for users to change?\r\n\r\nthanks\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241669892/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241692878",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12905#issuecomment-1241692878",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12905",
        "id": 1241692878,
        "node_id": "IC_kwDOCVq1mM5KArrO",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-09T08:51:26Z",
        "updated_at": "2022-09-09T08:51:26Z",
        "author_association": "MEMBER",
        "body": "I would expect a higher-level function like xnn_create_convolution2d_nhwc_qs8 will utilize the appropriate low-level kernels that plug in device and usage specific things like an avx512 specific kernel. It most likely isn't practical for ORT to try and directly use the low-level device specific kernels. \r\n\r\nI think that equates to ORT only calling functions in https://github.com/google/XNNPACK/blob/master/include/xnnpack.h.\r\n\r\nDo you have a production use-case involving a specific device where the model has operators that would benefit from adding them to the xnnpack EP? That would most likely involve any model running on 32-bit ARM or an fp32 model running on 64-bit ARM. For other devices the internal ORT MLAS kernels would provide similar capabilities without having to use xnnpack.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241692878/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241740857",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12905#issuecomment-1241740857",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12905",
        "id": 1241740857,
        "node_id": "IC_kwDOCVq1mM5KA3Y5",
        "user": {
            "login": "JasonMaojinsong",
            "id": 95834241,
            "node_id": "U_kgDOBbZQgQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/95834241?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JasonMaojinsong",
            "html_url": "https://github.com/JasonMaojinsong",
            "followers_url": "https://api.github.com/users/JasonMaojinsong/followers",
            "following_url": "https://api.github.com/users/JasonMaojinsong/following{/other_user}",
            "gists_url": "https://api.github.com/users/JasonMaojinsong/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JasonMaojinsong/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JasonMaojinsong/subscriptions",
            "organizations_url": "https://api.github.com/users/JasonMaojinsong/orgs",
            "repos_url": "https://api.github.com/users/JasonMaojinsong/repos",
            "events_url": "https://api.github.com/users/JasonMaojinsong/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JasonMaojinsong/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-09T09:35:05Z",
        "updated_at": "2022-09-09T09:35:05Z",
        "author_association": "NONE",
        "body": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n> I would expect a higher-level function like xnn_create_convolution2d_nhwc_qs8 will utilize the appropriate low-level kernels that plug in device and usage specific things like an avx512 specific kernel. It most likely isn't practical for ORT to try and directly use the low-level device specific kernels.\r\n> \r\n> I think that equates to ORT only calling functions in https://github.com/google/XNNPACK/blob/master/include/xnnpack.h.\r\n> \r\nlooks it's determined by XNNPACK internally.  \r\n\r\n> Do you have a production use-case involving a specific device where the model has operators that would benefit from adding them to the xnnpack EP? That would most likely involve any model running on 32-bit ARM or an fp32 model running on 64-bit ARM. For other devices the internal ORT MLAS kernels would provide similar capabilities without having to use xnnpack.\r\n\r\nnot any specific device requirement,  I am just curious about the requantization flow between different method,  Google involves that they use XNNPACK to replace QNNPACK with higher precision for fixed point inference,  \r\nI compared the final output confidence/score of resnet50 for one input image between CPUexecution_provider and CPU+XNNPACK execution provider with int8 quantized QDQ model.   which shows that XNNPACK can provide similar results compared to CPU execution provider(MLAS).  I am not sure if MLAS implement QDQgroup in fixedpoint inference or float point inference.\r\n\r\nthanks\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241740857/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]