[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1287224830",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13397#issuecomment-1287224830",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13397",
        "id": 1287224830,
        "node_id": "IC_kwDOCVq1mM5MuX3-",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-21T17:03:57Z",
        "updated_at": "2022-10-21T17:06:04Z",
        "author_association": "MEMBER",
        "body": "@skyline75489, Thanks for the feedback. I can reproduce the issue. We'll update the benchmark to only measure the encoder of bart.\r\n\r\nFor text generation, it is better to use end to end performance test. The decoding latency depending on multiple factors: \r\nbatch size, context sequence length, number of generated tokens, beam search/ greedy search/ beam sampling, early stop etc.\r\nI suggest to modify the following script if you need consider all these factors:\r\nhttps://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers/models/bart\r\nThe script exports model to onnx, and runs some example text generation. Only need small change to measure latency.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1287224830/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]