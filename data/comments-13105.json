[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1258651632",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13105#issuecomment-1258651632",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13105",
        "id": 1258651632,
        "node_id": "IC_kwDOCVq1mM5LBX_w",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-26T21:26:19Z",
        "updated_at": "2022-09-28T00:30:37Z",
        "author_association": "MEMBER",
        "body": "@dyerdave-gh, \r\nWe have a benchmark script for GPT-2 model without past state. It can run like the following:\r\n```\r\npython -m onnxruntime.transformers.benchmark -m gpt2 --model_class AutoModelForCausalLM -e onnxruntime -g -p fp32 -o by_script -i 1 -t 1000 -b 1 -s 32\r\n\r\npython -m onnxruntime.transformers.benchmark -m gpt2 --model_class AutoModelForCausalLM -e torch -g -p fp32 -t 1000 -b 1 -s 32\r\n```\r\nIn T4 GPU, I saw average latency is 4ms for ORT and 7ms for PyTorch in this test.\r\n\r\nWe also have a benchmark script for GPT-2 model with past state: https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/models/gpt2/benchmark_gpt2.py\r\n\r\nCurrent optimization is targeting for GPT-2 model with past state (past_key_value). If you use past state, you will need use it with IO Binding, and try convert model to mixed precision (and keep past state in float16) to improve performance without significant loss in accuracy.\r\n\r\nYou might try BeamSearch or GreedySearch integration with GPT-2:\r\nhttps://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_generation.py\r\nIt is significantly (2X ~ 4X) faster than PyTorch in some of our test settings.\r\n\r\nExample to compare ORT and PyTorch performance:\r\n`\r\npython convert_generation.py -m gpt2 --num_beams 4 --max_length 30 --use_gpu --torch_performance --output gpt_beamsearch.onnx --disable_parity\r\n`\r\nOR\r\n`\r\npython convert_generation.py -m gpt2 --num_beams 4 --max_length 30 --use_gpu --torch_performance --output gpt_beamsearch_fp16.onnx --disable_parity -p fp16\r\n`\r\n\r\nYou might need change a line `bad_words_ids=[]` to `bad_words_ids=None` if you use latest transformers package.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1258651632/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1271886559",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13105#issuecomment-1271886559",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13105",
        "id": 1271886559,
        "node_id": "IC_kwDOCVq1mM5Lz3Lf",
        "user": {
            "login": "dyerdave-gh",
            "id": 93223986,
            "node_id": "U_kgDOBY58Mg",
            "avatar_url": "https://avatars.githubusercontent.com/u/93223986?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dyerdave-gh",
            "html_url": "https://github.com/dyerdave-gh",
            "followers_url": "https://api.github.com/users/dyerdave-gh/followers",
            "following_url": "https://api.github.com/users/dyerdave-gh/following{/other_user}",
            "gists_url": "https://api.github.com/users/dyerdave-gh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dyerdave-gh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dyerdave-gh/subscriptions",
            "organizations_url": "https://api.github.com/users/dyerdave-gh/orgs",
            "repos_url": "https://api.github.com/users/dyerdave-gh/repos",
            "events_url": "https://api.github.com/users/dyerdave-gh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dyerdave-gh/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-07T18:00:13Z",
        "updated_at": "2022-10-07T18:00:13Z",
        "author_association": "NONE",
        "body": "Hi @tianleiwu thank you so much for the assistance. We are seeing similar, faster, results to what you explained above, using IO Binding.  However, I noticed that the benchmark doesn't include converting the `ort_results` (of dtype ortvalue) to a numpy array.  We need faster inference, including the final step, which actually takes about 60% of the total time.  Is there any faster way than `.numpy()` to get the data out of ortvalue dtypes to something we can decode and use?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1271886559/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1279794818",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13105#issuecomment-1279794818",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13105",
        "id": 1279794818,
        "node_id": "IC_kwDOCVq1mM5MSB6C",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-15T17:57:26Z",
        "updated_at": "2022-10-15T17:57:26Z",
        "author_association": "MEMBER",
        "body": "@dyerdave-gh, \r\nOrtValue is a block of memory. If you use C++, you can directly read the values. \r\n\r\nIf the time takes 60%, is it possible to move some processing (like beam search etc) to ORT, so that only a small tensor (like word IDs)  is output.\r\n\r\nFor python API, you can bind the output to PyTorch tensor, then use it for post-processing (like convert word ID to string etc). See the last example of IO Binding in https://onnxruntime.ai/docs/api/python/api_summary.html.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1279794818/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]