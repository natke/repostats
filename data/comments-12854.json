[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237050937",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1237050937",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1237050937,
        "node_id": "IC_kwDOCVq1mM5Ju-Y5",
        "user": {
            "login": "xadupre",
            "id": 22452781,
            "node_id": "MDQ6VXNlcjIyNDUyNzgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/22452781?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xadupre",
            "html_url": "https://github.com/xadupre",
            "followers_url": "https://api.github.com/users/xadupre/followers",
            "following_url": "https://api.github.com/users/xadupre/following{/other_user}",
            "gists_url": "https://api.github.com/users/xadupre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xadupre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xadupre/subscriptions",
            "organizations_url": "https://api.github.com/users/xadupre/orgs",
            "repos_url": "https://api.github.com/users/xadupre/repos",
            "events_url": "https://api.github.com/users/xadupre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xadupre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-05T13:41:08Z",
        "updated_at": "2022-09-05T13:41:08Z",
        "author_association": "MEMBER",
        "body": "I was able to replicate your issue with this model [vocoder.onnx](https://huggingface.co/npc-engine/exported-flowtron-waveglow-librispeech-tts/blob/main/vocoder.onnx) and the following code:\r\n\r\n```python\r\nimport numpy\r\nimport pandas\r\nimport matplotlib.pyplot as plt\r\nfrom onnxruntime import InferenceSession\r\nfrom mlprodict.onnxrt import OnnxInference\r\n\r\ndef profile_plot(df, size=(12, 4)):\r\n    gr_dur = df[['dur', \"args_op_name\"]].groupby(\"args_op_name\").sum().sort_values('dur')\r\n    gr_n = df[['dur', \"args_op_name\"]].groupby(\"args_op_name\").count().sort_values('dur')\r\n    gr_n = gr_n.loc[gr_dur.index, :]\r\n\r\n    fig, ax = plt.subplots(1, 2, figsize=size)\r\n    gr_dur.plot.barh(ax=ax[0])\r\n    gr_n.plot.barh(ax=ax[1])\r\n    ax[0].set_title(\"duration\")\r\n    ax[1].set_title(\"n occurences\");\r\n    return df, ax\r\n\r\ndef profile(filename):\r\n    x = numpy.random.random([1, 80, 10]).astype(numpy.float32)\r\n    oinf = OnnxInference(filename, runtime=\"onnxruntime1\",\r\n                         runtime_options={\"enable_profiling\": True})\r\n    for i in range(0, 10):\r\n        oinf.run({\"mels\": x})\r\n    df = oinf.get_profiling(as_df=True)\r\n    return df\r\n\r\ndf1 = profile(\"vocoder.onnx\")\r\ndf2 = profile(\"vocoder_uint8.onnx\")\r\ndf2[\"args_op_name\"] = df2[\"args_op_name\"] + \"-Q\"\r\ndf = pandas.concat([df1, df2], axis=0)\r\nprofile_plot(df, size=(14, 8));\r\n```\r\n\r\nIt gives the following results:\r\n\r\n![image](https://user-images.githubusercontent.com/22452781/188454034-fefc18c3-3c2c-4f3d-a260-0971f4090409.png)\r\n\r\nConvInteger is slow than Conv on this particular example. I compare the input sizes. First model:\r\n\r\n| shape                     |              dur |\r\n|:--------------------------|-----------------:|\r\n| 1x256x320@512x256x3@512   |      3.05178e+06 |\r\n| 1x640x320@4096x640x1@4096 |      2.34432e+06 |\r\n| 1x256x320@512x256x1@512   | 933696           |\r\n| 1x256x320@256x256x1@256   |  69256           |\r\n| 1x2x320@256x2x1@256       |   5858           |\r\n\r\nSecond model:\r\n\r\n| shape                |              dur |\r\n|:---------------------|-----------------:|\r\n| 1x256x320@512x256x3  |      6.29231e+06 |\r\n| 1x640x320@4096x640x1 |      1.88803e+06 |\r\n| 1x256x320@512x256x1  | 750161           |\r\n| 1x256x320@256x256x1  |  53866           |\r\n| 1x2x320@256x2x1      |   4132           |\r\n\r\nIt needs more investigation.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237050937/reactions",
            "total_count": 4,
            "+1": 4,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237075096",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1237075096",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1237075096,
        "node_id": "IC_kwDOCVq1mM5JvESY",
        "user": {
            "login": "yszhou2019",
            "id": 59953951,
            "node_id": "MDQ6VXNlcjU5OTUzOTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/59953951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yszhou2019",
            "html_url": "https://github.com/yszhou2019",
            "followers_url": "https://api.github.com/users/yszhou2019/followers",
            "following_url": "https://api.github.com/users/yszhou2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yszhou2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yszhou2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yszhou2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yszhou2019/orgs",
            "repos_url": "https://api.github.com/users/yszhou2019/repos",
            "events_url": "https://api.github.com/users/yszhou2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yszhou2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-05T13:56:02Z",
        "updated_at": "2022-09-05T13:56:02Z",
        "author_association": "NONE",
        "body": "Thank you, xadupre.\r\nI will profile the two TTS models tomorrow and give out the exact profiler outputs. (Although I think few efforts can be made to accelerate the inference speed based on the current CPU.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237075096/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237515063",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1237515063",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1237515063,
        "node_id": "IC_kwDOCVq1mM5Jwvs3",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-05T23:05:37Z",
        "updated_at": "2022-09-05T23:05:37Z",
        "author_association": "MEMBER",
        "body": "@yufenglee Is there a known reason why the ConvInteger nodes in a dynamically quantized model are taking longer than Conv Nodes in the original fp32 model? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237515063/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237850871",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1237850871",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1237850871,
        "node_id": "IC_kwDOCVq1mM5JyBr3",
        "user": {
            "login": "yszhou2019",
            "id": 59953951,
            "node_id": "MDQ6VXNlcjU5OTUzOTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/59953951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yszhou2019",
            "html_url": "https://github.com/yszhou2019",
            "followers_url": "https://api.github.com/users/yszhou2019/followers",
            "following_url": "https://api.github.com/users/yszhou2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yszhou2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yszhou2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yszhou2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yszhou2019/orgs",
            "repos_url": "https://api.github.com/users/yszhou2019/repos",
            "events_url": "https://api.github.com/users/yszhou2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yszhou2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-06T08:48:55Z",
        "updated_at": "2022-09-06T08:48:55Z",
        "author_association": "NONE",
        "body": "The profile result shows the ConvInteger nodes cost a lot of time.\r\n![image](https://user-images.githubusercontent.com/59953951/188590933-a89a4851-f3f8-4528-bbc9-0784e30c231d.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1237850871/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1238158471",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1238158471",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1238158471,
        "node_id": "IC_kwDOCVq1mM5JzMyH",
        "user": {
            "login": "xadupre",
            "id": 22452781,
            "node_id": "MDQ6VXNlcjIyNDUyNzgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/22452781?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xadupre",
            "html_url": "https://github.com/xadupre",
            "followers_url": "https://api.github.com/users/xadupre/followers",
            "following_url": "https://api.github.com/users/xadupre/following{/other_user}",
            "gists_url": "https://api.github.com/users/xadupre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xadupre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xadupre/subscriptions",
            "organizations_url": "https://api.github.com/users/xadupre/orgs",
            "repos_url": "https://api.github.com/users/xadupre/repos",
            "events_url": "https://api.github.com/users/xadupre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xadupre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-06T13:33:06Z",
        "updated_at": "2022-09-06T13:33:06Z",
        "author_association": "MEMBER",
        "body": "I looked into the code. ConvInteger is using im2col + qgemm. Conv<float> (and only float) has a specific implementation. I'm wondering if the quantized version is faster in some cases or always slower.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1238158471/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1238713414",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1238713414",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1238713414,
        "node_id": "IC_kwDOCVq1mM5J1URG",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-06T22:24:58Z",
        "updated_at": "2022-09-06T22:24:58Z",
        "author_association": "MEMBER",
        "body": "It is recommended to use static quantization for CNN based models and we don't have special performance optimization on ConvInteger yet. Please refer to the doc here: https://onnxruntime.ai/docs/performance/quantization.html#method-selection.\r\n\r\n@jchen351 is working on consolidating ConvInteger and QLinearConv. Once it is done, perf of ConvInteger will be improved. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1238713414/reactions",
            "total_count": 3,
            "+1": 3,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1238857094",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1238857094",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1238857094,
        "node_id": "IC_kwDOCVq1mM5J13WG",
        "user": {
            "login": "yszhou2019",
            "id": 59953951,
            "node_id": "MDQ6VXNlcjU5OTUzOTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/59953951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yszhou2019",
            "html_url": "https://github.com/yszhou2019",
            "followers_url": "https://api.github.com/users/yszhou2019/followers",
            "following_url": "https://api.github.com/users/yszhou2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yszhou2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yszhou2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yszhou2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yszhou2019/orgs",
            "repos_url": "https://api.github.com/users/yszhou2019/repos",
            "events_url": "https://api.github.com/users/yszhou2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yszhou2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-07T03:11:25Z",
        "updated_at": "2022-09-07T03:11:25Z",
        "author_association": "NONE",
        "body": "@yufenglee thanks~ I'll try the static quantize method. Hope it works. I will give out the static quant result later.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1238857094/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247609396",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1247609396",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1247609396,
        "node_id": "IC_kwDOCVq1mM5KXQI0",
        "user": {
            "login": "yszhou2019",
            "id": 59953951,
            "node_id": "MDQ6VXNlcjU5OTUzOTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/59953951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yszhou2019",
            "html_url": "https://github.com/yszhou2019",
            "followers_url": "https://api.github.com/users/yszhou2019/followers",
            "following_url": "https://api.github.com/users/yszhou2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yszhou2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yszhou2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yszhou2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yszhou2019/orgs",
            "repos_url": "https://api.github.com/users/yszhou2019/repos",
            "events_url": "https://api.github.com/users/yszhou2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yszhou2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-15T05:44:25Z",
        "updated_at": "2022-09-15T05:44:25Z",
        "author_association": "NONE",
        "body": "> It is recommended to use static quantization for CNN based models and we don't have special performance optimization on ConvInteger yet. Please refer to the doc here: https://onnxruntime.ai/docs/performance/quantization.html#method-selection.\r\n> \r\n> @jchen351 is working on consolidating ConvInteger and QLinearConv. Once it is done, perf of ConvInteger will be improved.\r\n\r\nThank you! I tried it today. The static_quantized model infer 25% faster than before. Although the quality of audio generated deduced heavily, at least it proved Integer quantization not fit for vocoder. Thank you anyway.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247609396/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247618658",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1247618658",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1247618658,
        "node_id": "IC_kwDOCVq1mM5KXSZi",
        "user": {
            "login": "yszhou2019",
            "id": 59953951,
            "node_id": "MDQ6VXNlcjU5OTUzOTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/59953951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yszhou2019",
            "html_url": "https://github.com/yszhou2019",
            "followers_url": "https://api.github.com/users/yszhou2019/followers",
            "following_url": "https://api.github.com/users/yszhou2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yszhou2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yszhou2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yszhou2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yszhou2019/orgs",
            "repos_url": "https://api.github.com/users/yszhou2019/repos",
            "events_url": "https://api.github.com/users/yszhou2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yszhou2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-15T06:00:22Z",
        "updated_at": "2022-09-15T06:00:22Z",
        "author_association": "NONE",
        "body": "Here is the profiler result of the static quantized model. \r\n![profile_uint8](https://user-images.githubusercontent.com/59953951/190324544-6bbd7b14-21a0-4c88-bdc1-769ce8a4d10d.png)\r\nMaybe I should try half-precision inference. The half-precision model is thought to be faster than fp32 if supported by CPU(For my case, I guess not. It seems only few products of Intel support the bf16 according to the [link](https://en.wikichip.org/wiki/brain_floating-point_format). Unfortunately, when I type \"lscpu | grep bf\", no result is showed.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247618658/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1634479980",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1634479980",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1634479980,
        "node_id": "IC_kwDOCVq1mM5hbC9s",
        "user": {
            "login": "mllopartbsc",
            "id": 133785234,
            "node_id": "U_kgDOB_lmkg",
            "avatar_url": "https://avatars.githubusercontent.com/u/133785234?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mllopartbsc",
            "html_url": "https://github.com/mllopartbsc",
            "followers_url": "https://api.github.com/users/mllopartbsc/followers",
            "following_url": "https://api.github.com/users/mllopartbsc/following{/other_user}",
            "gists_url": "https://api.github.com/users/mllopartbsc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mllopartbsc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mllopartbsc/subscriptions",
            "organizations_url": "https://api.github.com/users/mllopartbsc/orgs",
            "repos_url": "https://api.github.com/users/mllopartbsc/repos",
            "events_url": "https://api.github.com/users/mllopartbsc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mllopartbsc/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-13T15:43:50Z",
        "updated_at": "2023-07-13T15:43:50Z",
        "author_association": "NONE",
        "body": "> Here is the profiler result of the static quantized model. ![profile_uint8](https://user-images.githubusercontent.com/59953951/190324544-6bbd7b14-21a0-4c88-bdc1-769ce8a4d10d.png) Maybe I should try half-precision inference. The half-precision model is thought to be faster than fp32 if supported by CPU(For my case, I guess not. It seems only few products of Intel support the bf16 according to the [link](https://en.wikichip.org/wiki/brain_floating-point_format). Unfortunately, when I type \"lscpu | grep bf\", no result is showed.\r\n\r\nHi, could you please provide the code you used to perform this static quantization?\r\n Kind Regards",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1634479980/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1643226610",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1643226610",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1643226610,
        "node_id": "IC_kwDOCVq1mM5h8aXy",
        "user": {
            "login": "yszhou2019",
            "id": 59953951,
            "node_id": "MDQ6VXNlcjU5OTUzOTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/59953951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yszhou2019",
            "html_url": "https://github.com/yszhou2019",
            "followers_url": "https://api.github.com/users/yszhou2019/followers",
            "following_url": "https://api.github.com/users/yszhou2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yszhou2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yszhou2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yszhou2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yszhou2019/orgs",
            "repos_url": "https://api.github.com/users/yszhou2019/repos",
            "events_url": "https://api.github.com/users/yszhou2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yszhou2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-20T05:22:21Z",
        "updated_at": "2023-07-20T05:22:21Z",
        "author_association": "NONE",
        "body": "> > Here is the profiler result of the static quantized model. ![profile_uint8](https://user-images.githubusercontent.com/59953951/190324544-6bbd7b14-21a0-4c88-bdc1-769ce8a4d10d.png) Maybe I should try half-precision inference. The half-precision model is thought to be faster than fp32 if supported by CPU(For my case, I guess not. It seems only few products of Intel support the bf16 according to the [link](https://en.wikichip.org/wiki/brain_floating-point_format). Unfortunately, when I type \"lscpu | grep bf\", no result is showed.\r\n> \r\n> Hi, could you please provide the code you used to perform this static quantization? Kind Regards\r\n\r\nHi @mllopartbsc,  here it is.\r\n```python\r\nimport numpy\r\nimport pandas\r\nimport matplotlib.pyplot as plt\r\nfrom onnxruntime import InferenceSession\r\nfrom mlprodict.onnxrt import OnnxInference\r\n\r\ndef profile_plot(df, name, size=(12, 4)):\r\n    gr_dur = df[['dur', \"args_op_name\"]].groupby(\"args_op_name\").sum().sort_values('dur')\r\n    gr_n = df[['dur', \"args_op_name\"]].groupby(\"args_op_name\").count().sort_values('dur')\r\n    gr_n = gr_n.loc[gr_dur.index, :]\r\n\r\n    fig, ax = plt.subplots(1, 2, figsize=size)\r\n    gr_dur.plot.barh(ax=ax[0])\r\n    gr_n.plot.barh(ax=ax[1])\r\n    ax[0].set_title(\"duration\")\r\n    ax[1].set_title(\"n occurences\")\r\n    fig.savefig(f\"{name}_profile.png\")\r\n    return df, ax\r\n\r\ndef profile(filename):\r\n    x = numpy.random.random([1, 80, 80]).astype(numpy.float32)\r\n    oinf = OnnxInference(filename, runtime=\"onnxruntime1\",\r\n                         runtime_options={\"enable_profiling\": True})\r\n    for i in range(0, 10):\r\n        oinf.run({\"mel\": x})\r\n    df = oinf.get_profiling(as_df=True)\r\n    return df\r\n\r\ndf1 = profile(\"/path/to/model.onnx\")\r\n\r\n# quint8\r\ndf2 = profile(\"/path/to/model_uint8.onnx\")\r\ndf2[\"args_op_name\"] = df2[\"args_op_name\"] + \"-Quint8\"\r\ndf = pandas.concat([df1, df2], axis=0)\r\nprofile_plot(df, \"quint8\", size=(14, 8))\r\n\r\n# # int8\r\n# df2 = profile(\"/path/to/model_int8.onnx\")\r\n# df2[\"args_op_name\"] = df2[\"args_op_name\"] + \"-Qint8\"\r\n# df = pandas.concat([df1, df2], axis=0)\r\n# profile_plot(df, \"qint8\", size=(14, 8))\r\n\r\n# fp16\r\ndf2 = profile(\"/path/to/model_fp16.onnx\")\r\ndf2[\"args_op_name\"] = df2[\"args_op_name\"] + \"-Qfp16\"\r\ndf = pandas.concat([df1, df2], axis=0)\r\nprofile_plot(df, \"fp16\", size=(14, 8))\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1643226610/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1658265924",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12854#issuecomment-1658265924",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12854",
        "id": 1658265924,
        "node_id": "IC_kwDOCVq1mM5i1yFE",
        "user": {
            "login": "mllopartbsc",
            "id": 133785234,
            "node_id": "U_kgDOB_lmkg",
            "avatar_url": "https://avatars.githubusercontent.com/u/133785234?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mllopartbsc",
            "html_url": "https://github.com/mllopartbsc",
            "followers_url": "https://api.github.com/users/mllopartbsc/followers",
            "following_url": "https://api.github.com/users/mllopartbsc/following{/other_user}",
            "gists_url": "https://api.github.com/users/mllopartbsc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mllopartbsc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mllopartbsc/subscriptions",
            "organizations_url": "https://api.github.com/users/mllopartbsc/orgs",
            "repos_url": "https://api.github.com/users/mllopartbsc/repos",
            "events_url": "https://api.github.com/users/mllopartbsc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mllopartbsc/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-31T12:21:05Z",
        "updated_at": "2023-07-31T12:21:05Z",
        "author_association": "NONE",
        "body": "Hi @yszhou2019, thank you for providing the script you used for profiling, but I meant the script you've used to statically quantize your ONNX model. \r\n\r\nKind Regards.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1658265924/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]