[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883691405",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883691405",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883691405,
        "node_id": "IC_kwDOCVq1mM40rA-N",
        "user": {
            "login": "faxu",
            "id": 20780999,
            "node_id": "MDQ6VXNlcjIwNzgwOTk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/20780999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/faxu",
            "html_url": "https://github.com/faxu",
            "followers_url": "https://api.github.com/users/faxu/followers",
            "following_url": "https://api.github.com/users/faxu/following{/other_user}",
            "gists_url": "https://api.github.com/users/faxu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/faxu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/faxu/subscriptions",
            "organizations_url": "https://api.github.com/users/faxu/orgs",
            "repos_url": "https://api.github.com/users/faxu/repos",
            "events_url": "https://api.github.com/users/faxu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/faxu/received_events",
            "type": "User",
            "site_admin": true
        },
        "created_at": "2021-07-20T20:37:39Z",
        "updated_at": "2021-07-20T20:37:39Z",
        "author_association": "MEMBER",
        "body": "Did you check if the original model (non-ORT format) runs on DML? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883691405/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883700436",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883700436",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883700436,
        "node_id": "IC_kwDOCVq1mM40rDLU",
        "user": {
            "login": "gineshidalgo99",
            "id": 7797094,
            "node_id": "MDQ6VXNlcjc3OTcwOTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7797094?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gineshidalgo99",
            "html_url": "https://github.com/gineshidalgo99",
            "followers_url": "https://api.github.com/users/gineshidalgo99/followers",
            "following_url": "https://api.github.com/users/gineshidalgo99/following{/other_user}",
            "gists_url": "https://api.github.com/users/gineshidalgo99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gineshidalgo99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gineshidalgo99/subscriptions",
            "organizations_url": "https://api.github.com/users/gineshidalgo99/orgs",
            "repos_url": "https://api.github.com/users/gineshidalgo99/repos",
            "events_url": "https://api.github.com/users/gineshidalgo99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gineshidalgo99/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T20:52:07Z",
        "updated_at": "2021-07-20T20:56:25Z",
        "author_association": "CONTRIBUTOR",
        "body": "Yes, I forgot to say that, all models run (and we checked they match the original PyTorch model accuracies) if loaded from ONNX and set to DML\r\n\r\nWhat works:\r\n- ONNX file loaded, set to CPU and running inference on it\r\n- ONNX file loaded, set to GPU and running inference on it\r\n- ONNX file loaded, set to CPU, converted to ORT, loaded as ORT file, set to CPU, and running inference on it\r\n\r\nWhat does not work:\r\n- ONNX file loaded, set to GPU, converted to ORT, loaded as ORT file, set to GPU, and running inference on it\r\n\r\nWe also tried this:\r\n- ONNX file loaded, set to CPU, converted to ORT, loaded as ORT file, set to GPU, and running inference on it --> This one does not crash, but it is clearly running on CPU because its runtime timings are those of the CPU version (not the GPU version). So it seems that whatever session option was loaded for the ORT file is what it's used for it regardless of me trying to set it to another kind of device",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883700436/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883702796",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883702796",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883702796,
        "node_id": "IC_kwDOCVq1mM40rDwM",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T20:56:31Z",
        "updated_at": "2021-07-20T20:56:31Z",
        "author_association": "MEMBER",
        "body": "Did you try saving the optimized ONNX model as foo.onnx (where foo is the name of your model) *without* making a call to session_options.AddConfigEntry(\"session.save_model_format\", \"ORT\"); and then running the saved model with DML?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883702796/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883721516",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883721516",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883721516,
        "node_id": "IC_kwDOCVq1mM40rIUs",
        "user": {
            "login": "guoyu-wang",
            "id": 62914304,
            "node_id": "MDQ6VXNlcjYyOTE0MzA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/62914304?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guoyu-wang",
            "html_url": "https://github.com/guoyu-wang",
            "followers_url": "https://api.github.com/users/guoyu-wang/followers",
            "following_url": "https://api.github.com/users/guoyu-wang/following{/other_user}",
            "gists_url": "https://api.github.com/users/guoyu-wang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guoyu-wang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guoyu-wang/subscriptions",
            "organizations_url": "https://api.github.com/users/guoyu-wang/orgs",
            "repos_url": "https://api.github.com/users/guoyu-wang/repos",
            "events_url": "https://api.github.com/users/guoyu-wang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guoyu-wang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T21:30:21Z",
        "updated_at": "2021-07-20T21:30:21Z",
        "author_association": "CONTRIBUTOR",
        "body": "Please set the logging level to `ORT_LOGGING_LEVEL_VERBOSE` in \r\n`Environment = MakeUnique<Ort::Env>(ORT_LOGGING_LEVEL_WARNING, ModelRelativeFilePathCharPtr);` and attach the logs.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883721516/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883749680",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883749680",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883749680,
        "node_id": "IC_kwDOCVq1mM40rPMw",
        "user": {
            "login": "guoyu-wang",
            "id": 62914304,
            "node_id": "MDQ6VXNlcjYyOTE0MzA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/62914304?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guoyu-wang",
            "html_url": "https://github.com/guoyu-wang",
            "followers_url": "https://api.github.com/users/guoyu-wang/followers",
            "following_url": "https://api.github.com/users/guoyu-wang/following{/other_user}",
            "gists_url": "https://api.github.com/users/guoyu-wang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guoyu-wang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guoyu-wang/subscriptions",
            "organizations_url": "https://api.github.com/users/guoyu-wang/orgs",
            "repos_url": "https://api.github.com/users/guoyu-wang/repos",
            "events_url": "https://api.github.com/users/guoyu-wang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guoyu-wang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-20T22:33:23Z",
        "updated_at": "2021-07-20T22:34:37Z",
        "author_association": "CONTRIBUTOR",
        "body": "From the ORT file uploaded, the one converted with DML has no initializers in the graph (the model has no weights at all), seems the initializer of the graph are cleared out by the DML EP before the saving to ORT format happens, probably this is the main reason why the execution fails.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883749680/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883794612",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883794612",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883794612,
        "node_id": "IC_kwDOCVq1mM40raK0",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T00:30:40Z",
        "updated_at": "2021-07-21T00:30:40Z",
        "author_association": "MEMBER",
        "body": "What's the reason for attempting to use the ORT file format in the GPU scenarios? \r\n\r\nORT format is targeting mobile/edge scenarios where binary size is critical, so the current expected usage is with CPU kernels and optionally things like the NNAPI or CoreML EP to utilize the NPU on a device. CUDA kernels are massive so any binary size saving from using the ORT fomat is meaningless. Not sure how large the DML kernels are, although I know there's no infrastructure setup to exclude them in a minimal build, so a build with DML enabled would include all the kernels and not just the required ones. Based on that, there doesn't seem to be a binary size benefit, so it's not clear why you'd want/need to use an ORT format model.\r\n\r\n> ONNX file loaded, set to CPU, converted to ORT, loaded as ORT file, set to GPU, and running inference on it --> This one does not crash, but it is clearly running on CPU because its runtime timings are those of the CPU version (not the GPU version). So it seems that whatever session option was loaded for the ORT file is what it's used for it regardless of me trying to set it to another kind of device\r\n\r\nORT format doesn't support changing the static kernel assigned to a node at runtime. If you generated the ORT format model with CPU enabled, it will only use CPU at runtime. It does allow dynamic kernels (e.g. NNAPI and CoreML) taking nodes at runtime (node is executed as a CoreML or NNAPI model so the static kernel assigned is ignored), but that doesn't seem to be applicable to your usage. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883794612/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883807671",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883807671",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883807671,
        "node_id": "IC_kwDOCVq1mM40rdW3",
        "user": {
            "login": "gineshidalgo99",
            "id": 7797094,
            "node_id": "MDQ6VXNlcjc3OTcwOTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7797094?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gineshidalgo99",
            "html_url": "https://github.com/gineshidalgo99",
            "followers_url": "https://api.github.com/users/gineshidalgo99/followers",
            "following_url": "https://api.github.com/users/gineshidalgo99/following{/other_user}",
            "gists_url": "https://api.github.com/users/gineshidalgo99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gineshidalgo99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gineshidalgo99/subscriptions",
            "organizations_url": "https://api.github.com/users/gineshidalgo99/orgs",
            "repos_url": "https://api.github.com/users/gineshidalgo99/repos",
            "events_url": "https://api.github.com/users/gineshidalgo99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gineshidalgo99/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T01:07:12Z",
        "updated_at": "2021-07-21T01:07:12Z",
        "author_association": "CONTRIBUTOR",
        "body": "PS: I will answer with @pranavsharma and @gwang-msft tests tomorrow (foo.onnx and ORT_LOGGING_LEVEL_VERBOSE)\r\n\r\nAnswering to @skottmckay:\r\nIt is critical for us to be able to use a single and unified ORT API:\r\n- Using ORT files is important for us because we need to support Android, iOS, and CPU.\r\n- Supporting DML is also crucial because we need to support Windows/XBox machines.\r\n\r\nAnother hard requirement we have is that we cannot let the file sit on the hard-disk, we have to feed it to ORT on runtime. And ORT files/FlatBuffers are way simpler to serialize than protobuf/onnx ones.\r\n- I.e., it's extremely easy to send a std::vector<uint8> to the ORT API and hack it to read it instead of a foo.ort file (we already have this working).\r\n- ONNX/Protobuf: We tried doing this with the ONNX file, but feeding a buffer of Protobuf data to the ORT API is not that easy at all, and the ORT API seems to open the onnx file in many places, it does not only read it as a vector<int8> as it does with the ORT file.\r\n\r\nGiven these 2 reasons, having ORT files working with DML is very important for us in the short term.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883807671/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883819741",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883819741",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883819741,
        "node_id": "IC_kwDOCVq1mM40rgTd",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T01:41:54Z",
        "updated_at": "2021-07-21T01:42:44Z",
        "author_association": "MEMBER",
        "body": "ONNX format files are supported on all platforms. It's just that the binary size of the ORT library will be bigger vs. a minimal build that only supports ORT format models (by a few MB). For that you get a lot more flexibility though, such as the ability to use CPU or GPU depending on what's available at runtime. \r\n\r\nCan you provide more details on how you were trying to feed the ONNX format file at runtime? InferenceSession has an API where raw bytes can be provided, which can be used for both ONNX and ORT format models. Given that, I'm not quite following how 'the ORT API seems to open the onnx file in many places' given it's only seeing bytes and not a filename if that API is used.\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/894fc828587c919d815918c4da6cde314e5d54ed/onnxruntime/core/session/inference_session.cc#L686\r\n\r\nI did a quick test using the python API and it seemed to work fine with the ONNX format model being provided as bytes.\r\n\r\n```python\r\nimport onnxruntime as ort\r\nimport numpy as np\r\n\r\nmodel_path = r'my_test_model.onnx'\r\n\r\nso = ort.SessionOptions()\r\ns = ort.InferenceSession(model_path, so)\r\n\r\n# random input matching what the model requires\r\ninput_data = np.zeros((1, 5, 512, 867), dtype=np.float32)\r\ninputs = { 'input': input_data }\r\n\r\n# run with filename\r\no1 = s.run(None, inputs)\r\n\r\nwith open(model_path, 'rb') as infile:\r\n    bytes = infile.read()\r\n    # run with bytes\r\n    s2 = ort.InferenceSession(bytes, so)\r\n    o2 = s2.run(None, inputs)\r\n\r\n    # this model produces a single output so compare the run via filename with the run with bytes\r\n    print(np.array_equal(o1[0],o2[0]))\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883819741/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883834932",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883834932",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883834932,
        "node_id": "IC_kwDOCVq1mM40rkA0",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T02:21:41Z",
        "updated_at": "2021-07-21T02:21:41Z",
        "author_association": "MEMBER",
        "body": "@gineshidalgo99 Our public C API already provides a unified way to create sessions by passing the bytes associated with both ORT and ONNX models. Take a look at [this](https://github.com/microsoft/onnxruntime/blob/b46310b34996bcc47ca73d8ffe7c5af7ebc9a67f/include/onnxruntime/core/session/onnxruntime_c_api.h#L391) function. This way you can use the ORT format models on ios and android and ONNX format on desktop/server.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883834932/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883835578",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883835578",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883835578,
        "node_id": "IC_kwDOCVq1mM40rkK6",
        "user": {
            "login": "gineshidalgo99",
            "id": 7797094,
            "node_id": "MDQ6VXNlcjc3OTcwOTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7797094?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gineshidalgo99",
            "html_url": "https://github.com/gineshidalgo99",
            "followers_url": "https://api.github.com/users/gineshidalgo99/followers",
            "following_url": "https://api.github.com/users/gineshidalgo99/following{/other_user}",
            "gists_url": "https://api.github.com/users/gineshidalgo99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gineshidalgo99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gineshidalgo99/subscriptions",
            "organizations_url": "https://api.github.com/users/gineshidalgo99/orgs",
            "repos_url": "https://api.github.com/users/gineshidalgo99/repos",
            "events_url": "https://api.github.com/users/gineshidalgo99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gineshidalgo99/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T02:23:27Z",
        "updated_at": "2021-07-21T02:24:43Z",
        "author_association": "CONTRIBUTOR",
        "body": "We are happy to try this solution, it'd solve the problem for us in the short term (getting Windows fully working)!\r\n\r\nBut we are working on C++, and I could not find any C++ example of this `InferenceSession::Load(const void* model_data, int model_data_len)`. How can it be used from a onnx file in C++? Do I read it as a vector<int8>? As std::string? Or what exactly is what that void* takes? Any minimal C++ code snippet about how to turn the onnx file to that void* would highly help here!\r\n\r\n(Less important in the short term) Also, about why we cared about ORT files and DML, we need a solution that also works for our custom GPU EP (for platforms like Nintendo Switch and PlayStation 5), where we also need to minimize build size in eg PS5. Given the ORT file issue with DML, we are concerned this might also occur if we create our own GPU EP for PS5/Nintendo, is this the case?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883835578/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883839988",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883839988",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883839988,
        "node_id": "IC_kwDOCVq1mM40rlP0",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T02:36:35Z",
        "updated_at": "2021-07-21T02:36:35Z",
        "author_association": "MEMBER",
        "body": "Example of reading bytes from file:\r\nhttps://github.com/microsoft/onnxruntime/blob/894fc828587c919d815918c4da6cde314e5d54ed/onnxruntime/test/shared_lib/test_model_loading.cc#L21-L31\r\n\r\nThe bytes are just passed directly when creating the inference session.\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/894fc828587c919d815918c4da6cde314e5d54ed/onnxruntime/test/shared_lib/test_model_loading.cc#L41\r\n\r\nWe'll look into the DML issue as it should be possible to use that with an ORT format model. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883839988/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883840132",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883840132",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883840132,
        "node_id": "IC_kwDOCVq1mM40rlSE",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T02:36:52Z",
        "updated_at": "2021-07-21T02:36:52Z",
        "author_association": "MEMBER",
        "body": "One example in our repo is [here](https://github.com/microsoft/onnxruntime/blob/374acf14238b73ebf9596800e3dfdf1377fb39fa/onnxruntime/test/shared_lib/test_model_loading.cc#L23).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883840132/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883842433",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-883842433",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 883842433,
        "node_id": "IC_kwDOCVq1mM40rl2B",
        "user": {
            "login": "guoyu-wang",
            "id": 62914304,
            "node_id": "MDQ6VXNlcjYyOTE0MzA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/62914304?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guoyu-wang",
            "html_url": "https://github.com/guoyu-wang",
            "followers_url": "https://api.github.com/users/guoyu-wang/followers",
            "following_url": "https://api.github.com/users/guoyu-wang/following{/other_user}",
            "gists_url": "https://api.github.com/users/guoyu-wang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guoyu-wang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guoyu-wang/subscriptions",
            "organizations_url": "https://api.github.com/users/guoyu-wang/orgs",
            "repos_url": "https://api.github.com/users/guoyu-wang/repos",
            "events_url": "https://api.github.com/users/guoyu-wang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guoyu-wang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-21T02:43:43Z",
        "updated_at": "2021-07-21T02:43:43Z",
        "author_association": "CONTRIBUTOR",
        "body": "Or you can look at this past issue, https://github.com/microsoft/onnxruntime/issues/6475#issuecomment-768787689",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/883842433/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/885036494",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-885036494",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 885036494,
        "node_id": "IC_kwDOCVq1mM40wJXO",
        "user": {
            "login": "gineshidalgo99",
            "id": 7797094,
            "node_id": "MDQ6VXNlcjc3OTcwOTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7797094?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gineshidalgo99",
            "html_url": "https://github.com/gineshidalgo99",
            "followers_url": "https://api.github.com/users/gineshidalgo99/followers",
            "following_url": "https://api.github.com/users/gineshidalgo99/following{/other_user}",
            "gists_url": "https://api.github.com/users/gineshidalgo99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gineshidalgo99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gineshidalgo99/subscriptions",
            "organizations_url": "https://api.github.com/users/gineshidalgo99/orgs",
            "repos_url": "https://api.github.com/users/gineshidalgo99/repos",
            "events_url": "https://api.github.com/users/gineshidalgo99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gineshidalgo99/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-07-22T16:11:05Z",
        "updated_at": "2021-07-22T16:11:05Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks to those last answers we were able to feed the ONNX buffer into ORT directly, which is a working workaround for us!\r\n\r\nWe will keep an eye to this post to know when the DML-ORT file issue is solved, as we'd need to switch to it once it's working, but we are no longer blocked.\r\n\r\n Thanks for the quick answers and the great work!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/885036494/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/896320285",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-896320285",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 896320285,
        "node_id": "IC_kwDOCVq1mM41bMMd",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-10T21:13:37Z",
        "updated_at": "2021-08-10T21:13:37Z",
        "author_association": "MEMBER",
        "body": "Regarding the DML support, the DML EP has two different ways of handling parts of the graph. One is with statically registered kernels, and one is with dynamically created kernels. The static ones should work out-of-the-box with the ORT format. The dynamically registered ones however are making some changes to the graph earlier than expected, so parts of the graph aren't available to be saved in the ORT format model. As that's done somewhat unofficially (there's a const_cast to get access to initializers) we'd need to look into restructuring that to make sure that when we're creating the ORT format model that doesn't happen.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/896320285/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/896695973",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-896695973",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 896695973,
        "node_id": "IC_kwDOCVq1mM41cn6l",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-08-11T10:12:45Z",
        "updated_at": "2021-08-11T10:12:45Z",
        "author_association": "MEMBER",
        "body": "POC for adding support for DML when using an ORT format model: https://github.com/microsoft/onnxruntime/compare/skottmckay/ORT_model_support_with_DML_EP\r\n\r\nTechnically we could create the ORT format model with just basic optimizations and DML disabled to not require the changes in the DML graph partitioning. At runtime, if DML was enabled it could still execute the same nodes. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/896695973/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1198794819",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1198794819",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1198794819,
        "node_id": "IC_kwDOCVq1mM5HdChD",
        "user": {
            "login": "diablodale",
            "id": 679350,
            "node_id": "MDQ6VXNlcjY3OTM1MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/679350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/diablodale",
            "html_url": "https://github.com/diablodale",
            "followers_url": "https://api.github.com/users/diablodale/followers",
            "following_url": "https://api.github.com/users/diablodale/following{/other_user}",
            "gists_url": "https://api.github.com/users/diablodale/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/diablodale/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/diablodale/subscriptions",
            "organizations_url": "https://api.github.com/users/diablodale/orgs",
            "repos_url": "https://api.github.com/users/diablodale/repos",
            "events_url": "https://api.github.com/users/diablodale/events{/privacy}",
            "received_events_url": "https://api.github.com/users/diablodale/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-29T01:37:36Z",
        "updated_at": "2022-07-29T01:37:36Z",
        "author_association": "CONTRIBUTOR",
        "body": "I think I have the same or highly related issue.\r\n\r\n1. onnx runtime 1.12 with DML ep\r\n2. squeezenet1.0-7.onnx from Microsoft git repo; filesize = 4,952,222 bytes\r\n3. `SetOptimizedModelFilePath(thepath)`\r\n4. session optimizes model and saves it to thepath with a file size = 3,756 bytes\r\n5. inference runs correctly\r\n6. shutdown\r\n\r\nthen\r\n\r\n1. onnx runtime 1.12 with DML ep\r\n2. the optimized squeezenet1.0-7.onnx from above step 4\r\n3. session fails with `Load model from C:\\**redacted**\\squeezenet1.0-7.onnx failed:D:\\a\\_work\\1\\s\\onnxruntime\\core\\graph\\graph.cc:1203 onnxruntime::Graph::Graph This is an invalid model. Tensor does not have type information.`\r\n\r\nIf not the same issue, then please tell me and I'll open a new issue\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1198794819/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241777518",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1241777518",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1241777518,
        "node_id": "IC_kwDOCVq1mM5KBAVu",
        "user": {
            "login": "gedoensmax",
            "id": 44298237,
            "node_id": "MDQ6VXNlcjQ0Mjk4MjM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/44298237?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gedoensmax",
            "html_url": "https://github.com/gedoensmax",
            "followers_url": "https://api.github.com/users/gedoensmax/followers",
            "following_url": "https://api.github.com/users/gedoensmax/following{/other_user}",
            "gists_url": "https://api.github.com/users/gedoensmax/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gedoensmax/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gedoensmax/subscriptions",
            "organizations_url": "https://api.github.com/users/gedoensmax/orgs",
            "repos_url": "https://api.github.com/users/gedoensmax/repos",
            "events_url": "https://api.github.com/users/gedoensmax/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gedoensmax/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-09T10:10:20Z",
        "updated_at": "2022-09-09T10:11:30Z",
        "author_association": "NONE",
        "body": "@diablodale I am with him on this one. Getting the same error on multiple models and the resulting ONNX files are not viewable in netron.\r\nI also tried to set ORT_DISABLE_ALL optimizations in case ops are fused for DML but the Model is still broken.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1241777518/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1243054210",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1243054210",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1243054210,
        "node_id": "IC_kwDOCVq1mM5KF4CC",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-11T22:18:05Z",
        "updated_at": "2022-09-11T22:18:05Z",
        "author_association": "MEMBER",
        "body": "The DML EP makes some changes to the model during partitioning that are not really expected by ORT. Essentially it does a const_cast and steals initializers for memory usage reasons, but that means ORT doesn't have the initializers to write to the optimized file. @fdwr would your PR (still open I note) help with that? \r\n\r\n@diablodale @gedoensmax can you elaborate on your use case where you want/need DML to be enabled when creating an optimized model vs. doing that at runtime?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1243054210/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1243058585",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1243058585",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1243058585,
        "node_id": "IC_kwDOCVq1mM5KF5GZ",
        "user": {
            "login": "gedoensmax",
            "id": 44298237,
            "node_id": "MDQ6VXNlcjQ0Mjk4MjM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/44298237?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gedoensmax",
            "html_url": "https://github.com/gedoensmax",
            "followers_url": "https://api.github.com/users/gedoensmax/followers",
            "following_url": "https://api.github.com/users/gedoensmax/following{/other_user}",
            "gists_url": "https://api.github.com/users/gedoensmax/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gedoensmax/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gedoensmax/subscriptions",
            "organizations_url": "https://api.github.com/users/gedoensmax/orgs",
            "repos_url": "https://api.github.com/users/gedoensmax/repos",
            "events_url": "https://api.github.com/users/gedoensmax/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gedoensmax/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-11T22:46:56Z",
        "updated_at": "2022-09-11T22:46:56Z",
        "author_association": "NONE",
        "body": "I have been looking into session creation time on ORT. For some models it is quite drastically decreased if the shape is known for esch tensor. With a fixed size input model and simplifying these shapes are usually saved - but sometimes only to some stage within the model. If i understand ort correctly it runs the model to „really“ know all shapes if the input has a fixed size. \r\n\r\nI am aware that these models might get a complete shape inference with some graph surgeon magic. \r\nNonetheless some applications habe either fixed size engines that are used on demand but have this problem (would be great to cache this to disk for later use). Or use a dynamic size model but if one size is used it is being used multiple times so that you might want to save this fixed shape ONNX file after first use. Something like TensorRT engine caching for DML. Or would the better way to save to ORT format ? \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1243058585/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1243098604",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1243098604",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1243098604,
        "node_id": "IC_kwDOCVq1mM5KGC3s",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-12T01:11:38Z",
        "updated_at": "2022-09-12T01:11:38Z",
        "author_association": "MEMBER",
        "body": "@gedoensmax If you have a model with dynamic dimensions and want to make them fixed, you could use this tool: https://onnxruntime.ai/docs/reference/mobile/make-dynamic-shape-fixed.html\r\n\r\nI don't quite understand how model load time would be affected by having fixed shapes. If anything, I would expect more optimizations to be possible when shapes are fixed. \r\n\r\nI would suggest running the 'basic' level optimizations on the model with just the CPU EP enabled to do those optimizations ahead of time. They are not specific to any EP, only use official ONNX operators, and cover things like constant folding and common subexpression elimination. \r\n\r\nBeyond the 'basic' level you get into EP specific optimizations which may involve compiling nodes or fusing nodes that will use a custom operator. Currently there's no general purpose way to save a compiled node like TensorRT engine caching does. An inference session is intended to be re-used though, so this cost during loading is not per-inference.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1243098604/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1244256929",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1244256929",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1244256929,
        "node_id": "IC_kwDOCVq1mM5KKdqh",
        "user": {
            "login": "fdwr",
            "id": 1809166,
            "node_id": "MDQ6VXNlcjE4MDkxNjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1809166?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fdwr",
            "html_url": "https://github.com/fdwr",
            "followers_url": "https://api.github.com/users/fdwr/followers",
            "following_url": "https://api.github.com/users/fdwr/following{/other_user}",
            "gists_url": "https://api.github.com/users/fdwr/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fdwr/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fdwr/subscriptions",
            "organizations_url": "https://api.github.com/users/fdwr/orgs",
            "repos_url": "https://api.github.com/users/fdwr/repos",
            "events_url": "https://api.github.com/users/fdwr/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fdwr/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-12T19:38:11Z",
        "updated_at": "2022-09-14T17:44:32Z",
        "author_association": "MEMBER",
        "body": "@skottmckay 🤔 I should abandon that PR, as @sumitsays is working on a more complete solution after discussing with Cheng Tang about the EP interface refactor. Currently the DML EP fuses partitions of DML nodes into a single DML_GRAPH node, which is an IDMLOperator that contains all the operators for that partition, but if you attempt to reload the .ort graph containing a \"DmlFusedGraph\" node, ORT won't know how to map that to any operator because context is lost (there is no such ONNX operator with that name, and the internal subgraph only existed in memory).\r\n\r\nHowever, beware that even after Sumit's changes, it will *generally* not be robust to optimize the graph with one GPU and run the same graph on a *different* GPU, as differences between GPU's (e.g. which data types are supported) could actually make a difference in the optimized graph. Replaying on the *same* machine, or on a specific device (e.g. gaming console) would be more robust.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1244256929/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247443733",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1247443733",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1247443733,
        "node_id": "IC_kwDOCVq1mM5KWnsV",
        "user": {
            "login": "diablodale",
            "id": 679350,
            "node_id": "MDQ6VXNlcjY3OTM1MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/679350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/diablodale",
            "html_url": "https://github.com/diablodale",
            "followers_url": "https://api.github.com/users/diablodale/followers",
            "following_url": "https://api.github.com/users/diablodale/following{/other_user}",
            "gists_url": "https://api.github.com/users/diablodale/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/diablodale/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/diablodale/subscriptions",
            "organizations_url": "https://api.github.com/users/diablodale/orgs",
            "repos_url": "https://api.github.com/users/diablodale/repos",
            "events_url": "https://api.github.com/users/diablodale/events{/privacy}",
            "received_events_url": "https://api.github.com/users/diablodale/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-15T00:47:46Z",
        "updated_at": "2022-09-15T00:47:46Z",
        "author_association": "CONTRIBUTOR",
        "body": "> @diablodale @gedoensmax can you elaborate on your use case where you want/need DML to be enabled when creating an optimized model vs. doing that at runtime?\r\n\r\nI create a DLL plugin for the Cycling74 Max runtime patching system. My customers are educators, researchers, artists, musicians, etc. I provide one onnx model for a specific use case *plus* the ability to run any onnx model. My DLL transforms in/outs between native Max data. My plugin allows running the model on the cpu, directml, cuda, or tensorRT providers with a single setting change. I hide all the technical complexities so my customers can focus on their art/research/education.\r\n\r\nThe Max environment is always running, it is a graphical hack/patch environment where nodes are connected by patchcords. Patchcords and nodes are reshaped/connected hundreds of times a day as customers experiment and try ideas. This realtime iteration necessitates caching and reuse. The time burden of running the onnx optimization process every time they connect a patchcord or click \"go\" hampers their creativity and kills their \"flow\". \r\n\r\nI know when hardware, models, or settings change...therefore I can cache models after they go through the optimization process. I already do this successfully with the TensorRT provider. A similar ability with DirectML is desired and I attempted it with `SetOptimizedModelFilePath()` but ran into this same OP...the saved DirectML model is unusable.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247443733/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247557652",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1247557652",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1247557652,
        "node_id": "IC_kwDOCVq1mM5KXDgU",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-15T04:10:40Z",
        "updated_at": "2022-09-15T04:10:40Z",
        "author_association": "MEMBER",
        "body": "Unfortunately ORT doesn't have a way to general way to save a compiled node. The TensorRT EP is doing that via TensorRT's ability to save but AFAIK that is the only place that's possible. For CPU and CUDA you could save the fully optimized model as neither of those compile nodes. The saved model would contain internal operators that are specific to the CPU/CUDA EPs, but that should be fine for local caching. \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1247557652/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1343861828",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1343861828",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1343861828,
        "node_id": "IC_kwDOCVq1mM5QGbRE",
        "user": {
            "login": "fdwr",
            "id": 1809166,
            "node_id": "MDQ6VXNlcjE4MDkxNjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1809166?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fdwr",
            "html_url": "https://github.com/fdwr",
            "followers_url": "https://api.github.com/users/fdwr/followers",
            "following_url": "https://api.github.com/users/fdwr/following{/other_user}",
            "gists_url": "https://api.github.com/users/fdwr/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fdwr/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fdwr/subscriptions",
            "organizations_url": "https://api.github.com/users/fdwr/orgs",
            "repos_url": "https://api.github.com/users/fdwr/repos",
            "events_url": "https://api.github.com/users/fdwr/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fdwr/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-09T05:16:42Z",
        "updated_at": "2022-12-09T05:16:42Z",
        "author_association": "MEMBER",
        "body": "@diablodale / @gedoensmax:\r\n- This pending change allows exporting/reimporting the optimized model (recently enabled after Sumit's refactoring): https://github.com/microsoft/onnxruntime/pull/13913.\r\n- It will be in ORT 1.14.\r\n- Note the caveat remains that the same .ort file cannot reliably be replayed on different execution providers, and that the same file may not be replayable on the same execution provider across different GPU's, due to different graph partitioning assignments made based on GPU data type support.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1343861828/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1344369197",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/8440#issuecomment-1344369197",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/8440",
        "id": 1344369197,
        "node_id": "IC_kwDOCVq1mM5QIXIt",
        "user": {
            "login": "diablodale",
            "id": 679350,
            "node_id": "MDQ6VXNlcjY3OTM1MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/679350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/diablodale",
            "html_url": "https://github.com/diablodale",
            "followers_url": "https://api.github.com/users/diablodale/followers",
            "following_url": "https://api.github.com/users/diablodale/following{/other_user}",
            "gists_url": "https://api.github.com/users/diablodale/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/diablodale/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/diablodale/subscriptions",
            "organizations_url": "https://api.github.com/users/diablodale/orgs",
            "repos_url": "https://api.github.com/users/diablodale/repos",
            "events_url": "https://api.github.com/users/diablodale/events{/privacy}",
            "received_events_url": "https://api.github.com/users/diablodale/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-09T14:22:32Z",
        "updated_at": "2022-12-09T14:23:04Z",
        "author_association": "CONTRIBUTOR",
        "body": "Got it, I've already code in place to invalidate a persisted optimized model if any config changes.\r\n\r\nA question, in #13913 I saw the comment `This transformer applies DML-specific fusions that go beyond what ORT offers by default`. The following is some guessing...\r\nWhen we persist with `setOptFilePath=true`, it will not do the fusing of partitions of DML nodes into a single DML_GRAPH.\r\nIt will instead persist a slightly less optimized model lacking that fuse.\r\nWhen this persisted model is loaded, will Ort do that final fuse optimization?\r\nOr, is this the tradeoff to have a faster load?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1344369197/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]