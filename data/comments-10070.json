[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/996945070",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10070#issuecomment-996945070",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10070",
        "id": 996945070,
        "node_id": "IC_kwDOCVq1mM47bCyu",
        "user": {
            "login": "stevenlix",
            "id": 38092805,
            "node_id": "MDQ6VXNlcjM4MDkyODA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/38092805?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stevenlix",
            "html_url": "https://github.com/stevenlix",
            "followers_url": "https://api.github.com/users/stevenlix/followers",
            "following_url": "https://api.github.com/users/stevenlix/following{/other_user}",
            "gists_url": "https://api.github.com/users/stevenlix/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stevenlix/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stevenlix/subscriptions",
            "organizations_url": "https://api.github.com/users/stevenlix/orgs",
            "repos_url": "https://api.github.com/users/stevenlix/repos",
            "events_url": "https://api.github.com/users/stevenlix/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stevenlix/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-17T18:50:42Z",
        "updated_at": "2021-12-17T18:50:42Z",
        "author_association": "MEMBER",
        "body": "To enable FP16 in ORT-TRT, just set ORT_TENSORRT_FP16_ENABLE to 1. You can also use provider options in C/python API. Details can be found here,\r\nhttps://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#execution-provider-options\r\nCan you provide more details of why you think FP16 is not enabled? Do you see errors or perf is not getting better?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/996945070/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/997141093",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10070#issuecomment-997141093",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10070",
        "id": 997141093,
        "node_id": "IC_kwDOCVq1mM47bypl",
        "user": {
            "login": "QuantumLiu",
            "id": 21980268,
            "node_id": "MDQ6VXNlcjIxOTgwMjY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21980268?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/QuantumLiu",
            "html_url": "https://github.com/QuantumLiu",
            "followers_url": "https://api.github.com/users/QuantumLiu/followers",
            "following_url": "https://api.github.com/users/QuantumLiu/following{/other_user}",
            "gists_url": "https://api.github.com/users/QuantumLiu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/QuantumLiu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/QuantumLiu/subscriptions",
            "organizations_url": "https://api.github.com/users/QuantumLiu/orgs",
            "repos_url": "https://api.github.com/users/QuantumLiu/repos",
            "events_url": "https://api.github.com/users/QuantumLiu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/QuantumLiu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-18T04:24:28Z",
        "updated_at": "2021-12-18T04:24:28Z",
        "author_association": "NONE",
        "body": "> To enable FP16 in ORT-TRT, just set ORT_TENSORRT_FP16_ENABLE to 1. You can also use provider options in C/python API. Details can be found here, https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#execution-provider-options Can you provide more details of why you think FP16 is not enabled? Do you see errors or perf is not getting better?\r\n\r\nSolved, thank you very much. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/997141093/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]