[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12537",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12537/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12537/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12537/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12537",
        "id": 1334029948,
        "node_id": "PR_kwDOCVq1mM487Vw5",
        "number": 12537,
        "title": "[Dup] Fix SAME_UPPER/SAME_LOWER (auto_pad attribute) in ConvTranspose",
        "user": {
            "login": "jcwchen",
            "id": 14194980,
            "node_id": "MDQ6VXNlcjE0MTk0OTgw",
            "avatar_url": "https://avatars.githubusercontent.com/u/14194980?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jcwchen",
            "html_url": "https://github.com/jcwchen",
            "followers_url": "https://api.github.com/users/jcwchen/followers",
            "following_url": "https://api.github.com/users/jcwchen/following{/other_user}",
            "gists_url": "https://api.github.com/users/jcwchen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jcwchen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jcwchen/subscriptions",
            "organizations_url": "https://api.github.com/users/jcwchen/orgs",
            "repos_url": "https://api.github.com/users/jcwchen/repos",
            "events_url": "https://api.github.com/users/jcwchen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jcwchen/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-10T03:53:06Z",
        "updated_at": "2022-08-22T22:35:38Z",
        "closed_at": "2022-08-22T22:35:34Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12537",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12537",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12537.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12537.patch",
            "merged_at": "2022-08-22T22:35:34Z"
        },
        "body": "**Description**:\r\nDup of https://github.com/microsoft/onnxruntime/pull/5368/. Recreate this PR for cleaner commit log. Target it to ORT 1.13. Recreating this PR to retarget the main branch instead of master branch.\r\n\r\nTo sync the definition of SAME_UPPER/SAME_LOWER among all operators and make it same as ONNX definition, switch the logic of SAME_UPPER and SAME_LOWER in ConvTranspose.\r\n\r\nDefinition of SAME_UPPER and SAME_LOWER should be as follows:\r\n```\r\nif auto_pads == 'SAME_UPPER':\r\n  pad_head = paddings / 2  # smaller one \r\n  pad_tail = paddings - paddings / 2  # larger one\r\nelif auto_pads == 'SAME_LOWER':\r\n  pad_head = paddings - paddings / 2  # larger one\r\n  pad_tail = paddings / 2  # smaller one\r\n```\r\n\r\nBesides, revert the temporary warning from\" https://github.com/microsoft/onnxruntime/pull/11984.\r\n\r\n**Motivation and Context**\r\nThe `auto_pad` attribute, `SAME_UPPER` and `SAME_LOWER` of `ConvTranspose` is different from other operators' (pool and conv related operators) `auto_pad` attribute. The behavior of same attribute should be the same among all operators. Also, it does not meet the definition in ONNX.\r\n\r\n- `SAME_UPPER` and `SAME_LOWER` in other operators\r\nhttps://github.com/microsoft/onnxruntime/blob/c20fcf26ebc0eeda41b6fd4e54d87623030ead91/onnxruntime/core/providers/cpu/nn/pool_attributes.h#L149\r\n- ConvTranspose definition in ONNX:\r\nhttps://github.com/onnx/onnx/blob/b2ed660d0a065b8346816f2c3a95d79ca79b88c9/onnx/defs/nn/defs.cc#L1222\r\n- A related PR in ONNX (There is a contradiction of this in ONNX):\r\nhttps://github.com/onnx/onnx/pull/3019",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12537/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12537/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12538",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12538/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12538/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12538/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12538",
        "id": 1334072168,
        "node_id": "I_kwDOCVq1mM5PhFNo",
        "number": 12538,
        "title": "DML EP cannot load some onnx files.",
        "user": {
            "login": "creaiter",
            "id": 34888120,
            "node_id": "MDQ6VXNlcjM0ODg4MTIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/34888120?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/creaiter",
            "html_url": "https://github.com/creaiter",
            "followers_url": "https://api.github.com/users/creaiter/followers",
            "following_url": "https://api.github.com/users/creaiter/following{/other_user}",
            "gists_url": "https://api.github.com/users/creaiter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/creaiter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/creaiter/subscriptions",
            "organizations_url": "https://api.github.com/users/creaiter/orgs",
            "repos_url": "https://api.github.com/users/creaiter/repos",
            "events_url": "https://api.github.com/users/creaiter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/creaiter/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1673018947,
                "node_id": "MDU6TGFiZWwxNjczMDE4OTQ3",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:DML",
                "name": "ep:DML",
                "color": "0052CC",
                "default": false,
                "description": "issues related to the DirectML execution provider"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2022-08-10T05:02:54Z",
        "updated_at": "2022-09-05T07:56:47Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\nThere is an RuntimeException error when I try to load an onnx file with DML EP. However, the CPU EP works fine for the same file.\r\nError message: `Microsoft.ML.OnnxRuntime.OnnxRuntimeException: [ErrorCode:RuntimeException] Exception during initialization:`\r\n\r\n**Urgency**\r\nThere is no hard deadline, but this is important for me.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- ONNX Runtime installed from (source or binary): binary\r\n- ONNX Runtime version: 1.12.1\r\n- DirectML version: 1.9.0\r\n- Visual Studio version (if applicable): 2019 (16.11.17)\r\n- GPU model: NVIDIA Geforce GTX 1650\r\n- Onnx model: [Segmentation.onnx](https://drive.google.com/file/d/1C2EWlA3zciduDm2HJkrN8MGqDfPH5hHW/view?usp=sharing)\r\n\r\n**To Reproduce**\r\nThe test c# code is quite simple\r\n```\r\nusing Microsoft.ML.OnnxRuntime;\r\n\r\ninternal class Program\r\n{\r\n    static void Main(string[] args)\r\n    {\r\n        string modelFilePath = \"Segmentation.onnx\";\r\n        var options = new SessionOptions();\r\n        options.EnableMemoryPattern = false;\r\n        options.ExecutionMode = ExecutionMode.ORT_SEQUENTIAL;\r\n        options.LogSeverityLevel = OrtLoggingLevel.ORT_LOGGING_LEVEL_VERBOSE;\r\n        options.GraphOptimizationLevel = GraphOptimizationLevel.ORT_DISABLE_ALL;\r\n        options.AppendExecutionProvider_DML();\r\n        var session = new InferenceSession(modelFilePath, options);\r\n    }\r\n}\r\n```\r\nAnd this is the result:\r\n![image](https://user-images.githubusercontent.com/34888120/183819964-5569064d-a3ec-4bcd-847e-6a4315559472.png)\r\n\r\nI have also tested at a c++ [project](https://github.com/fdwr/OnnxRuntimeDirectMLEPSample), but there was no initialization error.\r\nDo you have any idea to deal with this?\r\n\r\n\r\n**Expected behavior**\r\nNot to throw an exception.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12538/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12538/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12539",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12539/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12539/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12539/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12539",
        "id": 1334241681,
        "node_id": "PR_kwDOCVq1mM488C43",
        "number": 12539,
        "title": "Enable PythonOp for --enable_training_torch_interop build",
        "user": {
            "login": "pengwa",
            "id": 10530022,
            "node_id": "MDQ6VXNlcjEwNTMwMDIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/10530022?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pengwa",
            "html_url": "https://github.com/pengwa",
            "followers_url": "https://api.github.com/users/pengwa/followers",
            "following_url": "https://api.github.com/users/pengwa/following{/other_user}",
            "gists_url": "https://api.github.com/users/pengwa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pengwa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pengwa/subscriptions",
            "organizations_url": "https://api.github.com/users/pengwa/orgs",
            "repos_url": "https://api.github.com/users/pengwa/repos",
            "events_url": "https://api.github.com/users/pengwa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pengwa/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-10T08:08:02Z",
        "updated_at": "2022-08-11T16:49:31Z",
        "closed_at": "2022-08-11T16:49:31Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12539",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12539",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12539.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12539.patch",
            "merged_at": "2022-08-11T16:49:31Z"
        },
        "body": "**Description**: Enable PythonOp by default for --enable_training_torch_interop build\r\n\r\nThe current state of using PythonOp to run ORTModule looks like this:\r\n\r\n    from onnxruntime.training.ortmodule import ORTModule\r\n    from onnxruntime.training.ortmodule._custom_autograd_function import enable_custom_autograd_support\r\n    enable_custom_autograd_support()\r\n    m = ORTModule(m)\r\n\r\n\r\nSome feedbacks: \r\n  1). we had a python frontend switch (enable_custom_autograd_support()) to enable the feature, without whom some models cannot run. \r\n  2). On the other hand, there is another build flag called \"--enable_training_torch_interop\" which enables the PythonOp backend support in ORT. \r\nThose two flags make is confusion to use sometimes. \r\n\r\nConsidering all our training python wheels released enable the \"--enable_training_torch_interop\" by default, we should enable the flag in Python frontend to align with it according to the build flag.\r\n\r\nAfterwards, for user who use \"--enable_training_torch_interop\" enabled wheel, integration code looks like this: \r\n\r\n    from onnxruntime.training.ortmodule import ORTModule\r\n    m = ORTModule(m)\r\n\r\nfor user who did not use \"\"--enable_training_torch_interop\" enabled wheel (private build most likely), the PythonOp feature won't work anyway as before. \r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12539/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12539/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12540",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12540/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12540/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12540/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12540",
        "id": 1334302265,
        "node_id": "I_kwDOCVq1mM5Ph9Y5",
        "number": 12540,
        "title": "java  deploy in k8s Failed to load library libonnxruntime_providers_cuda.so with error",
        "user": {
            "login": "BillyChao",
            "id": 19453102,
            "node_id": "MDQ6VXNlcjE5NDUzMTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/19453102?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BillyChao",
            "html_url": "https://github.com/BillyChao",
            "followers_url": "https://api.github.com/users/BillyChao/followers",
            "following_url": "https://api.github.com/users/BillyChao/following{/other_user}",
            "gists_url": "https://api.github.com/users/BillyChao/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BillyChao/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BillyChao/subscriptions",
            "organizations_url": "https://api.github.com/users/BillyChao/orgs",
            "repos_url": "https://api.github.com/users/BillyChao/repos",
            "events_url": "https://api.github.com/users/BillyChao/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BillyChao/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 2186357781,
                "node_id": "MDU6TGFiZWwyMTg2MzU3Nzgx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:CUDA",
                "name": "ep:CUDA",
                "color": "0052CC",
                "default": false,
                "description": "issues related to the CUDA execution provider"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2022-08-10T08:58:31Z",
        "updated_at": "2022-08-16T09:49:16Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Deploy inference service using java in k8s   onnxruntime error**\r\nai.onnxruntime.OrtException: Error code - ORT_RUNTIME_EXCEPTION - message: /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1029 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublas.so.11: cannot open shared object file: No such file or directory\r\n\tat ai.onnxruntime.OrtSession$SessionOptions.addCUDA(Native Method)\r\n\tat ai.onnxruntime.OrtSession$SessionOptions.addCUDA(OrtSession.java:768)\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (Alibaba Cloud Linux (Aliyun Linux) release 2.1903):\r\n- ONNX Runtime installed from (maven ):\r\n- ONNX Runtime version: 1.11.0\r\n- Java version: 1.8\r\n- CUDA  version: 11.2\r\n- GPU model and memory:   Tesla T4  15G\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12540/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12540/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12541",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12541/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12541/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12541/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12541",
        "id": 1334736848,
        "node_id": "PR_kwDOCVq1mM489r_L",
        "number": 12541,
        "title": "Remove CUDA 10.2 support",
        "user": {
            "login": "snnn",
            "id": 856316,
            "node_id": "MDQ6VXNlcjg1NjMxNg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/856316?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/snnn",
            "html_url": "https://github.com/snnn",
            "followers_url": "https://api.github.com/users/snnn/followers",
            "following_url": "https://api.github.com/users/snnn/following{/other_user}",
            "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/snnn/subscriptions",
            "organizations_url": "https://api.github.com/users/snnn/orgs",
            "repos_url": "https://api.github.com/users/snnn/repos",
            "events_url": "https://api.github.com/users/snnn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/snnn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-10T14:40:31Z",
        "updated_at": "2022-08-11T05:46:42Z",
        "closed_at": "2022-08-11T05:46:41Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12541",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12541",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12541.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12541.patch",
            "merged_at": "2022-08-11T05:46:41Z"
        },
        "body": "**Description**: \r\n\r\nRemove CUDA 10.2 support\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12541/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12541/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12542",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12542/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12542/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12542/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12542",
        "id": 1334904020,
        "node_id": "PR_kwDOCVq1mM48-Oo1",
        "number": 12542,
        "title": "fix qdq relu input bug",
        "user": {
            "login": "chenfucn",
            "id": 1316708,
            "node_id": "MDQ6VXNlcjEzMTY3MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1316708?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chenfucn",
            "html_url": "https://github.com/chenfucn",
            "followers_url": "https://api.github.com/users/chenfucn/followers",
            "following_url": "https://api.github.com/users/chenfucn/following{/other_user}",
            "gists_url": "https://api.github.com/users/chenfucn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chenfucn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chenfucn/subscriptions",
            "organizations_url": "https://api.github.com/users/chenfucn/orgs",
            "repos_url": "https://api.github.com/users/chenfucn/repos",
            "events_url": "https://api.github.com/users/chenfucn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chenfucn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-10T16:49:33Z",
        "updated_at": "2022-08-10T21:06:58Z",
        "closed_at": "2022-08-10T21:06:52Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12542",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12542",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12542.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12542.patch",
            "merged_at": "2022-08-10T21:06:52Z"
        },
        "body": "**Description**: Fix minor bug in qdq quantization tool\r\n\r\n**Motivation and Context**\r\nRelu node is removed in qdq quantization tool if it can be merged to its input node. When performing the removal, we forgot to check whether the input is actually the graph input",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12542/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12542/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12544",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12544/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12544/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12544/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12544",
        "id": 1335217793,
        "node_id": "PR_kwDOCVq1mM48_R0V",
        "number": 12544,
        "title": "QDQ debugger - activations compare",
        "user": {
            "login": "chenfucn",
            "id": 1316708,
            "node_id": "MDQ6VXNlcjEzMTY3MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1316708?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chenfucn",
            "html_url": "https://github.com/chenfucn",
            "followers_url": "https://api.github.com/users/chenfucn/followers",
            "following_url": "https://api.github.com/users/chenfucn/following{/other_user}",
            "gists_url": "https://api.github.com/users/chenfucn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chenfucn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chenfucn/subscriptions",
            "organizations_url": "https://api.github.com/users/chenfucn/orgs",
            "repos_url": "https://api.github.com/users/chenfucn/repos",
            "events_url": "https://api.github.com/users/chenfucn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chenfucn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2022-08-10T21:24:57Z",
        "updated_at": "2022-08-16T16:50:00Z",
        "closed_at": "2022-08-16T00:03:28Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12544",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12544",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12544.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12544.patch",
            "merged_at": "2022-08-16T00:03:28Z"
        },
        "body": "**Description**: Debugger for QDQ loss part 1\r\n\r\n**Motivation and Context**\r\nThis is the first part of the QDQ debugger tool: activation comparison. The idea is that during quantization, we have an original float model and a qdq model. The debugger can run the two models side by side using the same input data. By comparing intermediate activations, we can help the model author figure out where the values differ, and take steps to reduce precision loss.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12544/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12544/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12545",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12545/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12545/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12545/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12545",
        "id": 1335256011,
        "node_id": "PR_kwDOCVq1mM48_aCz",
        "number": 12545,
        "title": "[wasm] use same export name for SIMD/NOSIMD build",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-10T22:11:26Z",
        "updated_at": "2022-08-20T01:17:51Z",
        "closed_at": "2022-08-20T01:17:51Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12545",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12545",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12545.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12545.patch",
            "merged_at": "2022-08-20T01:17:51Z"
        },
        "body": "**Description**: Use same export name for WebAssembly when SIMD is on and off.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n    - Sometimes the generated .js file in the 2 builds does not match. As we want to use only one generated .js file for both SIMD/NOSIMD, we want to eliminate the difference.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12545/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12545/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12546",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12546/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12546/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12546/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12546",
        "id": 1335300389,
        "node_id": "PR_kwDOCVq1mM48_i0A",
        "number": 12546,
        "title": "Add banned no further than option to NGramRepeatBlock contrib op",
        "user": {
            "login": "xi-liu-ds",
            "id": 63893533,
            "node_id": "MDQ6VXNlcjYzODkzNTMz",
            "avatar_url": "https://avatars.githubusercontent.com/u/63893533?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xi-liu-ds",
            "html_url": "https://github.com/xi-liu-ds",
            "followers_url": "https://api.github.com/users/xi-liu-ds/followers",
            "following_url": "https://api.github.com/users/xi-liu-ds/following{/other_user}",
            "gists_url": "https://api.github.com/users/xi-liu-ds/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xi-liu-ds/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xi-liu-ds/subscriptions",
            "organizations_url": "https://api.github.com/users/xi-liu-ds/orgs",
            "repos_url": "https://api.github.com/users/xi-liu-ds/repos",
            "events_url": "https://api.github.com/users/xi-liu-ds/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xi-liu-ds/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2022-08-10T23:14:32Z",
        "updated_at": "2022-08-24T00:11:59Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12546",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12546",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12546.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12546.patch",
            "merged_at": null
        },
        "body": "**Description**: When enforce no repetition of n-grams, we add an option of only checking no more than certain numbers of previous n-gram repetitions. \r\n\r\n**Motivation and Context**: Needed by transformer models in sequence generation algorithms (greedy search and beam search) with a customization.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12546/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12546/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12547",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12547/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12547/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12547/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12547",
        "id": 1335307444,
        "node_id": "PR_kwDOCVq1mM48_kTF",
        "number": 12547,
        "title": "replace 'master' branch ref to 'main' in the code",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-10T23:26:57Z",
        "updated_at": "2022-08-22T17:48:13Z",
        "closed_at": "2022-08-22T17:48:12Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12547",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12547",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12547.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12547.patch",
            "merged_at": "2022-08-22T17:48:12Z"
        },
        "body": "**Description**: Replace all occurences of the following:\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/master/\r\nhttps://github.com/microsoft/onnxruntime/tree/master/\r\n\r\ninto:\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/main/\r\n\r\nin the whole code base.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12547/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12547/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12548",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12548/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12548/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12548/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12548",
        "id": 1335316742,
        "node_id": "PR_kwDOCVq1mM48_mTH",
        "number": 12548,
        "title": "[js/web] update branch name for pull:wasm",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-10T23:42:58Z",
        "updated_at": "2022-08-12T22:46:37Z",
        "closed_at": "2022-08-12T22:46:37Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12548",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12548",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12548.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12548.patch",
            "merged_at": "2022-08-12T22:46:37Z"
        },
        "body": "**Description**: update branch name from 'master' to 'main' for script `npm run pull:wasm` (ONNX Runtime Web).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12548/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12548/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12549",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12549/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12549/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12549/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12549",
        "id": 1335357637,
        "node_id": "PR_kwDOCVq1mM48_u1Z",
        "number": 12549,
        "title": "Add big endian support to murmurhash3",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T01:08:22Z",
        "updated_at": "2022-08-11T08:39:41Z",
        "closed_at": "2022-08-11T08:39:40Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12549",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12549",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12549.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12549.patch",
            "merged_at": "2022-08-11T08:39:40Z"
        },
        "body": "**Description**: \r\nAdd big endian handling to the murmurhash3 implementation.\r\n\r\n**Motivation and Context**\r\nAddress #12504\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12549/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12549/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12550",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12550/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12550/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12550/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12550",
        "id": 1335369947,
        "node_id": "PR_kwDOCVq1mM48_xaj",
        "number": 12550,
        "title": "Replace references to onnxruntime 'master' with 'main' in Dockerfiles.",
        "user": {
            "login": "edgchen1",
            "id": 18449977,
            "node_id": "MDQ6VXNlcjE4NDQ5OTc3",
            "avatar_url": "https://avatars.githubusercontent.com/u/18449977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/edgchen1",
            "html_url": "https://github.com/edgchen1",
            "followers_url": "https://api.github.com/users/edgchen1/followers",
            "following_url": "https://api.github.com/users/edgchen1/following{/other_user}",
            "gists_url": "https://api.github.com/users/edgchen1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/edgchen1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/edgchen1/subscriptions",
            "organizations_url": "https://api.github.com/users/edgchen1/orgs",
            "repos_url": "https://api.github.com/users/edgchen1/repos",
            "events_url": "https://api.github.com/users/edgchen1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/edgchen1/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T01:34:58Z",
        "updated_at": "2022-08-16T21:13:06Z",
        "closed_at": "2022-08-16T21:13:06Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12550",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12550",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12550.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12550.patch",
            "merged_at": "2022-08-16T21:13:06Z"
        },
        "body": "**Description**\r\nReplace references to onnxruntime 'master' with 'main' in Dockerfiles.\r\n\r\n**Motivation and Context**\r\nFix some remaining references to 'master' after the branch got renamed.\r\n\r\nAddressed instances found with `git grep -n master -- $(git ls-files *Dockerfile*)`\r\nAlso fixed a reference in dockerfiles/README.md.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12550/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12550/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12551",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12551/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12551/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12551/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12551",
        "id": 1335417819,
        "node_id": "I_kwDOCVq1mM5PmNvb",
        "number": 12551,
        "title": "engine decryption does not work  in TensorRT EP",
        "user": {
            "login": "luojung",
            "id": 45228811,
            "node_id": "MDQ6VXNlcjQ1MjI4ODEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/45228811?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/luojung",
            "html_url": "https://github.com/luojung",
            "followers_url": "https://api.github.com/users/luojung/followers",
            "following_url": "https://api.github.com/users/luojung/following{/other_user}",
            "gists_url": "https://api.github.com/users/luojung/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/luojung/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/luojung/subscriptions",
            "organizations_url": "https://api.github.com/users/luojung/orgs",
            "repos_url": "https://api.github.com/users/luojung/repos",
            "events_url": "https://api.github.com/users/luojung/events{/privacy}",
            "received_events_url": "https://api.github.com/users/luojung/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 2204061391,
                "node_id": "MDU6TGFiZWwyMjA0MDYxMzkx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:TensorRT",
                "name": "ep:TensorRT",
                "color": "0052CC",
                "default": false,
                "description": "issues related to TensorRT execution provider"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-11T03:14:16Z",
        "updated_at": "2022-08-16T02:22:41Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\nI want to encrypt TensorRT engine in cache, so i set tensorrt_options->trt_engine_decryption_enable=ture and trt_engine_decryption_lib_path, but  the program always to deserialize from the unencrypted TensorRT engine, trt_engine_decryption_enable did not work.  \r\n\r\nThe program entered the decryption function after deleting the unencrypted TensorRT engine, but it did not found the file on disk.\r\n\r\nI have some questions when i read the following source code, why the file path of this encrypted engine is the same as the unencrypted engines.  It always does not enter the engine_decryption_ function when the engine_cache_path is valid.\r\n![image](https://user-images.githubusercontent.com/45228811/184057797-039ba61e-98c2-4795-9cd7-62518e9bb815.png)\r\n\r\n\r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- OS Platform and Distribution ( Linux Ubuntu 16.04):\r\n- ONNX Runtime installed from (source  ):\r\n- ONNX Runtime version: 1.12.1\r\n- Python version:\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (5.4):\r\n- CUDA/cuDNN version:8.4.1\r\n- GPU model and memory:\r\n\r\n**Expected behavior**\r\nI want to know why the file paths of the unencrypted TensorRT engine and the encrypted TensorRT engine are the same, and how to encrypt TensorRT engine.\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12551/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12551/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12552",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12552/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12552/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12552/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12552",
        "id": 1335425460,
        "node_id": "PR_kwDOCVq1mM48_9ES",
        "number": 12552,
        "title": "Fix Compile Warning",
        "user": {
            "login": "iK1D",
            "id": 11661208,
            "node_id": "MDQ6VXNlcjExNjYxMjA4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11661208?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/iK1D",
            "html_url": "https://github.com/iK1D",
            "followers_url": "https://api.github.com/users/iK1D/followers",
            "following_url": "https://api.github.com/users/iK1D/following{/other_user}",
            "gists_url": "https://api.github.com/users/iK1D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/iK1D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/iK1D/subscriptions",
            "organizations_url": "https://api.github.com/users/iK1D/orgs",
            "repos_url": "https://api.github.com/users/iK1D/repos",
            "events_url": "https://api.github.com/users/iK1D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/iK1D/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1913759001,
                "node_id": "MDU6TGFiZWwxOTEzNzU5MDAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/training",
                "name": "training",
                "color": "BFD4F2",
                "default": false,
                "description": "issues related to ONNX Runtime training; typically submitted using template"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T03:29:50Z",
        "updated_at": "2022-08-11T08:00:37Z",
        "closed_at": "2022-08-11T08:00:36Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12552",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12552",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12552.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12552.patch",
            "merged_at": "2022-08-11T08:00:36Z"
        },
        "body": "Fix compile warning introduced by PR https://github.com/microsoft/onnxruntime/pull/11803.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12552/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12552/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12553",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12553/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12553/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12553/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12553",
        "id": 1335457315,
        "node_id": "PR_kwDOCVq1mM49ADxJ",
        "number": 12553,
        "title": "Chenta/fix cnn failure",
        "user": {
            "login": "souptc",
            "id": 11306809,
            "node_id": "MDQ6VXNlcjExMzA2ODA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/11306809?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/souptc",
            "html_url": "https://github.com/souptc",
            "followers_url": "https://api.github.com/users/souptc/followers",
            "following_url": "https://api.github.com/users/souptc/following{/other_user}",
            "gists_url": "https://api.github.com/users/souptc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/souptc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/souptc/subscriptions",
            "organizations_url": "https://api.github.com/users/souptc/orgs",
            "repos_url": "https://api.github.com/users/souptc/repos",
            "events_url": "https://api.github.com/users/souptc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/souptc/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T04:27:27Z",
        "updated_at": "2022-08-11T16:58:26Z",
        "closed_at": "2022-08-11T16:58:26Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12553",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12553",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12553.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12553.patch",
            "merged_at": "2022-08-11T16:58:25Z"
        },
        "body": "**Description**: \r\nuse task counter to avoid hanging issue in training build.\r\nfix the model test failure related to copy tensor in reshape\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12553/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12553/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12554",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12554/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12554/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12554/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12554",
        "id": 1335480526,
        "node_id": "PR_kwDOCVq1mM49AImL",
        "number": 12554,
        "title": "update {ONNXRUNTIME_BRANCHE} to main",
        "user": {
            "login": "mszhanyi",
            "id": 16190118,
            "node_id": "MDQ6VXNlcjE2MTkwMTE4",
            "avatar_url": "https://avatars.githubusercontent.com/u/16190118?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mszhanyi",
            "html_url": "https://github.com/mszhanyi",
            "followers_url": "https://api.github.com/users/mszhanyi/followers",
            "following_url": "https://api.github.com/users/mszhanyi/following{/other_user}",
            "gists_url": "https://api.github.com/users/mszhanyi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mszhanyi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mszhanyi/subscriptions",
            "organizations_url": "https://api.github.com/users/mszhanyi/orgs",
            "repos_url": "https://api.github.com/users/mszhanyi/repos",
            "events_url": "https://api.github.com/users/mszhanyi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mszhanyi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T05:13:17Z",
        "updated_at": "2022-08-11T05:48:26Z",
        "closed_at": "2022-08-11T05:48:26Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12554",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12554",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12554.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12554.patch",
            "merged_at": null
        },
        "body": "**Description**: \r\nContinue to update master to main\r\n\r\n**Motivation and Context**\r\nFix errors like\r\n\r\n```\r\nCloning into 'onnxruntime'...\r\nwarning: Could not find remote branch master to clone.\r\n```\r\n\r\nhttps://aiinfra.visualstudio.com/Lotus/_build/results?buildId=223012&view=logs&j=b6bfa4e2-8141-507f-8ca1-59b3f929fa71&t=fc64e110-ab59-54e4-1c37-853e84a52a7e\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12554/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12554/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12555",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12555/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12555/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12555/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12555",
        "id": 1335681838,
        "node_id": "PR_kwDOCVq1mM49AzIX",
        "number": 12555,
        "title": "[xnnpack] Have `Initializer` in Mobile related EPs in Minimal_build and creating EP specific dynamic-schema",
        "user": {
            "login": "wejoncy",
            "id": 9417365,
            "node_id": "MDQ6VXNlcjk0MTczNjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9417365?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wejoncy",
            "html_url": "https://github.com/wejoncy",
            "followers_url": "https://api.github.com/users/wejoncy/followers",
            "following_url": "https://api.github.com/users/wejoncy/following{/other_user}",
            "gists_url": "https://api.github.com/users/wejoncy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wejoncy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wejoncy/subscriptions",
            "organizations_url": "https://api.github.com/users/wejoncy/orgs",
            "repos_url": "https://api.github.com/users/wejoncy/repos",
            "events_url": "https://api.github.com/users/wejoncy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wejoncy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2022-08-11T09:11:28Z",
        "updated_at": "2022-09-06T06:32:16Z",
        "closed_at": "2022-09-06T06:32:15Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12555",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12555",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12555.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12555.patch",
            "merged_at": "2022-09-06T06:32:15Z"
        },
        "body": "**Description**: Describe your changes.\r\n- In Mobile/Edge senario, we don't expect a heavy initializer with fp16/bf16 and mutator functions.  The current class implementation `Initializer` is heavy and unvaible in MINIMAL_BUILD mode.  In this PR,  We are gonna undef something and make class `Initializer ` availble.\r\n- Remove the dependence of Qlinearsoftmax schema in MSDomain, we will create it on the fly\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n    reduce binarysize and support minimal build.\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12555/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12555/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12556",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12556/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12556/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12556/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12556",
        "id": 1336027426,
        "node_id": "I_kwDOCVq1mM5Poiki",
        "number": 12556,
        "title": "Error when exluding nodes from quantization: \"ValueError: list.remove(x): x not in list\"",
        "user": {
            "login": "vvolhejn",
            "id": 8401624,
            "node_id": "MDQ6VXNlcjg0MDE2MjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8401624?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vvolhejn",
            "html_url": "https://github.com/vvolhejn",
            "followers_url": "https://api.github.com/users/vvolhejn/followers",
            "following_url": "https://api.github.com/users/vvolhejn/following{/other_user}",
            "gists_url": "https://api.github.com/users/vvolhejn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vvolhejn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vvolhejn/subscriptions",
            "organizations_url": "https://api.github.com/users/vvolhejn/orgs",
            "repos_url": "https://api.github.com/users/vvolhejn/repos",
            "events_url": "https://api.github.com/users/vvolhejn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vvolhejn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1607058914,
                "node_id": "MDU6TGFiZWwxNjA3MDU4OTE0",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/quantization",
                "name": "quantization",
                "color": "C2E0C6",
                "default": false,
                "description": "issues related to quantization"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T14:16:42Z",
        "updated_at": "2022-08-12T20:54:48Z",
        "closed_at": "2022-08-12T20:54:48Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "When quantizing a 1D CNN, I found that excluding normalization (layer norm followed by learnable scale and shift) from quantization improves the accuracy quite a bit. Then I tried to do the same with a slightly different network ([.onnx here](https://drive.google.com/file/d/1D7o5pRScm6eiWCbT35JIFTmN3w8wHU4B/view?usp=sharing)) but ORT crashed when quantizing this network.\r\n\r\n**Urgency**: A fast resolution would allow me to include the results in my Master's thesis (deadline 31.8.) but I can also not use this network.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 11 (bullseye)\r\n- ONNX Runtime installed from (source or binary): binary (nightly through pip)\r\n- ONNX Runtime version: 1.12.0.dev20220708003\r\n- Python version: 3.8.12\r\n- CUDA/cuDNN version: not relevant\r\n- GPU model and memory: not relevant\r\n\r\nI am using the nightly version because it fixes a different bug that I encountered earlier.\r\n\r\n**To Reproduce**\r\n\r\nThis code is not quite self-contained, but hopefully gives enough information:\r\n\r\n```python\r\nimport re\r\n\r\nimport onnx\r\nimport onnxruntime.quantization as ortq\r\n\r\nmodel = onnx.load(save_path)\r\n\r\nnodes = [n.name for n in model.graph.node]\r\nnormalization_nodes = [\r\n    n for n in nodes if re.search(r\"/normalize_?[0-9]*/\", n)\r\n]\r\n\r\nortq.quantize_static(\r\n    save_path,\r\n    output_path,\r\n    data_reader,\r\n    activation_type=ortq.QuantType.QInt8,\r\n    weight_type=ortq.QuantType.QInt8,\r\n    per_channel=True,\r\n    reduce_range=True,\r\n    quant_format=ortq.QuantFormat.QDQ,\r\n    extra_options={\r\n        \"WeightSymmetric\": True,\r\n        \"ActivationSymmetric\": False,\r\n        \"CalibMovingAverage\": True,\r\n    },\r\n    calibrate_method=ortq.CalibrationMethod.MinMax,\r\n    nodes_to_exclude=normalization_nodes,\r\n)\r\n```\r\n\r\n[.onnx available here.](https://drive.google.com/file/d/1D7o5pRScm6eiWCbT35JIFTmN3w8wHU4B/view?usp=sharing)\r\n\r\nI also tried digging into this a bit and found that the problem is in the \"add\" nodes: If I remove them from `nodes_to_exclude` (so that they *are* quantized), quantization doesn't crash. But this increases loss again, defeating the original purpose of excluding the nodes.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12556/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12556/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12557",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12557/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12557/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12557/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12557",
        "id": 1336167967,
        "node_id": "PR_kwDOCVq1mM49CbiF",
        "number": 12557,
        "title": "replace unordered_map and unordered_set with Inlinedxx for allocation_planner",
        "user": {
            "login": "jslhcl",
            "id": 1175624,
            "node_id": "MDQ6VXNlcjExNzU2MjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1175624?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jslhcl",
            "html_url": "https://github.com/jslhcl",
            "followers_url": "https://api.github.com/users/jslhcl/followers",
            "following_url": "https://api.github.com/users/jslhcl/following{/other_user}",
            "gists_url": "https://api.github.com/users/jslhcl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jslhcl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jslhcl/subscriptions",
            "organizations_url": "https://api.github.com/users/jslhcl/orgs",
            "repos_url": "https://api.github.com/users/jslhcl/repos",
            "events_url": "https://api.github.com/users/jslhcl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jslhcl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T15:58:14Z",
        "updated_at": "2022-08-11T19:32:05Z",
        "closed_at": "2022-08-11T19:32:04Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12557",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12557",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12557.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12557.patch",
            "merged_at": "2022-08-11T19:32:04Z"
        },
        "body": "**Description**: replace unordered_map and unordered_set with Inlinedxx for allocation_planner.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12557/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12557/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12558",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12558/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12558/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12558/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12558",
        "id": 1336176915,
        "node_id": "I_kwDOCVq1mM5PpHET",
        "number": 12558,
        "title": "Could not find an implementation for ConvInteger(10) node with name 'Conv_59_quant'",
        "user": {
            "login": "talhaanwarch",
            "id": 37379131,
            "node_id": "MDQ6VXNlcjM3Mzc5MTMx",
            "avatar_url": "https://avatars.githubusercontent.com/u/37379131?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/talhaanwarch",
            "html_url": "https://github.com/talhaanwarch",
            "followers_url": "https://api.github.com/users/talhaanwarch/followers",
            "following_url": "https://api.github.com/users/talhaanwarch/following{/other_user}",
            "gists_url": "https://api.github.com/users/talhaanwarch/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/talhaanwarch/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/talhaanwarch/subscriptions",
            "organizations_url": "https://api.github.com/users/talhaanwarch/orgs",
            "repos_url": "https://api.github.com/users/talhaanwarch/repos",
            "events_url": "https://api.github.com/users/talhaanwarch/events{/privacy}",
            "received_events_url": "https://api.github.com/users/talhaanwarch/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T16:05:16Z",
        "updated_at": "2022-08-11T18:26:56Z",
        "closed_at": "2022-08-11T18:26:55Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am trying to quantize slow_r50 model, but when i load it, I got this error\r\n```\r\n[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name 'Conv_59_quant'\r\n```\r\nCode to reproduce\r\n```\r\nclass OurModel(LightningModule):\r\n    def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        self.model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=False)\r\n    def forward(self,x):\r\n        return self.model(x)\r\n\r\nmodel=OurModel()\r\n\r\n\r\ninput_sample = torch.randn((1,3,30, 256,256))\r\nmodel.to_onnx('onnx.onnx', input_sample,opset_version=11, export_params=True)\r\n\r\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\r\nmodel_fp32 = 'onnx/{}.onnx'.format(vital)\r\nmodel_quant = 'onnx/{}-quantized.onnx'.format(vital)\r\nquantized_model = quantize_dynamic(model_fp32, model_quant)\r\n```\r\nFull Error\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplemented                            Traceback (most recent call last)\r\nInput In [88], in <cell line: 1>()\r\n----> 1 onnxruntime.InferenceSession('onnx/{}-quantized.onnx'.format(vital))\r\n\r\nFile ~/venv/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:347, in InferenceSession.__init__(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\r\n    344 disabled_optimizers = kwargs[\"disabled_optimizers\"] if \"disabled_optimizers\" in kwargs else None\r\n    346 try:\r\n--> 347     self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n    348 except ValueError:\r\n    349     if self._enable_fallback:\r\n\r\nFile ~/venv/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:395, in InferenceSession._create_inference_session(self, providers, provider_options, disabled_optimizers)\r\n    392     disabled_optimizers = set(disabled_optimizers)\r\n    394 # initialize the C++ InferenceSession\r\n--> 395 sess.initialize_session(providers, provider_options, disabled_optimizers)\r\n    397 self._sess = sess\r\n    398 self._sess_options = self._sess.session_options\r\n\r\nNotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name 'Conv_59_quant'\r\n```\r\nIs there any chance of getting it implemented.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12558/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12558/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12559",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12559/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12559/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12559/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12559",
        "id": 1336270683,
        "node_id": "PR_kwDOCVq1mM49CxBB",
        "number": 12559,
        "title": "Fix clang bug",
        "user": {
            "login": "illsilin",
            "id": 98187287,
            "node_id": "U_kgDOBdo4Fw",
            "avatar_url": "https://avatars.githubusercontent.com/u/98187287?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/illsilin",
            "html_url": "https://github.com/illsilin",
            "followers_url": "https://api.github.com/users/illsilin/followers",
            "following_url": "https://api.github.com/users/illsilin/following{/other_user}",
            "gists_url": "https://api.github.com/users/illsilin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/illsilin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/illsilin/subscriptions",
            "organizations_url": "https://api.github.com/users/illsilin/orgs",
            "repos_url": "https://api.github.com/users/illsilin/repos",
            "events_url": "https://api.github.com/users/illsilin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/illsilin/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2022-08-11T17:33:06Z",
        "updated_at": "2022-08-11T19:27:56Z",
        "closed_at": "2022-08-11T17:49:02Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12559",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12559",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12559.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12559.patch",
            "merged_at": null
        },
        "body": "This is just a fix for an error thrown by clang when using auto keyword.\r\nSame as PR #12483 in the microsoft/master repo.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12559/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12559/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12560",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12560/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12560/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12560/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12560",
        "id": 1336281496,
        "node_id": "PR_kwDOCVq1mM49CzU3",
        "number": 12560,
        "title": "Updating binary ops in eager mode to support broadcasting.",
        "user": {
            "login": "WilBrady",
            "id": 25513670,
            "node_id": "MDQ6VXNlcjI1NTEzNjcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/25513670?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/WilBrady",
            "html_url": "https://github.com/WilBrady",
            "followers_url": "https://api.github.com/users/WilBrady/followers",
            "following_url": "https://api.github.com/users/WilBrady/following{/other_user}",
            "gists_url": "https://api.github.com/users/WilBrady/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/WilBrady/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/WilBrady/subscriptions",
            "organizations_url": "https://api.github.com/users/WilBrady/orgs",
            "repos_url": "https://api.github.com/users/WilBrady/repos",
            "events_url": "https://api.github.com/users/WilBrady/events{/privacy}",
            "received_events_url": "https://api.github.com/users/WilBrady/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T17:43:58Z",
        "updated_at": "2022-08-11T21:00:13Z",
        "closed_at": "2022-08-11T21:00:12Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12560",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12560",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12560.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12560.patch",
            "merged_at": "2022-08-11T21:00:12Z"
        },
        "body": "**Description**: Add, sub, mul, and div all support broadcasting\r\n\r\n**Motivation and Context**\r\nThe current generated code assumes the output will be the same size as the first input matrix, which is not the case when the first matrix is used with broadcasting. This change copies a broadcast calculation function and then uses that in these ops which hanve been moved to handwritten. In the future the generator can be modified to handle this case.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12560/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12560/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12561",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12561/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12561/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12561/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12561",
        "id": 1336293914,
        "node_id": "PR_kwDOCVq1mM49C1-B",
        "number": 12561,
        "title": "Enhance constant folding for Shape node",
        "user": {
            "login": "pengwa",
            "id": 10530022,
            "node_id": "MDQ6VXNlcjEwNTMwMDIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/10530022?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pengwa",
            "html_url": "https://github.com/pengwa",
            "followers_url": "https://api.github.com/users/pengwa/followers",
            "following_url": "https://api.github.com/users/pengwa/following{/other_user}",
            "gists_url": "https://api.github.com/users/pengwa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pengwa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pengwa/subscriptions",
            "organizations_url": "https://api.github.com/users/pengwa/orgs",
            "repos_url": "https://api.github.com/users/pengwa/repos",
            "events_url": "https://api.github.com/users/pengwa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pengwa/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 2014185961,
                "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/core%20runtime",
                "name": "core runtime",
                "color": "006B75",
                "default": false,
                "description": "issues related to core runtime"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T17:56:31Z",
        "updated_at": "2022-08-24T07:42:44Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12561",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12561",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12561.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12561.patch",
            "merged_at": null
        },
        "body": "**Description**: Enhance constant folding for Shape node. \r\n\r\nIn ConstantFolding optimizer, if a Shape node's input shape have concrete values in all dimensions, then we replace this Shape node with an initializer filled with the concrete 1-D shape values. \r\n\r\nIn some 1P model, we see many subgraph related to shape operation.  The Shape node's input shape have some dimension being symbolic value, the others being concrete values. Those Shape nodes usually are followed by Slice or Gather node. \r\n\r\nIf the dimension Slice/Gather nodes want to retrieve is concrete values, we can also constant fold this Slice/Gather node. \r\n\r\nHere is the op count comparison without and with this PR:\r\n\r\n> ![image](https://user-images.githubusercontent.com/10530022/184225035-7d549efb-3c13-408f-91d2-715dd2196042.png)\r\n> ![image](https://user-images.githubusercontent.com/10530022/184225162-c3052477-8c96-43a8-af03-7596552d4f32.png)\r\n\r\n\r\nUnsqueeze: 982-->252\r\nSlice: 652->120\r\nSqueeze: 628->2\r\nGather: 563->367\r\nConcatTraining: 483->195\r\nClip: 96 -> 0\r\n\r\n\r\nAn interesting thing is, it reduced CUDA memory peak for few MB. :) \r\n\r\n> ![image](https://user-images.githubusercontent.com/10530022/184301473-c16138bc-a967-4a81-9ad7-2656413a12c1.png)\r\n\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12561/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12561/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12562",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12562/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12562/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12562/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12562",
        "id": 1336310963,
        "node_id": "PR_kwDOCVq1mM49C5Z2",
        "number": 12562,
        "title": "Create cpu-gpu-openvino-ort-inferencing.md",
        "user": {
            "login": "KevinH48264",
            "id": 33188761,
            "node_id": "MDQ6VXNlcjMzMTg4NzYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/33188761?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/KevinH48264",
            "html_url": "https://github.com/KevinH48264",
            "followers_url": "https://api.github.com/users/KevinH48264/followers",
            "following_url": "https://api.github.com/users/KevinH48264/following{/other_user}",
            "gists_url": "https://api.github.com/users/KevinH48264/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/KevinH48264/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/KevinH48264/subscriptions",
            "organizations_url": "https://api.github.com/users/KevinH48264/orgs",
            "repos_url": "https://api.github.com/users/KevinH48264/repos",
            "events_url": "https://api.github.com/users/KevinH48264/events{/privacy}",
            "received_events_url": "https://api.github.com/users/KevinH48264/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T18:08:33Z",
        "updated_at": "2022-08-11T22:31:25Z",
        "closed_at": "2022-08-11T18:37:37Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12562",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12562",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12562.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12562.patch",
            "merged_at": null
        },
        "body": "**Description**: Adding a tutorial for ORT Inferencing on CPU, GPU, and OpenVINO\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve? It teaches developers how to use ORT for inferencing on CPU, GPU, and OpenVINO\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12562/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12562/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12563",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12563/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12563/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12563/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12563",
        "id": 1336324295,
        "node_id": "PR_kwDOCVq1mM49C7ml",
        "number": 12563,
        "title": "Create cpu-gpu-openvino-ort-inferencing.md",
        "user": {
            "login": "KevinH48264",
            "id": 33188761,
            "node_id": "MDQ6VXNlcjMzMTg4NzYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/33188761?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/KevinH48264",
            "html_url": "https://github.com/KevinH48264",
            "followers_url": "https://api.github.com/users/KevinH48264/followers",
            "following_url": "https://api.github.com/users/KevinH48264/following{/other_user}",
            "gists_url": "https://api.github.com/users/KevinH48264/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/KevinH48264/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/KevinH48264/subscriptions",
            "organizations_url": "https://api.github.com/users/KevinH48264/orgs",
            "repos_url": "https://api.github.com/users/KevinH48264/repos",
            "events_url": "https://api.github.com/users/KevinH48264/events{/privacy}",
            "received_events_url": "https://api.github.com/users/KevinH48264/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T18:16:09Z",
        "updated_at": "2022-08-11T22:31:25Z",
        "closed_at": "2022-08-11T18:40:53Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12563",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12563",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12563.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12563.patch",
            "merged_at": null
        },
        "body": "**Description**: Adding a tutorial for ORT Inferencing on CPU, GPU, and OpenVINO\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve? It teaches developers how to use ORT for inferencing on CPU, GPU, and OpenVINO\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12563/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12563/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12564",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12564/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12564/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12564/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12564",
        "id": 1336330952,
        "node_id": "PR_kwDOCVq1mM49C83m",
        "number": 12564,
        "title": "Create flexible-hardware-inferencing.md",
        "user": {
            "login": "KevinH48264",
            "id": 33188761,
            "node_id": "MDQ6VXNlcjMzMTg4NzYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/33188761?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/KevinH48264",
            "html_url": "https://github.com/KevinH48264",
            "followers_url": "https://api.github.com/users/KevinH48264/followers",
            "following_url": "https://api.github.com/users/KevinH48264/following{/other_user}",
            "gists_url": "https://api.github.com/users/KevinH48264/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/KevinH48264/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/KevinH48264/subscriptions",
            "organizations_url": "https://api.github.com/users/KevinH48264/orgs",
            "repos_url": "https://api.github.com/users/KevinH48264/repos",
            "events_url": "https://api.github.com/users/KevinH48264/events{/privacy}",
            "received_events_url": "https://api.github.com/users/KevinH48264/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T18:21:18Z",
        "updated_at": "2022-08-11T22:31:26Z",
        "closed_at": "2022-08-11T18:36:50Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12564",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12564",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12564.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12564.patch",
            "merged_at": null
        },
        "body": "**Description**: Adding a tutorial for ORT Inferencing on CPU, GPU, and OpenVINO\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve? It teaches developers how to use ORT for inferencing on CPU, GPU, and OpenVINO\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12564/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12564/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12565",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12565/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12565/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12565/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12565",
        "id": 1336331911,
        "node_id": "PR_kwDOCVq1mM49C9E8",
        "number": 12565,
        "title": "do not quantize Relu/Clip if their inputs are not quantized",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T18:22:22Z",
        "updated_at": "2022-08-11T23:16:11Z",
        "closed_at": "2022-08-11T23:16:10Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12565",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12565",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12565.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12565.patch",
            "merged_at": "2022-08-11T23:16:10Z"
        },
        "body": "Fix bug: #12556.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12565/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12565/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12566",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12566/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12566/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12566/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12566",
        "id": 1336388809,
        "node_id": "PR_kwDOCVq1mM49DJZ1",
        "number": 12566,
        "title": "Add tutorial for Resnet cross platform inference",
        "user": {
            "login": "KevinH48264",
            "id": 33188761,
            "node_id": "MDQ6VXNlcjMzMTg4NzYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/33188761?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/KevinH48264",
            "html_url": "https://github.com/KevinH48264",
            "followers_url": "https://api.github.com/users/KevinH48264/followers",
            "following_url": "https://api.github.com/users/KevinH48264/following{/other_user}",
            "gists_url": "https://api.github.com/users/KevinH48264/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/KevinH48264/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/KevinH48264/subscriptions",
            "organizations_url": "https://api.github.com/users/KevinH48264/orgs",
            "repos_url": "https://api.github.com/users/KevinH48264/repos",
            "events_url": "https://api.github.com/users/KevinH48264/events{/privacy}",
            "received_events_url": "https://api.github.com/users/KevinH48264/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-11T19:22:03Z",
        "updated_at": "2022-08-16T20:27:39Z",
        "closed_at": "2022-08-16T20:27:39Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12566",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12566",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12566.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12566.patch",
            "merged_at": "2022-08-16T20:27:39Z"
        },
        "body": "Description: Adding a tutorial for ORT Inferencing on CPU, GPU, and OpenVINO\r\n\r\nMotivation and Context\r\nIt teaches developers how to use ORT for inferencing on CPU, GPU, and OpenVINO\r\n\r\nStaging link: https://natke.github.io/onnxruntime/\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12566/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12566/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12567",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12567/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12567/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12567/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12567",
        "id": 1336402345,
        "node_id": "I_kwDOCVq1mM5Pp-Gp",
        "number": 12567,
        "title": "Fix the eager generator to support broadcasting ops add, sub, mul, div",
        "user": {
            "login": "WilBrady",
            "id": 25513670,
            "node_id": "MDQ6VXNlcjI1NTEzNjcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/25513670?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/WilBrady",
            "html_url": "https://github.com/WilBrady",
            "followers_url": "https://api.github.com/users/WilBrady/followers",
            "following_url": "https://api.github.com/users/WilBrady/following{/other_user}",
            "gists_url": "https://api.github.com/users/WilBrady/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/WilBrady/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/WilBrady/subscriptions",
            "organizations_url": "https://api.github.com/users/WilBrady/orgs",
            "repos_url": "https://api.github.com/users/WilBrady/repos",
            "events_url": "https://api.github.com/users/WilBrady/events{/privacy}",
            "received_events_url": "https://api.github.com/users/WilBrady/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1913759001,
                "node_id": "MDU6TGFiZWwxOTEzNzU5MDAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/training",
                "name": "training",
                "color": "BFD4F2",
                "default": false,
                "description": "issues related to ONNX Runtime training; typically submitted using template"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T19:35:09Z",
        "updated_at": "2022-08-15T18:00:00Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "body": "**Is your feature request related to a problem? Please describe.**\r\nRecently it was discovered that broadcasting failed for `add` op using the ort. The `add` op and `add_` and `add.out` all use `add.out` under the covers which was a recent expansion on eager ops support. However, this requires the out tensor be resized correctly and these binary ops support broadcasting, so the final broadcast output size needs to be calculated. As the generator doesn't support that now and this limitation was blocking a customer, these ops were handwritten - see https://github.com/microsoft/onnxruntime/pull/12560\r\n\r\nThe request is to update the generator to support these additional lines when generating these ops:\r\n```\r\nonnxruntime::TensorShape out_shape;\r\n  resize_output(\r\n      invoker,\r\n      dynamic_cast<ORTTensorImpl*>(out.unsafeGetTensorImpl()),\r\n      BroadcastShape(__func__, ort_input_0_self, ort_input_0_other, out_shape));\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12567/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12567/timeline",
        "performed_via_github_app": null,
        "state_reason": "reopened"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12568",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12568/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12568/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12568/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12568",
        "id": 1336461148,
        "node_id": "PR_kwDOCVq1mM49DYW6",
        "number": 12568,
        "title": "DirectML GEMM broken in opset 11 and 13 when optional tensor C not provided",
        "user": {
            "login": "smk2007",
            "id": 6754002,
            "node_id": "MDQ6VXNlcjY3NTQwMDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6754002?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/smk2007",
            "html_url": "https://github.com/smk2007",
            "followers_url": "https://api.github.com/users/smk2007/followers",
            "following_url": "https://api.github.com/users/smk2007/following{/other_user}",
            "gists_url": "https://api.github.com/users/smk2007/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/smk2007/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/smk2007/subscriptions",
            "organizations_url": "https://api.github.com/users/smk2007/orgs",
            "repos_url": "https://api.github.com/users/smk2007/repos",
            "events_url": "https://api.github.com/users/smk2007/events{/privacy}",
            "received_events_url": "https://api.github.com/users/smk2007/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T20:30:49Z",
        "updated_at": "2022-08-11T23:01:28Z",
        "closed_at": "2022-08-11T23:01:27Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12568",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12568",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12568.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12568.patch",
            "merged_at": "2022-08-11T23:01:27Z"
        },
        "body": "DirectML GEMM broken in opset 11 and 13 when optional tensor C not provided\r\n\r\nIn opset 11, GEMM's input C became optional. This was not accounted for in DML.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12568/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12568/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12569",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12569/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12569/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12569/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12569",
        "id": 1336529878,
        "node_id": "PR_kwDOCVq1mM49DnNY",
        "number": 12569,
        "title": "replace unordered_map and set wiht Inlinedxxx",
        "user": {
            "login": "jslhcl",
            "id": 1175624,
            "node_id": "MDQ6VXNlcjExNzU2MjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1175624?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jslhcl",
            "html_url": "https://github.com/jslhcl",
            "followers_url": "https://api.github.com/users/jslhcl/followers",
            "following_url": "https://api.github.com/users/jslhcl/following{/other_user}",
            "gists_url": "https://api.github.com/users/jslhcl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jslhcl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jslhcl/subscriptions",
            "organizations_url": "https://api.github.com/users/jslhcl/orgs",
            "repos_url": "https://api.github.com/users/jslhcl/repos",
            "events_url": "https://api.github.com/users/jslhcl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jslhcl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-11T21:47:05Z",
        "updated_at": "2022-08-12T19:48:41Z",
        "closed_at": "2022-08-12T19:48:40Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12569",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12569",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12569.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12569.patch",
            "merged_at": "2022-08-12T19:48:40Z"
        },
        "body": "**Description**: replace unordered_map and set wiht Inlinedxxx.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12569/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12569/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12570",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12570/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12570/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12570/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12570",
        "id": 1336538545,
        "node_id": "PR_kwDOCVq1mM49Do9h",
        "number": 12570,
        "title": "Add TRT uint8 support",
        "user": {
            "login": "kevinch-nv",
            "id": 45886021,
            "node_id": "MDQ6VXNlcjQ1ODg2MDIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/45886021?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kevinch-nv",
            "html_url": "https://github.com/kevinch-nv",
            "followers_url": "https://api.github.com/users/kevinch-nv/followers",
            "following_url": "https://api.github.com/users/kevinch-nv/following{/other_user}",
            "gists_url": "https://api.github.com/users/kevinch-nv/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kevinch-nv/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kevinch-nv/subscriptions",
            "organizations_url": "https://api.github.com/users/kevinch-nv/orgs",
            "repos_url": "https://api.github.com/users/kevinch-nv/repos",
            "events_url": "https://api.github.com/users/kevinch-nv/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kevinch-nv/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 13,
        "created_at": "2022-08-11T21:58:46Z",
        "updated_at": "2022-08-15T15:22:51Z",
        "closed_at": "2022-08-15T15:22:51Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12570",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12570",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12570.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12570.patch",
            "merged_at": "2022-08-15T15:22:51Z"
        },
        "body": "Signed-off-by: Kevin Chen <kevinch@nvidia.com>\r\n\r\n**Description**: Add UINT8 data support in TensorRT EP\r\n\r\n**Motivation and Context**\r\n- TRT EP will support UINT8 in its next release. Add UINT8 handling here to support it.\r\n- Previous versions of TRT will filter out UINT8 nodes in getCapability(), so it should not affect existing tests",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12570/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12570/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12571",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12571/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12571/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12571/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12571",
        "id": 1336616122,
        "node_id": "PR_kwDOCVq1mM49D5cp",
        "number": 12571,
        "title": "optionalGetElement and OptionalHasElement to support tensor and seque",
        "user": {
            "login": "liqunfu",
            "id": 3318051,
            "node_id": "MDQ6VXNlcjMzMTgwNTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3318051?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/liqunfu",
            "html_url": "https://github.com/liqunfu",
            "followers_url": "https://api.github.com/users/liqunfu/followers",
            "following_url": "https://api.github.com/users/liqunfu/following{/other_user}",
            "gists_url": "https://api.github.com/users/liqunfu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/liqunfu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/liqunfu/subscriptions",
            "organizations_url": "https://api.github.com/users/liqunfu/orgs",
            "repos_url": "https://api.github.com/users/liqunfu/repos",
            "events_url": "https://api.github.com/users/liqunfu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/liqunfu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-12T00:20:02Z",
        "updated_at": "2022-08-31T23:35:42Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12571",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12571",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12571.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12571.patch",
            "merged_at": null
        },
        "body": "**Description**: There is an update in ONNX for OptionalHasElement and OptionalGetElement. The 2 ops now accept tensor type as well. OptionalHasElement also treat it input as static optional, meaning if the input is not provided, return false.\r\n\r\n**Motivation and Context**\r\nupdate ORT kernel to support the new behavior.\r\n\r\nImportant note: change in ONNX has not in main branch yet so related build/test are likely to fail. I will update the ONNX commit once the change is in ONNX main branch. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12571/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12571/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12572",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12572/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12572/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12572/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12572",
        "id": 1336632344,
        "node_id": "PR_kwDOCVq1mM49D8w7",
        "number": 12572,
        "title": "Create mobile-web-superres.md",
        "user": {
            "login": "kspear18",
            "id": 106196300,
            "node_id": "U_kgDOBlRtTA",
            "avatar_url": "https://avatars.githubusercontent.com/u/106196300?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kspear18",
            "html_url": "https://github.com/kspear18",
            "followers_url": "https://api.github.com/users/kspear18/followers",
            "following_url": "https://api.github.com/users/kspear18/following{/other_user}",
            "gists_url": "https://api.github.com/users/kspear18/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kspear18/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kspear18/subscriptions",
            "organizations_url": "https://api.github.com/users/kspear18/orgs",
            "repos_url": "https://api.github.com/users/kspear18/repos",
            "events_url": "https://api.github.com/users/kspear18/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kspear18/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T00:52:55Z",
        "updated_at": "2022-08-12T00:52:55Z",
        "closed_at": null,
        "author_association": "FIRST_TIME_CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12572",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12572",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12572.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12572.patch",
            "merged_at": null
        },
        "body": "Add a new super resolution example tutorial\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12572/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12572/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12573",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12573/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12573/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12573/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12573",
        "id": 1336814891,
        "node_id": "I_kwDOCVq1mM5Pri0r",
        "number": 12573,
        "title": "Add execution provider selection for quantize_static",
        "user": {
            "login": "prikmm",
            "id": 47216475,
            "node_id": "MDQ6VXNlcjQ3MjE2NDc1",
            "avatar_url": "https://avatars.githubusercontent.com/u/47216475?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/prikmm",
            "html_url": "https://github.com/prikmm",
            "followers_url": "https://api.github.com/users/prikmm/followers",
            "following_url": "https://api.github.com/users/prikmm/following{/other_user}",
            "gists_url": "https://api.github.com/users/prikmm/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/prikmm/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/prikmm/subscriptions",
            "organizations_url": "https://api.github.com/users/prikmm/orgs",
            "repos_url": "https://api.github.com/users/prikmm/repos",
            "events_url": "https://api.github.com/users/prikmm/events{/privacy}",
            "received_events_url": "https://api.github.com/users/prikmm/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1607058914,
                "node_id": "MDU6TGFiZWwxNjA3MDU4OTE0",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/quantization",
                "name": "quantization",
                "color": "C2E0C6",
                "default": false,
                "description": "issues related to quantization"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-12T06:39:17Z",
        "updated_at": "2022-08-13T07:01:27Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Is your feature request related to a problem? Please describe.**\r\nI am currently working on adding ORT quantization to a physics research experiment and am curious to know why this option is not provided. Quantization currently takes place using the default CPU Execution Provider (default execution provider used for creating an inference session in `CalibrateBase`) irrespective of the execution provider to be used in deployment.\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): 1.11.0\r\n\r\n**Describe the solution you'd like**\r\nRefer https://github.com/microsoft/onnxruntime/discussions/12543\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12573/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12573/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12574",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12574/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12574/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12574/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12574",
        "id": 1336874118,
        "node_id": "PR_kwDOCVq1mM49Evq3",
        "number": 12574,
        "title": "Qlinearsoftmax take FLOAT lookup-table",
        "user": {
            "login": "wejoncy",
            "id": 9417365,
            "node_id": "MDQ6VXNlcjk0MTczNjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9417365?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wejoncy",
            "html_url": "https://github.com/wejoncy",
            "followers_url": "https://api.github.com/users/wejoncy/followers",
            "following_url": "https://api.github.com/users/wejoncy/following{/other_user}",
            "gists_url": "https://api.github.com/users/wejoncy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wejoncy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wejoncy/subscriptions",
            "organizations_url": "https://api.github.com/users/wejoncy/orgs",
            "repos_url": "https://api.github.com/users/wejoncy/repos",
            "events_url": "https://api.github.com/users/wejoncy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wejoncy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T07:50:13Z",
        "updated_at": "2022-08-18T01:54:40Z",
        "closed_at": "2022-08-18T01:54:39Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12574",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12574",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12574.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12574.patch",
            "merged_at": "2022-08-18T01:54:39Z"
        },
        "body": "**Description**: Describe your changes.\r\nUse `float` loopup_table for qlinearsoftmax \r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12574/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12574/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12575",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12575/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12575/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12575/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12575",
        "id": 1337028432,
        "node_id": "I_kwDOCVq1mM5PsW9Q",
        "number": 12575,
        "title": "Missing `onnx` dependency when running cli",
        "user": {
            "login": "reinvantveer",
            "id": 9550733,
            "node_id": "MDQ6VXNlcjk1NTA3MzM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9550733?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/reinvantveer",
            "html_url": "https://github.com/reinvantveer",
            "followers_url": "https://api.github.com/users/reinvantveer/followers",
            "following_url": "https://api.github.com/users/reinvantveer/following{/other_user}",
            "gists_url": "https://api.github.com/users/reinvantveer/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/reinvantveer/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/reinvantveer/subscriptions",
            "organizations_url": "https://api.github.com/users/reinvantveer/orgs",
            "repos_url": "https://api.github.com/users/reinvantveer/repos",
            "events_url": "https://api.github.com/users/reinvantveer/events{/privacy}",
            "received_events_url": "https://api.github.com/users/reinvantveer/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-12T10:25:54Z",
        "updated_at": "2022-08-12T18:09:09Z",
        "closed_at": "2022-08-12T18:09:09Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "To reproduce:\r\n```\r\npipenv install onnxruntime  # creates a new virtualenv with only onnxruntime\r\npipenv run python -m onnxruntime.tools.check_onnx_model_mobile_usability --help\r\n```\r\nOutput: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/path/to/your/virtualenv/lib/python3.9/site-packages/onnxruntime/tools/check_onnx_model_mobile_usability.py\", line 11, in <module>\r\n    from .mobile_helpers import check_model_can_use_ort_mobile_pkg, usability_checker  # noqa\r\n  File \"/path/to/your/virtualenv/lib/python3.9/site-packages/onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py\", line 12, in <module>\r\n    import onnx\r\nModuleNotFoundError: No module named 'onnx'\r\n```\r\nBelieve this dependency should be added to `requirements.txt`",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12575/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12575/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12576",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12576/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12576/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12576/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12576",
        "id": 1337231656,
        "node_id": "I_kwDOCVq1mM5PtIko",
        "number": 12576,
        "title": "Potential updates on `ONNXModel.save_model_to_file` of quantization and optimization",
        "user": {
            "login": "JingyaHuang",
            "id": 44135271,
            "node_id": "MDQ6VXNlcjQ0MTM1Mjcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/44135271?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JingyaHuang",
            "html_url": "https://github.com/JingyaHuang",
            "followers_url": "https://api.github.com/users/JingyaHuang/followers",
            "following_url": "https://api.github.com/users/JingyaHuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/JingyaHuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JingyaHuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JingyaHuang/subscriptions",
            "organizations_url": "https://api.github.com/users/JingyaHuang/orgs",
            "repos_url": "https://api.github.com/users/JingyaHuang/repos",
            "events_url": "https://api.github.com/users/JingyaHuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JingyaHuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1607058914,
                "node_id": "MDU6TGFiZWwxNjA3MDU4OTE0",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/quantization",
                "name": "quantization",
                "color": "C2E0C6",
                "default": false,
                "description": "issues related to quantization"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-12T13:48:05Z",
        "updated_at": "2022-08-22T13:50:06Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "## Context\r\n\r\nHi ORT team, recently we got some issues from the Optimum side about exporting large proto after graph optimization or quantization. We found that unlike the [`save_model_to_file` for the optimization](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/onnx_model.py), for the quantization `save_model_to_file` export the external tensors on the local working path\r\nhttps://github.com/microsoft/onnxruntime/blob/24eab921bee4dbf9002942fbc514b2b2d9ee3a64/onnxruntime/python/tools/quantization/onnx_model.py#L342\r\ninstead of the path where I store the model proto\r\nhttps://github.com/microsoft/onnxruntime/blob/24eab921bee4dbf9002942fbc514b2b2d9ee3a64/onnxruntime/python/tools/quantization/onnx_model.py#L344\r\n\r\nIn terms of user experience, it seems to be quite confusing, would it make sense to put the external files in the same directory as what is done for the optimization?\r\n\r\nBesides, ONNX now provides [more variables](https://github.com/onnx/onnx/blob/main/onnx/__init__.py#L188) including `all_tensors_to_one_file`(already added for the optimization but not for the quantization), `size_threshold` and `convert_attribute` to help export large proto, is there any plan from ONNX Runtime side to integrate those extra flags?\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): 1.12.0\r\n- ONNX version: 1.12.0\r\n- Optimum version: built from src\r\n\r\n**Describe the solution you'd like**\r\n\r\nExport directly with \r\n```python\r\nOnnxModel.save(self.model, output_path, use_external_data_format)\r\n```\r\ninstead of treating `use_external_data_format` separately.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nDefine the external file path the same as the model proto\r\n\r\n**Additional context**\r\n\r\nI can help to submit a PR for it.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12576/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12576/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12577",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12577/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12577/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12577/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12577",
        "id": 1337613954,
        "node_id": "I_kwDOCVq1mM5Pul6C",
        "number": 12577,
        "title": "Resize op generates invalid WebGL code",
        "user": {
            "login": "praeclarum",
            "id": 323548,
            "node_id": "MDQ6VXNlcjMyMzU0OA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/323548?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/praeclarum",
            "html_url": "https://github.com/praeclarum",
            "followers_url": "https://api.github.com/users/praeclarum/followers",
            "following_url": "https://api.github.com/users/praeclarum/following{/other_user}",
            "gists_url": "https://api.github.com/users/praeclarum/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/praeclarum/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/praeclarum/subscriptions",
            "organizations_url": "https://api.github.com/users/praeclarum/orgs",
            "repos_url": "https://api.github.com/users/praeclarum/repos",
            "events_url": "https://api.github.com/users/praeclarum/events{/privacy}",
            "received_events_url": "https://api.github.com/users/praeclarum/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3066979818,
                "node_id": "MDU6TGFiZWwzMDY2OTc5ODE4",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:web",
                "name": "platform:web",
                "color": "FEF2C0",
                "default": false,
                "description": "issues related to ONNX Runtime web; typically submitted using template"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T19:04:17Z",
        "updated_at": "2022-08-22T04:47:29Z",
        "closed_at": "2022-08-22T04:47:29Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\n\r\nWhen executing a Torch model using ort.js, I get the following error:\r\n\r\n```\r\n[Error] WebGL: ERROR: 0:66: '0.03125.0' : invalid number\r\n\tcompileShader\r\n\tcompileShader (ort.js:20494)\r\n\tcompile (ort.js:19290)\r\n\t(anonymous function) (ort.js:19269)\r\n\tevent (ort.js:21902)\r\n\texecuteProgram (ort.js:14046)\r\n\trun (ort.js:14053)\r\n\tresize (ort.js:17520)\r\n\tasyncFunctionResume\r\n\tevent (ort.js:21902)\r\n\t(anonymous function) (ort.js:20970)\r\n\tasyncFunctionResume\r\n\t(anonymous function)\r\n\tpromiseReactionJobWithoutPromise\r\n```\r\n\r\nIt seems to be caused by this invalid line of generated fragment shader code:\r\n\r\n```\r\nconst vec2 inputWH = vec2(256.0, 128.0);\r\nconst vec4 scaleWHWH = vec4(0.03125.0, 0.03125.0, 0.03125.0, 0.03125.0);\r\n```\r\n\r\nThat `scaleWHWH` definition causes a syntax error which causes the model to fail.\r\n\r\n**Urgency**\r\n\r\nThis blocks all my models that use Upsample modules or interpolate functions.\r\n\r\n**System information**\r\n- OS Platform and Distribution: 22.04\r\n- ONNX Runtime installed from: https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js\r\n\r\n**To Reproduce**\r\n\r\n```python\r\nsegmentation = torch.nn.functional.interpolate(segmentation, size=x.size()[2:], mode=\"bilinear\", align_corners=True)\r\n```\r\n\r\n**Expected behavior**\r\n\r\nResize happens.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12577/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12577/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12578",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12578/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12578/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12578/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12578",
        "id": 1337614778,
        "node_id": "PR_kwDOCVq1mM49HO_v",
        "number": 12578,
        "title": "[Automated]: Update Java API docs",
        "user": {
            "login": "github-actions[bot]",
            "id": 41898282,
            "node_id": "MDM6Qm90NDE4OTgyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/15368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/github-actions%5Bbot%5D",
            "html_url": "https://github.com/apps/github-actions",
            "followers_url": "https://api.github.com/users/github-actions%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/github-actions%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/github-actions%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/github-actions%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/github-actions%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T19:05:19Z",
        "updated_at": "2022-08-12T19:55:23Z",
        "closed_at": "2022-08-12T19:55:23Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12578",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12578",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12578.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12578.patch",
            "merged_at": "2022-08-12T19:55:23Z"
        },
        "body": "Automated changes by [create-pull-request](https://github.com/peter-evans/create-pull-request) GitHub action",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12578/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12578/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12579",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12579/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12579/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12579/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12579",
        "id": 1337677143,
        "node_id": "I_kwDOCVq1mM5Pu1VX",
        "number": 12579,
        "title": "Failed to find kernel for SimplifiedLayerNormalization(1) (node SimplifiedLayerNormalization). ",
        "user": {
            "login": "dashesy",
            "id": 873905,
            "node_id": "MDQ6VXNlcjg3MzkwNQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/873905?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dashesy",
            "html_url": "https://github.com/dashesy",
            "followers_url": "https://api.github.com/users/dashesy/followers",
            "following_url": "https://api.github.com/users/dashesy/following{/other_user}",
            "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions",
            "organizations_url": "https://api.github.com/users/dashesy/orgs",
            "repos_url": "https://api.github.com/users/dashesy/repos",
            "events_url": "https://api.github.com/users/dashesy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dashesy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1974915201,
                "node_id": "MDU6TGFiZWwxOTc0OTE1MjAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/model:transformer",
                "name": "model:transformer",
                "color": "4EF6CD",
                "default": false,
                "description": "issues related to a transformer model: BERT, GPT2, Hugging Face, Longformer, T5, etc."
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-12T20:30:18Z",
        "updated_at": "2022-09-10T01:12:53Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\nI have a model that runs fine with ORT 1.10 (both under CPU and GPU) but fails to run with ORT 1.12 under CPU\r\n\r\nThe model does not even have the `SimplifiedLayerNormalization` node, mentioned in the error message. I think the optimization adds the node (few transformers) but then it cannot run it because the tensor type is fp16!\r\n\r\n```text\r\nRuntimeOptions: { ExecutionMode: Sequential, IntraOpNumThreads: 0, InterOpNumThreads: 0, EnableMemoryPattern: True, EnableCpuMemArena: True, GraphOptimizationLevel: EnableAll, EnableProfiling: False, ProfileOutputPathPrefix: onnxruntime_profile_, MaxParallelism: 40, ExecutionProvider: Cpu, DeviceId: 0 }\r\nMicrosoft.ML.OnnxRuntime.OnnxRuntimeException: [ErrorCode:NotImplemented] Failed to find kernel for SimplifiedLayerNormalization(1) (node SimplifiedLayerNormalization). Op with name (SimplifiedLayerNormalization) and type (SimplifiedLayerNormalization) kernel is not supported in CPUExecutionProvider. Encountered following errors: (Found kernel for Op with name (SimplifiedLayerNormalization) and type (SimplifiedLayerNormalization) in the supported version range (node_version: 1 kernel start version: 1 kernel_end_version: 2147483647). However the types are incompatible. This op has been implemented only for the following types (tensor(float),), but the node in the model has the following type (tensor(float16))\r\nFound kernel for Op with name (SimplifiedLayerNormalization) and type (SimplifiedLayerNormalization) in the supported version range (node_version: 1 kernel start version: 1 kernel_end_version: 2147483647). However the types are incompatible. This op has been implemented only for the following types (tensor(double),), but the node in the model has the following type (tensor(float16))\r\n)\r\n```\r\n\r\n**Urgency**\r\nWe have this model with 1.10 released. But I need to update to 1.12 because of some other new ops in a new combined model\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- ONNX Runtime installed from (source or binary): nuget Microsoft.ML.OnnxRuntime.Gpu\r\n- ONNX Runtime version: 1.12\r\n- Visual Studio version (if applicable): 2019\r\n\r\n**Expected behavior**\r\nIf optimizer adds a node the model should run\r\n\r\n**Additional context**\r\nThe model is intended to run with GPU. But it should also run fine on CPU in order for unit tests to pass.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12579/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12579/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12580",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12580/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12580/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12580/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12580",
        "id": 1337705694,
        "node_id": "PR_kwDOCVq1mM49Hhpv",
        "number": 12580,
        "title": "ONNX Protobuf natvis with some google::protobuf",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-12T21:05:19Z",
        "updated_at": "2022-08-15T16:59:08Z",
        "closed_at": "2022-08-15T16:59:07Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12580",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12580",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12580.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12580.patch",
            "merged_at": "2022-08-15T16:59:07Z"
        },
        "body": "**Description**: \r\nAdd custom natvis visualizers for better debugging in VS Studio and VS Code\r\n\r\n![image](https://user-images.githubusercontent.com/11303988/184455000-1e5b8738-c1e1-4341-b4e4-c76c3ffb3b4d.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/11303988/184454873-b8ec2727-4e48-47eb-b46f-40a220b0d3e1.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12580/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12580/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12581",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12581/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12581/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12581/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12581",
        "id": 1337715852,
        "node_id": "I_kwDOCVq1mM5Pu-yM",
        "number": 12581,
        "title": "request `Ort::InitApi()` be `noexcept`",
        "user": {
            "login": "diablodale",
            "id": 679350,
            "node_id": "MDQ6VXNlcjY3OTM1MA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/679350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/diablodale",
            "html_url": "https://github.com/diablodale",
            "followers_url": "https://api.github.com/users/diablodale/followers",
            "following_url": "https://api.github.com/users/diablodale/following{/other_user}",
            "gists_url": "https://api.github.com/users/diablodale/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/diablodale/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/diablodale/subscriptions",
            "organizations_url": "https://api.github.com/users/diablodale/orgs",
            "repos_url": "https://api.github.com/users/diablodale/repos",
            "events_url": "https://api.github.com/users/diablodale/events{/privacy}",
            "received_events_url": "https://api.github.com/users/diablodale/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1122493981,
                "node_id": "MDU6TGFiZWwxMTIyNDkzOTgx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/feature%20request",
                "name": "feature request",
                "color": "5319E7",
                "default": false,
                "description": "request for unsupported feature or enhancement"
            },
            {
                "id": 4419012097,
                "node_id": "LA_kwDOCVq1mM8AAAABB2TGAQ",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api",
                "name": "api",
                "color": "E67B8C",
                "default": false,
                "description": "issues related to all other APIs: C, C++, Python, etc."
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T21:21:43Z",
        "updated_at": "2022-08-12T21:31:40Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "In ORT v1.12.0 the `Ort::InitApi()` is not defined `noexcept`.\r\n\r\nI request it be `noexcept` so that it can be used easily in all code blocks including default constructors (which should be themselves noexcept and therefore cascades down).\r\n\r\nBoth `ORT_API_MANUAL_INIT` codepaths are already exception safe today in their code\r\n\r\n### Setup\r\n\r\n* `<package id=\"Microsoft.ML.OnnxRuntime.DirectML\" version=\"1.12.0\" />`\r\n* VS2019 community v16.11.18\r\n\r\n### Recommended code\r\n\r\nin `packages\\Microsoft.ML.OnnxRuntime.DirectML.1.12.0\\build\\native\\include\\onnxruntime_cxx_api.h`\r\n```c++\r\ninline void InitApi() { Global<void>::api_ = OrtGetApiBase()->GetApi(ORT_API_VERSION); }\r\n// to\r\ninline void InitApi() NO_EXCEPTION { Global<void>::api_ = OrtGetApiBase()->GetApi(ORT_API_VERSION); }\r\n```\r\n\r\n### Workaround\r\n\r\n* private build\r\n* try/catch wrapping",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12581/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12581/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12582",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12582/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12582/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12582/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12582",
        "id": 1337718278,
        "node_id": "PR_kwDOCVq1mM49HkWb",
        "number": 12582,
        "title": "Introduce ordered quantization ops for the CUDA EP [1/n]",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 10,
        "created_at": "2022-08-12T21:25:27Z",
        "updated_at": "2022-09-07T18:58:16Z",
        "closed_at": "2022-09-07T18:58:15Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12582",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12582",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12582.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12582.patch",
            "merged_at": "2022-09-07T18:58:15Z"
        },
        "body": "**Description**:  This PR introduces the first set of schemas, kernels, and tests for ordered quantization ops for use in the CUDA EP. The kernels were mostly developed by @zhanghuanrong with some minor enhancements / tests contributed across the team (true for upcoming similar PRs)\r\n\r\nThe fusion logic to fuse QDQ subgraphs into ordered ops will be contributed to the transformer optimization tools suite once the kernels have been merged (https://github.com/microsoft/onnxruntime/pull/12661)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12582/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12582/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12583",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12583/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12583/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12583/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12583",
        "id": 1337733559,
        "node_id": "PR_kwDOCVq1mM49Hnlv",
        "number": 12583,
        "title": "build break",
        "user": {
            "login": "jslhcl",
            "id": 1175624,
            "node_id": "MDQ6VXNlcjExNzU2MjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1175624?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jslhcl",
            "html_url": "https://github.com/jslhcl",
            "followers_url": "https://api.github.com/users/jslhcl/followers",
            "following_url": "https://api.github.com/users/jslhcl/following{/other_user}",
            "gists_url": "https://api.github.com/users/jslhcl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jslhcl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jslhcl/subscriptions",
            "organizations_url": "https://api.github.com/users/jslhcl/orgs",
            "repos_url": "https://api.github.com/users/jslhcl/repos",
            "events_url": "https://api.github.com/users/jslhcl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jslhcl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T21:51:35Z",
        "updated_at": "2022-08-12T22:03:23Z",
        "closed_at": "2022-08-12T22:03:22Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12583",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12583",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12583.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12583.patch",
            "merged_at": "2022-08-12T22:03:22Z"
        },
        "body": "**Description**: build break\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12583/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12583/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12584",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12584/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12584/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12584/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12584",
        "id": 1337735749,
        "node_id": "I_kwDOCVq1mM5PvDpF",
        "number": 12584,
        "title": "Document beamsearch ",
        "user": {
            "login": "dashesy",
            "id": 873905,
            "node_id": "MDQ6VXNlcjg3MzkwNQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/873905?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dashesy",
            "html_url": "https://github.com/dashesy",
            "followers_url": "https://api.github.com/users/dashesy/followers",
            "following_url": "https://api.github.com/users/dashesy/following{/other_user}",
            "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions",
            "organizations_url": "https://api.github.com/users/dashesy/orgs",
            "repos_url": "https://api.github.com/users/dashesy/repos",
            "events_url": "https://api.github.com/users/dashesy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dashesy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1974915201,
                "node_id": "MDU6TGFiZWwxOTc0OTE1MjAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/model:transformer",
                "name": "model:transformer",
                "color": "4EF6CD",
                "default": false,
                "description": "issues related to a transformer model: BERT, GPT2, Hugging Face, Longformer, T5, etc."
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2022-08-12T21:55:46Z",
        "updated_at": "2022-08-16T18:39:03Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Is your feature request related to a problem? Please describe.**\r\nI currently run the encoder ONNX, get features then prepare things like `input_ids` and pass to another decoder ONNX multiple times.\r\nThis process is not very efficient. Specially if the models run on GPU and we have to go to CPU for some tensors and back to GPU again.\r\n\r\nI tried combining them but it runs into cyclical issues and very large models.\r\nI just realized there is some \"BeamSearch\" [support in the kernels](https://github.com/microsoft/onnxruntime/blob/main/docs/OperatorKernels.md) and some \"conver_beam_search.py\" in some commits.\r\n\r\nIt would be nice if `conver_beam_search.py` could work with any 2 encder-decoder models. The models I have are neither GPT, nor T5.  \r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): 1.10\r\n\r\n**Describe the solution you'd like**\r\nGeneral method to combine encoder-decoder and do a beam-search. As long as encoder returns a specific tensor, and decoder accepts specific tensors this should be a generic operation.\r\n\r\nIt would be nice if the conversion is smart enough to reuse decoding history\r\n\r\n**Describe alternatives you've considered**\r\nI use two ONNX files and do autoregression outside ONNX\r\n\r\n**Additional context**\r\nI am interested in Vision-Language (VL) models. These models are SOTA, and some have specific architectures that do convert to ONNX but are not standard or well known \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12584/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12584/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12585",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12585/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12585/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12585/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12585",
        "id": 1337752418,
        "node_id": "PR_kwDOCVq1mM49HroZ",
        "number": 12585,
        "title": "fix build break",
        "user": {
            "login": "jslhcl",
            "id": 1175624,
            "node_id": "MDQ6VXNlcjExNzU2MjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1175624?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jslhcl",
            "html_url": "https://github.com/jslhcl",
            "followers_url": "https://api.github.com/users/jslhcl/followers",
            "following_url": "https://api.github.com/users/jslhcl/following{/other_user}",
            "gists_url": "https://api.github.com/users/jslhcl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jslhcl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jslhcl/subscriptions",
            "organizations_url": "https://api.github.com/users/jslhcl/orgs",
            "repos_url": "https://api.github.com/users/jslhcl/repos",
            "events_url": "https://api.github.com/users/jslhcl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jslhcl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T22:26:08Z",
        "updated_at": "2022-08-12T22:26:45Z",
        "closed_at": "2022-08-12T22:26:44Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12585",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12585",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12585.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12585.patch",
            "merged_at": "2022-08-12T22:26:44Z"
        },
        "body": "**Description**: Fix build break.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12585/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12585/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12586",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12586/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12586/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12586/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12586",
        "id": 1337761765,
        "node_id": "PR_kwDOCVq1mM49Htlg",
        "number": 12586,
        "title": "[Automated]: Update Java API docs",
        "user": {
            "login": "github-actions[bot]",
            "id": 41898282,
            "node_id": "MDM6Qm90NDE4OTgyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/15368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/github-actions%5Bbot%5D",
            "html_url": "https://github.com/apps/github-actions",
            "followers_url": "https://api.github.com/users/github-actions%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/github-actions%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/github-actions%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/github-actions%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/github-actions%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T22:47:54Z",
        "updated_at": "2022-08-15T20:31:55Z",
        "closed_at": "2022-08-15T20:31:54Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12586",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12586",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12586.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12586.patch",
            "merged_at": "2022-08-15T20:31:54Z"
        },
        "body": "Automated changes by [create-pull-request](https://github.com/peter-evans/create-pull-request) GitHub action",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12586/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12586/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12587",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12587/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12587/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12587/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12587",
        "id": 1337763543,
        "node_id": "PR_kwDOCVq1mM49Ht9L",
        "number": 12587,
        "title": "orttaining build break",
        "user": {
            "login": "jslhcl",
            "id": 1175624,
            "node_id": "MDQ6VXNlcjExNzU2MjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1175624?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jslhcl",
            "html_url": "https://github.com/jslhcl",
            "followers_url": "https://api.github.com/users/jslhcl/followers",
            "following_url": "https://api.github.com/users/jslhcl/following{/other_user}",
            "gists_url": "https://api.github.com/users/jslhcl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jslhcl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jslhcl/subscriptions",
            "organizations_url": "https://api.github.com/users/jslhcl/orgs",
            "repos_url": "https://api.github.com/users/jslhcl/repos",
            "events_url": "https://api.github.com/users/jslhcl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jslhcl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T22:52:45Z",
        "updated_at": "2022-08-12T22:53:16Z",
        "closed_at": "2022-08-12T22:53:15Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12587",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12587",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12587.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12587.patch",
            "merged_at": "2022-08-12T22:53:15Z"
        },
        "body": "**Description**: orttaining build break\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12587/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12587/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12588",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12588/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12588/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12588/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12588",
        "id": 1337768305,
        "node_id": "PR_kwDOCVq1mM49Hu-s",
        "number": 12588,
        "title": "[js/web] fix incorrect shader for 'Resize'",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-12T23:04:16Z",
        "updated_at": "2022-08-22T04:47:29Z",
        "closed_at": "2022-08-22T04:47:29Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12588",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12588",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12588.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12588.patch",
            "merged_at": "2022-08-22T04:47:28Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12588/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12588/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12589",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12589/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12589/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12589/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12589",
        "id": 1337823563,
        "node_id": "I_kwDOCVq1mM5PvZFL",
        "number": 12589,
        "title": "Initializing inference session on worker thread for web applications using onnxruntime-web",
        "user": {
            "login": "jackylu0124",
            "id": 28615340,
            "node_id": "MDQ6VXNlcjI4NjE1MzQw",
            "avatar_url": "https://avatars.githubusercontent.com/u/28615340?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jackylu0124",
            "html_url": "https://github.com/jackylu0124",
            "followers_url": "https://api.github.com/users/jackylu0124/followers",
            "following_url": "https://api.github.com/users/jackylu0124/following{/other_user}",
            "gists_url": "https://api.github.com/users/jackylu0124/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jackylu0124/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jackylu0124/subscriptions",
            "organizations_url": "https://api.github.com/users/jackylu0124/orgs",
            "repos_url": "https://api.github.com/users/jackylu0124/repos",
            "events_url": "https://api.github.com/users/jackylu0124/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jackylu0124/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3066979818,
                "node_id": "MDU6TGFiZWwzMDY2OTc5ODE4",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:web",
                "name": "platform:web",
                "color": "FEF2C0",
                "default": false,
                "description": "issues related to ONNX Runtime web; typically submitted using template"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "fs-eire",
                "id": 7679871,
                "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
                "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/fs-eire",
                "html_url": "https://github.com/fs-eire",
                "followers_url": "https://api.github.com/users/fs-eire/followers",
                "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
                "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
                "organizations_url": "https://api.github.com/users/fs-eire/orgs",
                "repos_url": "https://api.github.com/users/fs-eire/repos",
                "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
                "received_events_url": "https://api.github.com/users/fs-eire/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-13T01:51:13Z",
        "updated_at": "2022-08-15T20:27:11Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Describe the bug\r\nI am trying to create an inference session with the \r\n\r\n`create(uri: string, options?: SessionOptions: Promise<InferenceSessionInferenceSession)>` \r\n\r\nfunction in a worker ([https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers)) script, but the inference session creation function gave me the `Uncaught (in promise) Error: Can't create a session` error (see screenshot below).\r\n![image](https://user-images.githubusercontent.com/28615340/184463438-1ed937fb-ca29-4d05-8e9e-941f3a984b77.png)\r\n\r\nFor the time being, I am able to read in the `.onnx` file into an `ArrayBuffer` on the main thread and then transfer the `ArrayBuffer` representing the onnx model to the worker thread, and was able to create the inference session on the worker thread with the transferred `ArrayBuffer` and also run inference successfully. However, I am still interested in knowing if it's possible to create an inference session directly with a given file path on the worker thread.\r\n\r\n### System information\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- ONNX Runtime version: `\"onnxruntime-web\": \"^1.12.1\"` (in package.json)\r\n\r\n### Additional context\r\n**Note that is an `onnxruntime-web` issue, not an `onnxruntime-react-native` issue.**\r\n\r\n### To Reproduce\r\nLink to repo: [https://github.com/jackylu0124/onnxruntime-web-worker-initialization-issue](https://github.com/jackylu0124/onnxruntime-web-worker-initialization-issue)\r\n\r\nI have commented out the line that emits the error message since my alternative way mentioned earlier can both create the inference session and also peform inference. To reproduce the error message, look for the comment starting with `// NOTE:` in `inferenceWorker.ts` and uncomment the line below that line of comment. For convenience, the link to that line is: [https://github.com/jackylu0124/onnxruntime-web-worker-initialization-issue/blob/f64c0df784a1c0de0b0ac3199a7d405226ffa610/web-worker-001/src/workers/inferenceWorker.ts#L4](https://github.com/jackylu0124/onnxruntime-web-worker-initialization-issue/blob/f64c0df784a1c0de0b0ac3199a7d405226ffa610/web-worker-001/src/workers/inferenceWorker.ts#L4)\r\n\r\n**Steps to run the code:**\r\n1. Uncomment the line at [https://github.com/jackylu0124/onnxruntime-web-worker-initialization-issue/blob/f64c0df784a1c0de0b0ac3199a7d405226ffa610/web-worker-001/src/workers/inferenceWorker.ts#L4](https://github.com/jackylu0124/onnxruntime-web-worker-initialization-issue/blob/f64c0df784a1c0de0b0ac3199a7d405226ffa610/web-worker-001/src/workers/inferenceWorker.ts#L4).\r\n2. `cd` into the `web-worker-001` folder.\r\n3. `npm install` and then `npm start` in the terminal.\r\n4. Open DevTools in the browser and see the error message.\r\n\r\n### Expected behavior\r\nThe inference session should be able to be created with the file path argument on the worker thread.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12589/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12589/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12590",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12590/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12590/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12590/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12590",
        "id": 1337849963,
        "node_id": "PR_kwDOCVq1mM49H_xV",
        "number": 12590,
        "title": "fix training pipeline model tests",
        "user": {
            "login": "souptc",
            "id": 11306809,
            "node_id": "MDQ6VXNlcjExMzA2ODA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/11306809?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/souptc",
            "html_url": "https://github.com/souptc",
            "followers_url": "https://api.github.com/users/souptc/followers",
            "following_url": "https://api.github.com/users/souptc/following{/other_user}",
            "gists_url": "https://api.github.com/users/souptc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/souptc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/souptc/subscriptions",
            "organizations_url": "https://api.github.com/users/souptc/orgs",
            "repos_url": "https://api.github.com/users/souptc/repos",
            "events_url": "https://api.github.com/users/souptc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/souptc/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-13T04:21:14Z",
        "updated_at": "2022-08-13T04:22:07Z",
        "closed_at": "2022-08-13T04:22:07Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12590",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12590",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12590.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12590.patch",
            "merged_at": "2022-08-13T04:22:07Z"
        },
        "body": "**Description**: fix training pipeline's model tests.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12590/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12590/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12591",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12591/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12591/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12591/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12591",
        "id": 1337906475,
        "node_id": "I_kwDOCVq1mM5PvtUr",
        "number": 12591,
        "title": "Bumps [terser](https://github.com/terser/terser) from 5.14.0 to 5.14.2.",
        "user": {
            "login": "Amadeuy",
            "id": 107383181,
            "node_id": "U_kgDOBmaJjQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/107383181?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Amadeuy",
            "html_url": "https://github.com/Amadeuy",
            "followers_url": "https://api.github.com/users/Amadeuy/followers",
            "following_url": "https://api.github.com/users/Amadeuy/following{/other_user}",
            "gists_url": "https://api.github.com/users/Amadeuy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Amadeuy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Amadeuy/subscriptions",
            "organizations_url": "https://api.github.com/users/Amadeuy/orgs",
            "repos_url": "https://api.github.com/users/Amadeuy/repos",
            "events_url": "https://api.github.com/users/Amadeuy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Amadeuy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-13T08:50:15Z",
        "updated_at": "2022-08-13T08:51:50Z",
        "closed_at": "2022-08-13T08:51:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Bumps [terser](https://github.com/terser/terser) from 5.14.0 to 5.14.2.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/terser/terser/blob/master/CHANGELOG.md\">terser's changelog</a>.</em></p>\n<blockquote>\n<h2>v5.14.2</h2>\n<ul>\n<li>Security fix for RegExps that should not be evaluated (regexp DDOS)</li>\n<li>Source maps improvements (<a href=\"https://github-redirect.dependabot.com/terser/terser/issues/1211\">#1211</a>)</li>\n<li>Performance improvements in long property access evaluation (<a href=\"https://github-redirect.dependabot.com/terser/terser/issues/1213\">#1213</a>)</li>\n</ul>\n<h2>v5.14.1</h2>\n<ul>\n<li>keep_numbers option added to TypeScript defs (<a href=\"https://github-redirect.dependabot.com/terser/terser/issues/1208\">#1208</a>)</li>\n<li>Fixed parsing of nested template strings (<a href=\"https://github-redirect.dependabot.com/terser/terser/issues/1204\">#1204</a>)</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/terser/terser/commits\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=terser&package-manager=npm_and_yarn&previous-version=5.14.0&new-version=5.14.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/Amadeuy/typescript-eslint/network/alerts).\n\n</details>\n\n__Originally posted by @dependabot in https://github.com/Amadeuy/typescript-eslint/pull/9__",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12591/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12591/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12592",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12592/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12592/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12592/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12592",
        "id": 1338582385,
        "node_id": "I_kwDOCVq1mM5PySVx",
        "number": 12592,
        "title": "Export to multiple files with torch.onnx.export and can't load them in .NET Microsoft.ML.OnnxRuntime.",
        "user": {
            "login": "zhaoxh16",
            "id": 23635588,
            "node_id": "MDQ6VXNlcjIzNjM1NTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/23635588?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zhaoxh16",
            "html_url": "https://github.com/zhaoxh16",
            "followers_url": "https://api.github.com/users/zhaoxh16/followers",
            "following_url": "https://api.github.com/users/zhaoxh16/following{/other_user}",
            "gists_url": "https://api.github.com/users/zhaoxh16/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zhaoxh16/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zhaoxh16/subscriptions",
            "organizations_url": "https://api.github.com/users/zhaoxh16/orgs",
            "repos_url": "https://api.github.com/users/zhaoxh16/repos",
            "events_url": "https://api.github.com/users/zhaoxh16/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zhaoxh16/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2022-08-15T06:13:09Z",
        "updated_at": "2022-08-17T00:49:54Z",
        "closed_at": "2022-08-17T00:49:54Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\nI was trying to export a MBart model with some modifications using torch.onnx.export, and it exported the model to multiple files named with numbers and layer names. I could load them with python or .NET OnnxRuntime when all the files are put in the root of working directory, but I couldn't load them when they were put in another directory, although the model file \"MBART.onnx\" was in the same directory with other files.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Export model on Ubuntu 20.04.4, load in .NET on windows 10.\r\n- ONNX Runtime installed from (source or binary): nuget\r\n- ONNX Runtime version: 1.11.0\r\n- Python version: 3.8\r\n- Visual Studio version (if applicable): 2022\r\n- CUDA/cuDNN version: use CPU\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Model:\r\n```\r\n\"\"\" PyTorch MBART model.\"\"\"\r\nimport copy\r\nimport math\r\nimport random\r\nfrom typing import List, Optional, Tuple, Union\r\n\r\nimport torch\r\nimport torch.utils.checkpoint\r\nfrom torch import nn\r\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\r\n\r\nfrom transformers.activations import ACT2FN\r\nfrom transformers.modeling_outputs import (\r\n\tBaseModelOutput,\r\n\tBaseModelOutputWithPastAndCrossAttentions,\r\n\tCausalLMOutputWithCrossAttentions,\r\n\tSeq2SeqLMOutput,\r\n\tSeq2SeqModelOutput,\r\n\tSeq2SeqQuestionAnsweringModelOutput,\r\n\tSeq2SeqSequenceClassifierOutput,\r\n)\r\nfrom transformers.modeling_utils import PreTrainedModel\r\nfrom transformers.utils import (\r\n\tadd_code_sample_docstrings,\r\n\tadd_end_docstrings,\r\n\tadd_start_docstrings,\r\n\tadd_start_docstrings_to_model_forward,\r\n\tlogging,\r\n\treplace_return_docstrings,\r\n)\r\nfrom transformers.models.mbart.configuration_mbart import MBartConfig\r\n\r\n\r\nlogger = logging.get_logger(__name__)\r\n\r\n_CHECKPOINT_FOR_DOC = \"facebook/mbart-large-cc25\"\r\n_CONFIG_FOR_DOC = \"MBartConfig\"\r\n_TOKENIZER_FOR_DOC = \"MBartTokenizer\"\r\n\r\n# Base model docstring\r\n_EXPECTED_OUTPUT_SHAPE = [1, 8, 1024]\r\n\r\n# SequenceClassification docstring\r\n_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"hf-internal-testing/tiny-random-mbart\"\r\n_SEQ_CLASS_EXPECTED_LOSS = 0.69\r\n_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_1'\"\r\n\r\n# QuestionAsnwering docstring\r\n_CHECKPOINT_FOR_QA = \"hf-internal-testing/tiny-random-mbart\"\r\n_QA_EXPECTED_LOSS = 3.55\r\n_QA_EXPECTED_OUTPUT = \"'? Jim Henson was a'\"\r\n\r\n\r\nMBART_PRETRAINED_MODEL_ARCHIVE_LIST = [\r\n\t\"facebook/mbart-large-cc25\",\r\n\t# See all MBART models at https://huggingface.co/models?filter=mbart\r\n]\r\n\r\n\r\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\r\n\t\"\"\"\r\n\tShift input ids one token to the right, and wrap the last non pad token (the <LID> token) Note that MBart does not\r\n\thave a single `decoder_start_token_id` in contrast to other Bart-like models.\r\n\t\"\"\"\r\n\tprev_output_tokens = input_ids.clone()\r\n\r\n\tif pad_token_id is None:\r\n\t\traise ValueError(\"self.model.config.pad_token_id has to be defined.\")\r\n\t# replace possible -100 values in labels by `pad_token_id`\r\n\tprev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\r\n\r\n\tindex_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\r\n\tdecoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\r\n\tprev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\r\n\tprev_output_tokens[:, 0] = decoder_start_tokens\r\n\r\n\treturn prev_output_tokens\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\r\ndef _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\r\n\t\"\"\"\r\n\tMake causal mask used for bi-directional self-attention.\r\n\t\"\"\"\r\n\tbsz, tgt_len = input_ids_shape\r\n\tmask = torch.full((tgt_len, tgt_len), torch.tensor(float(\"-inf\")))\r\n\tmask_cond = torch.arange(mask.size(-1))\r\n\tmask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\r\n\tmask = mask.to(dtype)\r\n\r\n\tif past_key_values_length > 0:\r\n\t\tmask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\r\n\treturn mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart._expand_mask\r\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\r\n\t\"\"\"\r\n\tExpands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\r\n\t\"\"\"\r\n\tbsz, src_len = mask.size()\r\n\ttgt_len = tgt_len if tgt_len is not None else src_len\r\n\r\n\texpanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\r\n\r\n\tinverted_mask = 1.0 - expanded_mask\r\n\r\n\treturn inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->MBart\r\nclass MBartLearnedPositionalEmbedding(nn.Embedding):\r\n\t\"\"\"\r\n\tThis module learns positional embeddings up to a fixed maximum size.\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, num_embeddings: int, embedding_dim: int):\r\n\t\t# MBart is set up so that if padding_idx is specified then offset the embedding ids by 2\r\n\t\t# and adjust num_embeddings appropriately. Other models don't have this hack\r\n\t\tself.offset = 2\r\n\t\tsuper().__init__(num_embeddings + self.offset, embedding_dim)\r\n\r\n\tdef forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\r\n\t\t\"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\r\n\t\tbsz, seq_len = input_ids_shape[:2]\r\n\t\tpositions = torch.arange(\r\n\t\t\tpast_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\r\n\t\t)\r\n\t\treturn super().forward(positions + self.offset)\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->MBart\r\nclass MBartAttention(nn.Module):\r\n\t\"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\r\n\r\n\tdef __init__(\r\n\t\tself,\r\n\t\tembed_dim: int,\r\n\t\tnum_heads: int,\r\n\t\tdropout: float = 0.0,\r\n\t\tis_decoder: bool = False,\r\n\t\tbias: bool = True,\r\n\t):\r\n\t\tsuper().__init__()\r\n\t\tself.embed_dim = embed_dim\r\n\t\tself.num_heads = num_heads\r\n\t\tself.dropout = dropout\r\n\t\tself.head_dim = embed_dim // num_heads\r\n\r\n\t\tif (self.head_dim * num_heads) != self.embed_dim:\r\n\t\t\traise ValueError(\r\n\t\t\t\tf\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\r\n\t\t\t\tf\" and `num_heads`: {num_heads}).\"\r\n\t\t\t)\r\n\t\tself.scaling = self.head_dim**-0.5\r\n\t\tself.is_decoder = is_decoder\r\n\r\n\t\tself.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\r\n\t\tself.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\r\n\t\tself.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\r\n\t\tself.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\r\n\r\n\tdef _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\r\n\t\treturn tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\r\n\r\n\tdef forward(\r\n\t\tself,\r\n\t\thidden_states: torch.Tensor,\r\n\t\tkey_value_states: Optional[torch.Tensor] = None,\r\n\t\tpast_key_value: Optional[Tuple[torch.Tensor]] = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\tlayer_head_mask: Optional[torch.Tensor] = None,\r\n\t\toutput_attentions: bool = False,\r\n\t) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\r\n\t\t\"\"\"Input shape: Batch x Time x Channel\"\"\"\r\n\r\n\t\t# if key_value_states are provided this layer is used as a cross-attention layer\r\n\t\t# for the decoder\r\n\t\tis_cross_attention = key_value_states is not None\r\n\r\n\t\tbsz, tgt_len, _ = hidden_states.size()\r\n\r\n\t\t# get query proj\r\n\t\tquery_states = self.q_proj(hidden_states) * self.scaling\r\n\t\t# get key, value proj\r\n\t\tif is_cross_attention and past_key_value is not None:\r\n\t\t\t# reuse k,v, cross_attentions\r\n\t\t\tkey_states = past_key_value[0]\r\n\t\t\tvalue_states = past_key_value[1]\r\n\t\telif is_cross_attention:\r\n\t\t\t# cross_attentions\r\n\t\t\tkey_states = self._shape(self.k_proj(key_value_states), -1, bsz)\r\n\t\t\tvalue_states = self._shape(self.v_proj(key_value_states), -1, bsz)\r\n\t\telif past_key_value is not None:\r\n\t\t\t# reuse k, v, self_attention\r\n\t\t\tkey_states = self._shape(self.k_proj(hidden_states), -1, bsz)\r\n\t\t\tvalue_states = self._shape(self.v_proj(hidden_states), -1, bsz)\r\n\t\t\tkey_states = torch.cat([past_key_value[0], key_states], dim=2)\r\n\t\t\tvalue_states = torch.cat([past_key_value[1], value_states], dim=2)\r\n\t\telse:\r\n\t\t\t# self_attention\r\n\t\t\tkey_states = self._shape(self.k_proj(hidden_states), -1, bsz)\r\n\t\t\tvalue_states = self._shape(self.v_proj(hidden_states), -1, bsz)\r\n\r\n\t\tif self.is_decoder:\r\n\t\t\t# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\r\n\t\t\t# Further calls to cross_attention layer can then reuse all cross-attention\r\n\t\t\t# key/value_states (first \"if\" case)\r\n\t\t\t# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\r\n\t\t\t# all previous decoder key/value_states. Further calls to uni-directional self-attention\r\n\t\t\t# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\r\n\t\t\t# if encoder bi-directional self-attention `past_key_value` is always `None`\r\n\t\t\tpast_key_value = (key_states, value_states)\r\n\r\n\t\tproj_shape = (bsz * self.num_heads, -1, self.head_dim)\r\n\t\tquery_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\r\n\t\tkey_states = key_states.view(*proj_shape)\r\n\t\tvalue_states = value_states.view(*proj_shape)\r\n\r\n\t\tsrc_len = key_states.size(1)\r\n\t\tattn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\r\n\r\n\t\tif attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\r\n\t\t\traise ValueError(\r\n\t\t\t\tf\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\r\n\t\t\t\tf\" {attn_weights.size()}\"\r\n\t\t\t)\r\n\r\n\t\tif attention_mask is not None:\r\n\t\t\tif attention_mask.size() != (bsz, 1, tgt_len, src_len):\r\n\t\t\t\traise ValueError(\r\n\t\t\t\t\tf\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\r\n\t\t\t\t)\r\n\t\t\tattn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\r\n\t\t\tattn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\r\n\r\n\t\tattn_weights = nn.functional.softmax(attn_weights, dim=-1)\r\n\r\n\t\tif layer_head_mask is not None:\r\n\t\t\tif layer_head_mask.size() != (self.num_heads,):\r\n\t\t\t\traise ValueError(\r\n\t\t\t\t\tf\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\r\n\t\t\t\t\tf\" {layer_head_mask.size()}\"\r\n\t\t\t\t)\r\n\t\t\tattn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\r\n\t\t\tattn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\r\n\r\n\t\tif output_attentions:\r\n\t\t\t# this operation is a bit awkward, but it's required to\r\n\t\t\t# make sure that attn_weights keeps its gradient.\r\n\t\t\t# In order to do so, attn_weights have to be reshaped\r\n\t\t\t# twice and have to be reused in the following\r\n\t\t\tattn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\r\n\t\t\tattn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\r\n\t\telse:\r\n\t\t\tattn_weights_reshaped = None\r\n\r\n\t\tattn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\r\n\r\n\t\tattn_output = torch.bmm(attn_probs, value_states)\r\n\r\n\t\tif attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\r\n\t\t\traise ValueError(\r\n\t\t\t\tf\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\r\n\t\t\t\tf\" {attn_output.size()}\"\r\n\t\t\t)\r\n\r\n\t\tattn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\r\n\t\tattn_output = attn_output.transpose(1, 2)\r\n\r\n\t\t# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\r\n\t\t# partitioned aross GPUs when using tensor-parallelism.\r\n\t\tattn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\r\n\r\n\t\tattn_output = self.out_proj(attn_output)\r\n\r\n\t\treturn attn_output, attn_weights_reshaped, past_key_value\r\n\r\n\r\nclass MBartEncoderLayer(nn.Module):\r\n\tdef __init__(self, config: MBartConfig):\r\n\t\tsuper().__init__()\r\n\t\tself.embed_dim = config.d_model\r\n\t\tself.self_attn = MBartAttention(\r\n\t\t\tembed_dim=self.embed_dim,\r\n\t\t\tnum_heads=config.encoder_attention_heads,\r\n\t\t\tdropout=config.attention_dropout,\r\n\t\t)\r\n\t\tself.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\r\n\t\tself.dropout = config.dropout\r\n\t\tself.activation_fn = ACT2FN[config.activation_function]\r\n\t\tself.activation_dropout = config.activation_dropout\r\n\t\tself.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\r\n\t\tself.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\r\n\t\tself.final_layer_norm = nn.LayerNorm(self.embed_dim)\r\n\r\n\tdef forward(\r\n\t\tself,\r\n\t\thidden_states: torch.Tensor,\r\n\t\tattention_mask: torch.Tensor,\r\n\t\tlayer_head_mask: torch.Tensor,\r\n\t\toutput_attentions: bool = False,\r\n\t) -> torch.Tensor:\r\n\t\t\"\"\"\r\n\t\tArgs:\r\n\t\t\thidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\r\n\t\t\tattention_mask (`torch.FloatTensor`): attention mask of size\r\n\t\t\t\t*(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\r\n\t\t\tlayer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\r\n\t\t\t\t*(encoder_attention_heads,)*.\r\n\t\t\toutput_attentions (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the attentions tensors of all attention layers. See `attentions` under\r\n\t\t\t\treturned tensors for more detail.\r\n\t\t\"\"\"\r\n\t\tresidual = hidden_states\r\n\t\thidden_states = self.self_attn_layer_norm(hidden_states)\r\n\t\thidden_states, attn_weights, _ = self.self_attn(\r\n\t\t\thidden_states=hidden_states,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\tlayer_head_mask=layer_head_mask,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t)\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\t\thidden_states = residual + hidden_states\r\n\r\n\t\tresidual = hidden_states\r\n\t\thidden_states = self.final_layer_norm(hidden_states)\r\n\t\thidden_states = self.activation_fn(self.fc1(hidden_states))\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\r\n\t\thidden_states = self.fc2(hidden_states)\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\t\thidden_states = residual + hidden_states\r\n\r\n\t\tif hidden_states.dtype == torch.float16 and (\r\n\t\t\ttorch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\r\n\t\t):\r\n\t\t\tclamp_value = torch.finfo(hidden_states.dtype).max - 1000\r\n\t\t\thidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\r\n\r\n\t\toutputs = (hidden_states,)\r\n\r\n\t\tif output_attentions:\r\n\t\t\toutputs += (attn_weights,)\r\n\r\n\t\treturn outputs\r\n\r\n\r\nclass MBartDecoderLayer(nn.Module):\r\n\tdef __init__(self, config: MBartConfig):\r\n\t\tsuper().__init__()\r\n\t\tself.embed_dim = config.d_model\r\n\r\n\t\tself.self_attn = MBartAttention(\r\n\t\t\tembed_dim=self.embed_dim,\r\n\t\t\tnum_heads=config.decoder_attention_heads,\r\n\t\t\tdropout=config.attention_dropout,\r\n\t\t\tis_decoder=True,\r\n\t\t)\r\n\t\tself.dropout = config.dropout\r\n\t\tself.activation_fn = ACT2FN[config.activation_function]\r\n\t\tself.activation_dropout = config.activation_dropout\r\n\r\n\t\tself.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\r\n\t\tself.encoder_attn = MBartAttention(\r\n\t\t\tself.embed_dim,\r\n\t\t\tconfig.decoder_attention_heads,\r\n\t\t\tdropout=config.attention_dropout,\r\n\t\t\tis_decoder=True,\r\n\t\t)\r\n\t\tself.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\r\n\t\tself.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\r\n\t\tself.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\r\n\t\tself.final_layer_norm = nn.LayerNorm(self.embed_dim)\r\n\r\n\tdef forward(\r\n\t\tself,\r\n\t\thidden_states: torch.Tensor,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_hidden_states: Optional[torch.Tensor] = None,\r\n\t\tencoder_attention_mask: Optional[torch.Tensor] = None,\r\n\t\tlayer_head_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_layer_head_mask: Optional[torch.Tensor] = None,\r\n\t\tpast_key_value: Optional[Tuple[torch.Tensor]] = None,\r\n\t\toutput_attentions: Optional[bool] = False,\r\n\t\tuse_cache: Optional[bool] = True,\r\n\t) -> torch.Tensor:\r\n\t\t\"\"\"\r\n\t\tArgs:\r\n\t\t\thidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\r\n\t\t\tattention_mask (`torch.FloatTensor`): attention mask of size\r\n\t\t\t\t*(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\r\n\t\t\tencoder_hidden_states (`torch.FloatTensor`):\r\n\t\t\t\tcross attention input to the layer of shape *(seq_len, batch, embed_dim)*\r\n\t\t\tencoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\r\n\t\t\t\t*(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\r\n\t\t\tlayer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\r\n\t\t\t\t*(encoder_attention_heads,)*.\r\n\t\t\tcross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\r\n\t\t\t\tsize *(decoder_attention_heads,)*.\r\n\t\t\tpast_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\r\n\t\t\toutput_attentions (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the attentions tensors of all attention layers. See `attentions` under\r\n\t\t\t\treturned tensors for more detail.\r\n\t\t\"\"\"\r\n\t\tresidual = hidden_states\r\n\t\thidden_states = self.self_attn_layer_norm(hidden_states)\r\n\r\n\t\t# Self Attention\r\n\t\t# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\r\n\t\tself_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\r\n\t\t# add present self-attn cache to positions 1,2 of present_key_value tuple\r\n\t\thidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n\t\t\thidden_states=hidden_states,\r\n\t\t\tpast_key_value=self_attn_past_key_value,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\tlayer_head_mask=layer_head_mask,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t)\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\t\thidden_states = residual + hidden_states\r\n\r\n\t\t# Cross-Attention Block\r\n\t\tcross_attn_present_key_value = None\r\n\t\tcross_attn_weights = None\r\n\t\tif encoder_hidden_states is not None:\r\n\t\t\tresidual = hidden_states\r\n\t\t\thidden_states = self.encoder_attn_layer_norm(hidden_states)\r\n\r\n\t\t\t# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\r\n\t\t\tcross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\r\n\t\t\thidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\r\n\t\t\t\thidden_states=hidden_states,\r\n\t\t\t\tkey_value_states=encoder_hidden_states,\r\n\t\t\t\tattention_mask=encoder_attention_mask,\r\n\t\t\t\tlayer_head_mask=cross_attn_layer_head_mask,\r\n\t\t\t\tpast_key_value=cross_attn_past_key_value,\r\n\t\t\t\toutput_attentions=output_attentions,\r\n\t\t\t)\r\n\t\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\t\t\thidden_states = residual + hidden_states\r\n\r\n\t\t\t# add cross-attn to positions 3,4 of present_key_value tuple\r\n\t\t\tpresent_key_value = present_key_value + cross_attn_present_key_value\r\n\r\n\t\t# Fully Connected\r\n\t\tresidual = hidden_states\r\n\t\thidden_states = self.final_layer_norm(hidden_states)\r\n\t\thidden_states = self.activation_fn(self.fc1(hidden_states))\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\r\n\t\thidden_states = self.fc2(hidden_states)\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\t\thidden_states = residual + hidden_states\r\n\r\n\t\toutputs = (hidden_states,)\r\n\r\n\t\tif output_attentions:\r\n\t\t\toutputs += (self_attn_weights, cross_attn_weights)\r\n\r\n\t\tif use_cache:\r\n\t\t\toutputs += (present_key_value,)\r\n\r\n\t\treturn outputs\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart.BartClassificationHead with Bart->MBart\r\nclass MBartClassificationHead(nn.Module):\r\n\t\"\"\"Head for sentence-level classification tasks.\"\"\"\r\n\r\n\tdef __init__(\r\n\t\tself,\r\n\t\tinput_dim: int,\r\n\t\tinner_dim: int,\r\n\t\tnum_classes: int,\r\n\t\tpooler_dropout: float,\r\n\t):\r\n\t\tsuper().__init__()\r\n\t\tself.dense = nn.Linear(input_dim, inner_dim)\r\n\t\tself.dropout = nn.Dropout(p=pooler_dropout)\r\n\t\tself.out_proj = nn.Linear(inner_dim, num_classes)\r\n\r\n\tdef forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\r\n\t\thidden_states = self.dropout(hidden_states)\r\n\t\thidden_states = self.dense(hidden_states)\r\n\t\thidden_states = torch.tanh(hidden_states)\r\n\t\thidden_states = self.dropout(hidden_states)\r\n\t\thidden_states = self.out_proj(hidden_states)\r\n\t\treturn hidden_states\r\n\r\n\r\nclass MBartPreTrainedModel(PreTrainedModel):\r\n\tconfig_class = MBartConfig\r\n\tbase_model_prefix = \"model\"\r\n\tsupports_gradient_checkpointing = True\r\n\r\n\tdef _init_weights(self, module):\r\n\t\tstd = self.config.init_std\r\n\t\tif isinstance(module, nn.Linear):\r\n\t\t\tmodule.weight.data.normal_(mean=0.0, std=std)\r\n\t\t\tif module.bias is not None:\r\n\t\t\t\tmodule.bias.data.zero_()\r\n\t\telif isinstance(module, nn.Embedding):\r\n\t\t\tmodule.weight.data.normal_(mean=0.0, std=std)\r\n\t\t\tif module.padding_idx is not None:\r\n\t\t\t\tmodule.weight.data[module.padding_idx].zero_()\r\n\r\n\tdef _set_gradient_checkpointing(self, module, value=False):\r\n\t\tif isinstance(module, (MBartDecoder, MBartDecoder)):\r\n\t\t\tmodule.gradient_checkpointing = value\r\n\r\n\t@property\r\n\tdef dummy_inputs(self):\r\n\t\tpad_token = self.config.pad_token_id\r\n\t\tinput_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\r\n\t\tdummy_inputs = {\r\n\t\t\t\"attention_mask\": input_ids.ne(pad_token),\r\n\t\t\t\"input_ids\": input_ids,\r\n\t\t}\r\n\t\treturn dummy_inputs\r\n\r\n\r\nMBART_START_DOCSTRING = r\"\"\"\r\n\tThis model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\r\n\tlibrary implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\r\n\tetc.)\r\n\r\n\tThis model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\r\n\tUse it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\r\n\tand behavior.\r\n\r\n\tParameters:\r\n\t\tconfig ([`MBartConfig`]):\r\n\t\t\tModel configuration class with all the parameters of the model. Initializing with a config file does not\r\n\t\t\tload the weights associated with the model, only the configuration. Check out the\r\n\t\t\t[`~PreTrainedModel.from_pretrained`] method to load the model weights.\r\n\"\"\"\r\n\r\nMBART_GENERATION_EXAMPLE = r\"\"\"\r\n\tTranslation example:\r\n\r\n\t```python\r\n\t>>> from transformers import MBartTokenizer, MBartForConditionalGeneration\r\n\r\n\t>>> model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\r\n\t>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\")\r\n\r\n\t>>> example_english_phrase = \"42 is the answer\"\r\n\t>>> inputs = tokenizer(example_english_phrase, return_tensors=\"pt\")\r\n\r\n\t>>> # Translate\r\n\t>>> generated_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5)\r\n\t>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\r\n\t'42 este rspuns'\r\n\t```\r\n\r\n\tMask filling example:\r\n\r\n\t```python\r\n\t>>> from transformers import MBartTokenizer, MBartForConditionalGeneration\r\n\r\n\t>>> model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\r\n\t>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\r\n\r\n\t>>> # de_DE is the language symbol id <LID> for German\r\n\t>>> TXT = \"</s> Meine Freunde sind <mask> nett aber sie essen zu viel Kuchen. </s> de_DE\"\r\n\r\n\t>>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\r\n\t>>> logits = model(input_ids).logits\r\n\r\n\t>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\r\n\t>>> probs = logits[0, masked_index].softmax(dim=0)\r\n\t>>> values, predictions = probs.topk(5)\r\n\r\n\t>>> tokenizer.decode(predictions).split()\r\n\t['nett', 'sehr', 'ganz', 'nicht', 'so']\r\n\t```\r\n\"\"\"\r\n\r\nMBART_INPUTS_DOCSTRING = r\"\"\"\r\n\tArgs:\r\n\t\tinput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\r\n\t\t\tIndices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\r\n\t\t\tit.\r\n\r\n\t\t\tIndices can be obtained using [`MBartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\r\n\t\t\t[`PreTrainedTokenizer.__call__`] for details.\r\n\r\n\t\t\t[What are input IDs?](../glossary#input-ids)\r\n\t\tattention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\tMask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t- 1 for tokens that are **not masked**,\r\n\t\t\t- 0 for tokens that are **masked**.\r\n\r\n\t\t\t[What are attention masks?](../glossary#attention-mask)\r\n\t\tdecoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\r\n\t\t\tIndices of decoder input sequence tokens in the vocabulary.\r\n\r\n\t\t\tIndices can be obtained using [`MBartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\r\n\t\t\t[`PreTrainedTokenizer.__call__`] for details.\r\n\r\n\t\t\t[What are decoder input IDs?](../glossary#decoder-input-ids)\r\n\r\n\t\t\tMBart uses a specific language id token as the starting token for `decoder_input_ids` generation that\r\n\t\t\tvaries according to source and target language, *e.g.* 25004 for *en_XX*, and 25003 for *de_DE*. If\r\n\t\t\t`past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\r\n\t\t\t`past_key_values`).\r\n\r\n\t\t\tFor translation and summarization training, `decoder_input_ids` should be provided. If no\r\n\t\t\t`decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\r\n\t\t\tfor denoising pre-training following the paper.\r\n\t\tdecoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\r\n\t\t\tDefault behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\r\n\t\t\tbe used by default.\r\n\t\thead_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\r\n\t\t\tMask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\tdecoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\r\n\t\t\tMask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\tcross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\r\n\t\t\tMask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\r\n\t\t\t1]`:\r\n\r\n\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\tencoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\r\n\t\t\tTuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\r\n\t\t\t`last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\r\n\t\t\thidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\r\n\t\tpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\r\n\t\t\tTuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\r\n\t\t\t`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\r\n\t\t\t`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\r\n\r\n\t\t\tContains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\r\n\t\t\tblocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\r\n\r\n\t\t\tIf `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\r\n\t\t\tdon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\r\n\t\t\t`decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\r\n\t\t\t`(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\r\n\t\t\tcan choose to directly pass an embedded representation. This is useful if you want more control over how to\r\n\t\t\tconvert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\r\n\t\tdecoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\r\n\t\t\tOptionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\r\n\t\t\trepresentation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\r\n\t\t\tinput (see `past_key_values`). This is useful if you want more control over how to convert\r\n\t\t\t`decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\r\n\r\n\t\t\tIf `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\r\n\t\t\tof `inputs_embeds`.\r\n\t\tuse_cache (`bool`, *optional*):\r\n\t\t\tIf set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\r\n\t\t\t`past_key_values`).\r\n\t\toutput_attentions (`bool`, *optional*):\r\n\t\t\tWhether or not to return the attentions tensors of all attention layers. See `attentions` under returned\r\n\t\t\ttensors for more detail.\r\n\t\toutput_hidden_states (`bool`, *optional*):\r\n\t\t\tWhether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\r\n\t\t\tmore detail.\r\n\t\treturn_dict (`bool`, *optional*):\r\n\t\t\tWhether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\r\n\"\"\"\r\n\r\n\r\nclass MBartEncoder(MBartPreTrainedModel):\r\n\t\"\"\"\r\n\tTransformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\r\n\t[`MBartEncoderLayer`].\r\n\r\n\tArgs:\r\n\t\tconfig: MBartConfig\r\n\t\tembed_tokens (nn.Embedding): output embedding\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = None):\r\n\t\tsuper().__init__(config)\r\n\r\n\t\tself.dropout = config.dropout\r\n\t\tself.layerdrop = config.encoder_layerdrop\r\n\r\n\t\tembed_dim = config.d_model\r\n\t\tself.padding_idx = config.pad_token_id\r\n\t\tself.max_source_positions = config.max_position_embeddings\r\n\t\tself.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\r\n\r\n\t\tif embed_tokens is not None:\r\n\t\t\tself.embed_tokens = embed_tokens\r\n\t\telse:\r\n\t\t\tself.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\r\n\r\n\t\tself.embed_positions = MBartLearnedPositionalEmbedding(\r\n\t\t\tconfig.max_position_embeddings,\r\n\t\t\tembed_dim,\r\n\t\t)\r\n\t\tself.token_type_embedding = nn.Embedding(3, embed_dim)\r\n\r\n\t\tself.layers = nn.ModuleList([MBartEncoderLayer(config) for _ in range(config.encoder_layers)])\r\n\t\tself.layernorm_embedding = nn.LayerNorm(embed_dim)\r\n\t\tself.layer_norm = nn.LayerNorm(config.d_model)\r\n\r\n\t\tself.gradient_checkpointing = False\r\n\t\t# Initialize weights and apply final processing\r\n\t\tself.post_init()\r\n\r\n\tdef _backward_compatibility_gradient_checkpointing(self):\r\n\t\t# Override to not delete the attribute from the config\r\n\t\tif self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\r\n\t\t\tself.gradient_checkpointing_enable()\r\n\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.LongTensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\ttoken_type_ids: torch.LongTensor = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Tuple, BaseModelOutput]:\r\n\t\tr\"\"\"\r\n\t\tArgs:\r\n\t\t\tinput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\r\n\t\t\t\tIndices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\r\n\t\t\t\tprovide it.\r\n\r\n\t\t\t\tIndices can be obtained using [`MBartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\r\n\t\t\t\t[`PreTrainedTokenizer.__call__`] for details.\r\n\r\n\t\t\t\t[What are input IDs?](../glossary#input-ids)\r\n\t\t\tattention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\t\tMask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 for tokens that are **not masked**,\r\n\t\t\t\t- 0 for tokens that are **masked**.\r\n\r\n\t\t\t\t[What are attention masks?](../glossary#attention-mask)\r\n\t\t\thead_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\r\n\t\t\t\tMask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\t\tinputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\r\n\t\t\t\tOptionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\r\n\t\t\t\tThis is useful if you want more control over how to convert `input_ids` indices into associated vectors\r\n\t\t\t\tthan the model's internal embedding lookup matrix.\r\n\t\t\toutput_attentions (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the attentions tensors of all attention layers. See `attentions` under\r\n\t\t\t\treturned tensors for more detail.\r\n\t\t\toutput_hidden_states (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\r\n\t\t\t\tfor more detail.\r\n\t\t\treturn_dict (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\r\n\t\t\"\"\"\r\n\t\toutput_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\r\n\t\toutput_hidden_states = (\r\n\t\t\toutput_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\r\n\t\t)\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\r\n\t\t# retrieve input_ids and inputs_embeds\r\n\t\tif input_ids is not None and inputs_embeds is not None:\r\n\t\t\traise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\r\n\t\telif input_ids is not None:\r\n\t\t\tinput_shape = input_ids.size()\r\n\t\t\tinput_ids = input_ids.view(-1, input_shape[-1])\r\n\t\telif inputs_embeds is not None:\r\n\t\t\tinput_shape = inputs_embeds.size()[:-1]\r\n\t\telse:\r\n\t\t\traise ValueError(\"You have to specify either input_ids or inputs_embeds\")\r\n\r\n\t\tif inputs_embeds is None:\r\n\t\t\tinputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\r\n\r\n\t\tassert self.max_source_positions == input_shape[-1]\r\n\r\n\t\tinput_shape = (input_ids.shape[0], input_shape[-1] // 2)\r\n\t\tembed_pos = self.embed_positions(input_shape)\r\n\t\td_embed_pos = self.embed_positions(input_shape)\r\n\t\tembed_pos = torch.cat((embed_pos, d_embed_pos), dim=0)\r\n\r\n\t\ttoken_type_embeds = self.token_type_embedding(token_type_ids)\r\n\r\n\t\thidden_states = inputs_embeds + embed_pos + token_type_embeds\r\n\t\thidden_states = self.layernorm_embedding(hidden_states)\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\r\n\t\t# expand attention_mask\r\n\t\tif attention_mask is not None:\r\n\t\t\t# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\r\n\t\t\tattention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\r\n\r\n\t\tencoder_states = () if output_hidden_states else None\r\n\t\tall_attentions = () if output_attentions else None\r\n\r\n\t\t# check if head_mask has a correct number of layers specified if desired\r\n\t\tif head_mask is not None:\r\n\t\t\tif head_mask.size()[0] != len(self.layers):\r\n\t\t\t\traise ValueError(\r\n\t\t\t\t\tf\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\r\n\t\t\t\t\tf\" {head_mask.size()[0]}.\"\r\n\t\t\t\t)\r\n\t\tfor idx, encoder_layer in enumerate(self.layers):\r\n\t\t\tif output_hidden_states:\r\n\t\t\t\tencoder_states = encoder_states + (hidden_states,)\r\n\t\t\t# add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\r\n\t\t\tdropout_probability = random.uniform(0, 1)\r\n\t\t\tif self.training and (dropout_probability < self.layerdrop):  # skip the layer\r\n\t\t\t\tlayer_outputs = (None, None)\r\n\t\t\telse:\r\n\t\t\t\tif self.gradient_checkpointing and self.training:\r\n\r\n\t\t\t\t\tdef create_custom_forward(module):\r\n\t\t\t\t\t\tdef custom_forward(*inputs):\r\n\t\t\t\t\t\t\treturn module(*inputs, output_attentions)\r\n\r\n\t\t\t\t\t\treturn custom_forward\r\n\r\n\t\t\t\t\tlayer_outputs = torch.utils.checkpoint.checkpoint(\r\n\t\t\t\t\t\tcreate_custom_forward(encoder_layer),\r\n\t\t\t\t\t\thidden_states,\r\n\t\t\t\t\t\tattention_mask,\r\n\t\t\t\t\t\t(head_mask[idx] if head_mask is not None else None),\r\n\t\t\t\t\t)\r\n\t\t\t\telse:\r\n\t\t\t\t\tlayer_outputs = encoder_layer(\r\n\t\t\t\t\t\thidden_states,\r\n\t\t\t\t\t\tattention_mask,\r\n\t\t\t\t\t\tlayer_head_mask=(head_mask[idx] if head_mask is not None else None),\r\n\t\t\t\t\t\toutput_attentions=output_attentions,\r\n\t\t\t\t\t)\r\n\r\n\t\t\t\thidden_states = layer_outputs[0]\r\n\r\n\t\t\tif output_attentions:\r\n\t\t\t\tall_attentions = all_attentions + (layer_outputs[1],)\r\n\r\n\t\thidden_states = self.layer_norm(hidden_states)\r\n\r\n\t\tif output_hidden_states:\r\n\t\t\tencoder_states = encoder_states + (hidden_states,)\r\n\r\n\t\tif not return_dict:\r\n\t\t\treturn tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\r\n\t\treturn BaseModelOutput(\r\n\t\t\tlast_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\r\n\t\t)\r\n\r\n\r\nclass MBartDecoder(MBartPreTrainedModel):\r\n\t\"\"\"\r\n\tTransformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`MBartDecoderLayer`]\r\n\r\n\tArgs:\r\n\t\tconfig: MBartConfig\r\n\t\tembed_tokens (nn.Embedding): output embedding\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = None):\r\n\t\tsuper().__init__(config)\r\n\t\tself.dropout = config.dropout\r\n\t\tself.layerdrop = config.decoder_layerdrop\r\n\t\tself.padding_idx = config.pad_token_id\r\n\t\tself.max_target_positions = config.max_position_embeddings\r\n\t\tself.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\r\n\r\n\t\tif embed_tokens is not None:\r\n\t\t\tself.embed_tokens = embed_tokens\r\n\t\telse:\r\n\t\t\tself.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\r\n\r\n\t\tself.embed_positions = MBartLearnedPositionalEmbedding(\r\n\t\t\tconfig.max_position_embeddings,\r\n\t\t\tconfig.d_model,\r\n\t\t)\r\n\t\tself.layers = nn.ModuleList([MBartDecoderLayer(config) for _ in range(config.decoder_layers)])\r\n\t\tself.layernorm_embedding = nn.LayerNorm(config.d_model)\r\n\t\tself.layer_norm = nn.LayerNorm(config.d_model)\r\n\r\n\t\tself.gradient_checkpointing = False\r\n\t\t# Initialize weights and apply final processing\r\n\t\tself.post_init()\r\n\r\n\tdef get_input_embeddings(self):\r\n\t\treturn self.embed_tokens\r\n\r\n\tdef set_input_embeddings(self, value):\r\n\t\tself.embed_tokens = value\r\n\r\n\t# Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\r\n\tdef _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\r\n\t\t# create causal mask\r\n\t\t# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\r\n\t\tcombined_attention_mask = None\r\n\t\tif input_shape[-1] > 1:\r\n\t\t\tcombined_attention_mask = _make_causal_mask(\r\n\t\t\t\tinput_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\r\n\t\t\t).to(inputs_embeds.device)\r\n\r\n\t\tif attention_mask is not None:\r\n\t\t\t# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\r\n\t\t\texpanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\r\n\t\t\tcombined_attention_mask = (\r\n\t\t\t\texpanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\r\n\t\t\t)\r\n\r\n\t\treturn combined_attention_mask\r\n\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.LongTensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_hidden_states: Optional[torch.FloatTensor] = None,\r\n\t\tencoder_attention_mask: Optional[torch.LongTensor] = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_head_mask: Optional[torch.Tensor] = None,\r\n\t\tpast_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tuse_cache: Optional[bool] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\r\n\t\tr\"\"\"\r\n\t\tArgs:\r\n\t\t\tinput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\r\n\t\t\t\tIndices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\r\n\t\t\t\tprovide it.\r\n\r\n\t\t\t\tIndices can be obtained using [`MBartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\r\n\t\t\t\t[`PreTrainedTokenizer.__call__`] for details.\r\n\r\n\t\t\t\t[What are input IDs?](../glossary#input-ids)\r\n\t\t\tattention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\t\tMask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 for tokens that are **not masked**,\r\n\t\t\t\t- 0 for tokens that are **masked**.\r\n\r\n\t\t\t\t[What are attention masks?](../glossary#attention-mask)\r\n\t\t\tencoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\r\n\t\t\t\tSequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\r\n\t\t\t\tof the decoder.\r\n\t\t\tencoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\r\n\t\t\t\tMask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\r\n\t\t\t\tselected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 for tokens that are **not masked**,\r\n\t\t\t\t- 0 for tokens that are **masked**.\r\n\r\n\t\t\t\t[What are attention masks?](../glossary#attention-mask)\r\n\t\t\thead_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\r\n\t\t\t\tMask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\t\tcross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\r\n\t\t\t\tMask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\r\n\t\t\t\tcross-attention on hidden heads. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\t\tpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\r\n\t\t\t\tTuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\r\n\t\t\t\tshape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\r\n\t\t\t\tshape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\r\n\r\n\t\t\t\tContains pre-computed hidden-states (key and values in the self-attention blocks and in the\r\n\t\t\t\tcross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\r\n\r\n\t\t\t\tIf `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\r\n\t\t\t\tthat don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\r\n\t\t\t\tall `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\r\n\t\t\t\tshape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\r\n\t\t\t\t`input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\r\n\t\t\t\tcontrol over how to convert `input_ids` indices into associated vectors than the model's internal\r\n\t\t\t\tembedding lookup matrix.\r\n\t\t\toutput_attentions (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the attentions tensors of all attention layers. See `attentions` under\r\n\t\t\t\treturned tensors for more detail.\r\n\t\t\toutput_hidden_states (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\r\n\t\t\t\tfor more detail.\r\n\t\t\treturn_dict (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\r\n\t\t\"\"\"\r\n\t\toutput_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\r\n\t\toutput_hidden_states = (\r\n\t\t\toutput_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\r\n\t\t)\r\n\t\tuse_cache = use_cache if use_cache is not None else self.config.use_cache\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\r\n\t\t# retrieve input_ids and inputs_embeds\r\n\t\tif input_ids is not None and inputs_embeds is not None:\r\n\t\t\traise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\r\n\t\telif input_ids is not None:\r\n\t\t\tinput_shape = input_ids.size()\r\n\t\t\tinput_ids = input_ids.view(-1, input_shape[-1])\r\n\t\telif inputs_embeds is not None:\r\n\t\t\tinput_shape = inputs_embeds.size()[:-1]\r\n\t\telse:\r\n\t\t\traise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\r\n\r\n\t\t# past_key_values_length\r\n\t\tpast_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\r\n\r\n\t\tif inputs_embeds is None:\r\n\t\t\tinputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\r\n\r\n\t\tattention_mask = self._prepare_decoder_attention_mask(\r\n\t\t\tattention_mask, input_shape, inputs_embeds, past_key_values_length\r\n\t\t)\r\n\r\n\t\t# expand encoder attention mask\r\n\t\tif encoder_hidden_states is not None and encoder_attention_mask is not None:\r\n\t\t\t# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\r\n\t\t\tencoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\r\n\r\n\t\t# embed positions\r\n\t\tpositions = self.embed_positions(input_shape, past_key_values_length)\r\n\r\n\t\thidden_states = inputs_embeds + positions\r\n\t\thidden_states = self.layernorm_embedding(hidden_states)\r\n\r\n\t\thidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\r\n\r\n\t\t# decoder layers\r\n\t\tall_hidden_states = () if output_hidden_states else None\r\n\t\tall_self_attns = () if output_attentions else None\r\n\t\tall_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\r\n\t\tnext_decoder_cache = () if use_cache else None\r\n\r\n\t\t# check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\r\n\t\tfor attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\r\n\t\t\tif attn_mask is not None:\r\n\t\t\t\tif attn_mask.size()[0] != len(self.layers):\r\n\t\t\t\t\traise ValueError(\r\n\t\t\t\t\t\tf\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\r\n\t\t\t\t\t\tf\" {head_mask.size()[0]}.\"\r\n\t\t\t\t\t)\r\n\t\tfor idx, decoder_layer in enumerate(self.layers):\r\n\t\t\t# add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\r\n\t\t\tif output_hidden_states:\r\n\t\t\t\tall_hidden_states += (hidden_states,)\r\n\t\t\tdropout_probability = random.uniform(0, 1)\r\n\t\t\tif self.training and (dropout_probability < self.layerdrop):\r\n\t\t\t\tcontinue\r\n\r\n\t\t\tpast_key_value = past_key_values[idx] if past_key_values is not None else None\r\n\r\n\t\t\tif self.gradient_checkpointing and self.training:\r\n\r\n\t\t\t\tif use_cache:\r\n\t\t\t\t\tlogger.warning(\r\n\t\t\t\t\t\t\"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\r\n\t\t\t\t\t)\r\n\t\t\t\t\tuse_cache = False\r\n\r\n\t\t\t\tdef create_custom_forward(module):\r\n\t\t\t\t\tdef custom_forward(*inputs):\r\n\t\t\t\t\t\t# None for past_key_value\r\n\t\t\t\t\t\treturn module(*inputs, output_attentions, use_cache)\r\n\r\n\t\t\t\t\treturn custom_forward\r\n\r\n\t\t\t\tlayer_outputs = torch.utils.checkpoint.checkpoint(\r\n\t\t\t\t\tcreate_custom_forward(decoder_layer),\r\n\t\t\t\t\thidden_states,\r\n\t\t\t\t\tattention_mask,\r\n\t\t\t\t\tencoder_hidden_states,\r\n\t\t\t\t\tencoder_attention_mask,\r\n\t\t\t\t\thead_mask[idx] if head_mask is not None else None,\r\n\t\t\t\t\tcross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\r\n\t\t\t\t\tNone,\r\n\t\t\t\t)\r\n\t\t\telse:\r\n\r\n\t\t\t\tlayer_outputs = decoder_layer(\r\n\t\t\t\t\thidden_states,\r\n\t\t\t\t\tattention_mask=attention_mask,\r\n\t\t\t\t\tencoder_hidden_states=encoder_hidden_states,\r\n\t\t\t\t\tencoder_attention_mask=encoder_attention_mask,\r\n\t\t\t\t\tlayer_head_mask=(head_mask[idx] if head_mask is not None else None),\r\n\t\t\t\t\tcross_attn_layer_head_mask=(\r\n\t\t\t\t\t\tcross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\r\n\t\t\t\t\t),\r\n\t\t\t\t\tpast_key_value=past_key_value,\r\n\t\t\t\t\toutput_attentions=output_attentions,\r\n\t\t\t\t\tuse_cache=use_cache,\r\n\t\t\t\t)\r\n\t\t\thidden_states = layer_outputs[0]\r\n\r\n\t\t\tif use_cache:\r\n\t\t\t\tnext_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\r\n\r\n\t\t\tif output_attentions:\r\n\t\t\t\tall_self_attns += (layer_outputs[1],)\r\n\r\n\t\t\t\tif encoder_hidden_states is not None:\r\n\t\t\t\t\tall_cross_attentions += (layer_outputs[2],)\r\n\r\n\t\thidden_states = self.layer_norm(hidden_states)\r\n\r\n\t\t# add hidden states from the last decoder layer\r\n\t\tif output_hidden_states:\r\n\t\t\tall_hidden_states += (hidden_states,)\r\n\r\n\t\tnext_cache = next_decoder_cache if use_cache else None\r\n\t\tif not return_dict:\r\n\t\t\treturn tuple(\r\n\t\t\t\tv\r\n\t\t\t\tfor v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\r\n\t\t\t\tif v is not None\r\n\t\t\t)\r\n\t\treturn BaseModelOutputWithPastAndCrossAttentions(\r\n\t\t\tlast_hidden_state=hidden_states,\r\n\t\t\tpast_key_values=next_cache,\r\n\t\t\thidden_states=all_hidden_states,\r\n\t\t\tattentions=all_self_attns,\r\n\t\t\tcross_attentions=all_cross_attentions,\r\n\t\t)\r\n\r\n\r\n@add_start_docstrings(\r\n\t\"The bare MBART Model outputting raw hidden-states without any specific head on top.\",\r\n\tMBART_START_DOCSTRING,\r\n)\r\nclass MBartModel(MBartPreTrainedModel):\r\n\tdef __init__(self, config: MBartConfig):\r\n\t\tsuper().__init__(config)\r\n\r\n\t\tpadding_idx, vocab_size = config.pad_token_id, config.vocab_size\r\n\t\tself.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\r\n\r\n\t\tself.encoder = MBartEncoder(config, self.shared)\r\n\t\tself.decoder = MBartDecoder(config, self.shared)\r\n\r\n\t\t# Initialize weights and apply final processing\r\n\t\tself.post_init()\r\n\r\n\tdef get_input_embeddings(self):\r\n\t\treturn self.shared\r\n\r\n\tdef set_input_embeddings(self, value):\r\n\t\tself.shared = value\r\n\t\tself.encoder.embed_tokens = self.shared\r\n\t\tself.decoder.embed_tokens = self.shared\r\n\r\n\tdef get_encoder(self):\r\n\t\treturn self.encoder\r\n\r\n\tdef get_decoder(self):\r\n\t\treturn self.decoder\r\n\r\n\t@add_start_docstrings_to_model_forward(MBART_INPUTS_DOCSTRING)\r\n\t@add_code_sample_docstrings(\r\n\t\tprocessor_class=_TOKENIZER_FOR_DOC,\r\n\t\tcheckpoint=_CHECKPOINT_FOR_DOC,\r\n\t\toutput_type=Seq2SeqModelOutput,\r\n\t\tconfig_class=_CONFIG_FOR_DOC,\r\n\t\texpected_output=_EXPECTED_OUTPUT_SHAPE,\r\n\t)\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.LongTensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\ttoken_type_ids: Optional[torch.LongTensor] = None,\r\n\t\tdecoder_input_ids: Optional[torch.LongTensor] = None,\r\n\t\tdecoder_attention_mask: Optional[torch.LongTensor] = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tdecoder_head_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_head_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\r\n\t\tpast_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tdecoder_inputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tuse_cache: Optional[bool] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Seq2SeqModelOutput, Tuple[torch.FloatTensor]]:\r\n\t\toutput_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\r\n\t\toutput_hidden_states = (\r\n\t\t\toutput_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\r\n\t\t)\r\n\t\tuse_cache = use_cache if use_cache is not None else self.config.use_cache\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\r\n\t\t# different to other models, MBart automatically creates decoder_input_ids from\r\n\t\t# input_ids if no decoder_input_ids are provided\r\n\t\tif decoder_input_ids is None and decoder_inputs_embeds is None:\r\n\t\t\tdecoder_input_ids = shift_tokens_right(input_ids, self.config.pad_token_id)\r\n\r\n\t\tif encoder_outputs is None:\r\n\t\t\tencoder_outputs = self.encoder(\r\n\t\t\t\tinput_ids=input_ids,\r\n\t\t\t\tattention_mask=attention_mask,\r\n\t\t\t\ttoken_type_ids=token_type_ids,\r\n\t\t\t\thead_mask=head_mask,\r\n\t\t\t\tinputs_embeds=inputs_embeds,\r\n\t\t\t\toutput_attentions=output_attentions,\r\n\t\t\t\toutput_hidden_states=output_hidden_states,\r\n\t\t\t\treturn_dict=return_dict,\r\n\t\t\t)\r\n\t\t# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\r\n\t\telif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\r\n\t\t\tencoder_outputs = BaseModelOutput(\r\n\t\t\t\tlast_hidden_state=encoder_outputs[0],\r\n\t\t\t\thidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\r\n\t\t\t\tattentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\r\n\t\t\t)\r\n\r\n\t\t# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\r\n\t\tdecoder_outputs = self.decoder(\r\n\t\t\tinput_ids=decoder_input_ids,\r\n\t\t\tattention_mask=decoder_attention_mask,\r\n\t\t\tencoder_hidden_states=encoder_outputs[0],\r\n\t\t\tencoder_attention_mask=attention_mask,\r\n\t\t\thead_mask=decoder_head_mask,\r\n\t\t\tcross_attn_head_mask=cross_attn_head_mask,\r\n\t\t\tpast_key_values=past_key_values,\r\n\t\t\tinputs_embeds=decoder_inputs_embeds,\r\n\t\t\tuse_cache=use_cache,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t\toutput_hidden_states=output_hidden_states,\r\n\t\t\treturn_dict=return_dict,\r\n\t\t)\r\n\r\n\t\tif not return_dict:\r\n\t\t\treturn decoder_outputs + encoder_outputs\r\n\r\n\t\treturn Seq2SeqModelOutput(\r\n\t\t\tlast_hidden_state=decoder_outputs.last_hidden_state,\r\n\t\t\tpast_key_values=decoder_outputs.past_key_values,\r\n\t\t\tdecoder_hidden_states=decoder_outputs.hidden_states,\r\n\t\t\tdecoder_attentions=decoder_outputs.attentions,\r\n\t\t\tcross_attentions=decoder_outputs.cross_attentions,\r\n\t\t\tencoder_last_hidden_state=encoder_outputs.last_hidden_state,\r\n\t\t\tencoder_hidden_states=encoder_outputs.hidden_states,\r\n\t\t\tencoder_attentions=encoder_outputs.attentions,\r\n\t\t)\r\n\r\n\r\n@add_start_docstrings(\r\n\t\"The MBART Model with a language modeling head. Can be used for summarization.\", MBART_START_DOCSTRING\r\n)\r\nclass MBartForConditionalGeneration(MBartPreTrainedModel):\r\n\tbase_model_prefix = \"model\"\r\n\t_keys_to_ignore_on_load_missing = [\r\n\t\tr\"final_logits_bias\",\r\n\t\tr\"encoder.version\",\r\n\t\tr\"decoder.version\",\r\n\t\tr\"lm_head.weight\",\r\n\t]\r\n\r\n\tdef __init__(self, config: MBartConfig):\r\n\t\tsuper().__init__(config)\r\n\t\tself.model = MBartModel(config)\r\n\t\tself.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\r\n\t\tself.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\r\n\r\n\t\t# Initialize weights and apply final processing\r\n\t\tself.post_init()\r\n\r\n\tdef get_encoder(self):\r\n\t\treturn self.model.get_encoder()\r\n\r\n\tdef get_decoder(self):\r\n\t\treturn self.model.get_decoder()\r\n\r\n\tdef resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\r\n\t\tnew_embeddings = super().resize_token_embeddings(new_num_tokens)\r\n\t\tself._resize_final_logits_bias(new_num_tokens)\r\n\t\treturn new_embeddings\r\n\r\n\tdef _resize_final_logits_bias(self, new_num_tokens: int) -> None:\r\n\t\told_num_tokens = self.final_logits_bias.shape[-1]\r\n\t\tif new_num_tokens <= old_num_tokens:\r\n\t\t\tnew_bias = self.final_logits_bias[:, :new_num_tokens]\r\n\t\telse:\r\n\t\t\textra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\r\n\t\t\tnew_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\r\n\t\tself.register_buffer(\"final_logits_bias\", new_bias)\r\n\r\n\tdef get_output_embeddings(self):\r\n\t\treturn self.lm_head\r\n\r\n\tdef set_output_embeddings(self, new_embeddings):\r\n\t\tself.lm_head = new_embeddings\r\n\r\n\t@add_start_docstrings_to_model_forward(MBART_INPUTS_DOCSTRING)\r\n\t@replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\r\n\t@add_end_docstrings(MBART_GENERATION_EXAMPLE)\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.LongTensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\ttoken_type_ids: Optional[torch.LongTensor] = None,\r\n\t\tdecoder_input_ids: Optional[torch.LongTensor] = None,\r\n\t\tdecoder_attention_mask: Optional[torch.LongTensor] = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tdecoder_head_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_head_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\r\n\t\tpast_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tdecoder_inputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tlabels: Optional[torch.LongTensor] = None,\r\n\t\tuse_cache: Optional[bool] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\r\n\t\tr\"\"\"\r\n\t\tlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\tLabels for computing the masked language modeling loss. Indices should either be in `[0, ...,\r\n\t\t\tconfig.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\r\n\t\t\t(masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\r\n\r\n\t\tReturns:\r\n\r\n\t\t\"\"\"\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\r\n\t\tif labels is not None:\r\n\t\t\tif use_cache:\r\n\t\t\t\tlogger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\r\n\t\t\tuse_cache = False\r\n\t\t\tif decoder_input_ids is None:\r\n\t\t\t\tdecoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id)\r\n\r\n\t\toutputs = self.model(\r\n\t\t\tinput_ids,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\ttoken_type_ids=token_type_ids,\r\n\t\t\tdecoder_input_ids=decoder_input_ids,\r\n\t\t\tencoder_outputs=encoder_outputs,\r\n\t\t\tdecoder_attention_mask=decoder_attention_mask,\r\n\t\t\thead_mask=head_mask,\r\n\t\t\tdecoder_head_mask=decoder_head_mask,\r\n\t\t\tcross_attn_head_mask=cross_attn_head_mask,\r\n\t\t\tpast_key_values=past_key_values,\r\n\t\t\tinputs_embeds=inputs_embeds,\r\n\t\t\tdecoder_inputs_embeds=decoder_inputs_embeds,\r\n\t\t\tuse_cache=use_cache,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t\toutput_hidden_states=output_hidden_states,\r\n\t\t\treturn_dict=return_dict,\r\n\t\t)\r\n\t\tlm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\r\n\r\n\t\tmasked_lm_loss = None\r\n\t\tif labels is not None:\r\n\t\t\tloss_fct = CrossEntropyLoss()\r\n\t\t\tmasked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\r\n\r\n\t\tif not return_dict:\r\n\t\t\toutput = (lm_logits,) + outputs[1:]\r\n\t\t\treturn ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\r\n\r\n\t\treturn Seq2SeqLMOutput(\r\n\t\t\tloss=masked_lm_loss,\r\n\t\t\tlogits=lm_logits,\r\n\t\t\tpast_key_values=outputs.past_key_values,\r\n\t\t\tdecoder_hidden_states=outputs.decoder_hidden_states,\r\n\t\t\tdecoder_attentions=outputs.decoder_attentions,\r\n\t\t\tcross_attentions=outputs.cross_attentions,\r\n\t\t\tencoder_last_hidden_state=outputs.encoder_last_hidden_state,\r\n\t\t\tencoder_hidden_states=outputs.encoder_hidden_states,\r\n\t\t\tencoder_attentions=outputs.encoder_attentions,\r\n\t\t)\r\n\r\n\tdef prepare_inputs_for_generation(\r\n\t\tself,\r\n\t\tdecoder_input_ids,\r\n\t\tpast=None,\r\n\t\tattention_mask=None,\r\n\t\thead_mask=None,\r\n\t\tdecoder_head_mask=None,\r\n\t\tcross_attn_head_mask=None,\r\n\t\tuse_cache=None,\r\n\t\tencoder_outputs=None,\r\n\t\t**kwargs\r\n\t):\r\n\t\t# cut decoder_input_ids if past is used\r\n\t\tif past is not None:\r\n\t\t\tdecoder_input_ids = decoder_input_ids[:, -1:]\r\n\r\n\t\treturn {\r\n\t\t\t\"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\r\n\t\t\t\"encoder_outputs\": encoder_outputs,\r\n\t\t\t\"past_key_values\": past,\r\n\t\t\t\"decoder_input_ids\": decoder_input_ids,\r\n\t\t\t\"attention_mask\": attention_mask,\r\n\t\t\t\"head_mask\": head_mask,\r\n\t\t\t\"decoder_head_mask\": decoder_head_mask,\r\n\t\t\t\"cross_attn_head_mask\": cross_attn_head_mask,\r\n\t\t\t\"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\r\n\t\t}\r\n\r\n\tdef prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\r\n\t\treturn shift_tokens_right(labels, self.config.pad_token_id)\r\n\r\n\t@staticmethod\r\n\tdef _reorder_cache(past, beam_idx):\r\n\t\treordered_past = ()\r\n\t\tfor layer_past in past:\r\n\t\t\t# cached cross_attention states don't have to be reordered -> they are always the same\r\n\t\t\treordered_past += (\r\n\t\t\t\ttuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\r\n\t\t\t)\r\n\t\treturn reordered_past\r\n\r\n\r\n@add_start_docstrings(\r\n\t\"\"\"\r\n\tMBart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\r\n\ttasks.\r\n\t\"\"\",\r\n\tMBART_START_DOCSTRING,\r\n)\r\nclass MBartForSequenceClassification(MBartPreTrainedModel):\r\n\tdef __init__(self, config: MBartConfig, **kwargs):\r\n\t\tsuper().__init__(config, **kwargs)\r\n\t\tself.model = MBartModel(config)\r\n\t\tself.classification_head = MBartClassificationHead(\r\n\t\t\tconfig.d_model,\r\n\t\t\tconfig.d_model,\r\n\t\t\tconfig.num_labels,\r\n\t\t\tconfig.classifier_dropout,\r\n\t\t)\r\n\t\tself.model._init_weights(self.classification_head.dense)\r\n\t\tself.model._init_weights(self.classification_head.out_proj)\r\n\r\n\t@add_start_docstrings_to_model_forward(MBART_INPUTS_DOCSTRING)\r\n\t@add_code_sample_docstrings(\r\n\t\tprocessor_class=_TOKENIZER_FOR_DOC,\r\n\t\tcheckpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\r\n\t\toutput_type=Seq2SeqSequenceClassifierOutput,\r\n\t\tconfig_class=_CONFIG_FOR_DOC,\r\n\t\texpected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\r\n\t\texpected_loss=_SEQ_CLASS_EXPECTED_LOSS,\r\n\t)\r\n\t# Copied from transformers.models.bart.modeling_bart.BartForSequenceClassification.forward\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.LongTensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\tdecoder_input_ids: Optional[torch.LongTensor] = None,\r\n\t\tdecoder_attention_mask: Optional[torch.LongTensor] = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tdecoder_head_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_head_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_outputs: Optional[List[torch.FloatTensor]] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tdecoder_inputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tlabels: Optional[torch.LongTensor] = None,\r\n\t\tuse_cache: Optional[bool] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\r\n\t\tr\"\"\"\r\n\t\tlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\r\n\t\t\tLabels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\r\n\t\t\tconfig.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\r\n\t\t\"\"\"\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\t\tif labels is not None:\r\n\t\t\tuse_cache = False\r\n\r\n\t\tif input_ids is None and inputs_embeds is not None:\r\n\t\t\traise NotImplementedError(\r\n\t\t\t\tf\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\r\n\t\t\t)\r\n\r\n\t\toutputs = self.model(\r\n\t\t\tinput_ids,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\tdecoder_input_ids=decoder_input_ids,\r\n\t\t\tdecoder_attention_mask=decoder_attention_mask,\r\n\t\t\thead_mask=head_mask,\r\n\t\t\tdecoder_head_mask=decoder_head_mask,\r\n\t\t\tcross_attn_head_mask=cross_attn_head_mask,\r\n\t\t\tencoder_outputs=encoder_outputs,\r\n\t\t\tinputs_embeds=inputs_embeds,\r\n\t\t\tdecoder_inputs_embeds=decoder_inputs_embeds,\r\n\t\t\tuse_cache=use_cache,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t\toutput_hidden_states=output_hidden_states,\r\n\t\t\treturn_dict=return_dict,\r\n\t\t)\r\n\t\thidden_states = outputs[0]  # last hidden state\r\n\r\n\t\teos_mask = input_ids.eq(self.config.eos_token_id)\r\n\r\n\t\tif len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\r\n\t\t\traise ValueError(\"All examples must have the same number of <eos> tokens.\")\r\n\t\tsentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\r\n\t\t\t:, -1, :\r\n\t\t]\r\n\t\tlogits = self.classification_head(sentence_representation)\r\n\r\n\t\tloss = None\r\n\t\tif labels is not None:\r\n\t\t\tif self.config.problem_type is None:\r\n\t\t\t\tif self.config.num_labels == 1:\r\n\t\t\t\t\tself.config.problem_type = \"regression\"\r\n\t\t\t\telif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\r\n\t\t\t\t\tself.config.problem_type = \"single_label_classification\"\r\n\t\t\t\telse:\r\n\t\t\t\t\tself.config.problem_type = \"multi_label_classification\"\r\n\r\n\t\t\tif self.config.problem_type == \"regression\":\r\n\t\t\t\tloss_fct = MSELoss()\r\n\t\t\t\tif self.config.num_labels == 1:\r\n\t\t\t\t\tloss = loss_fct(logits.squeeze(), labels.squeeze())\r\n\t\t\t\telse:\r\n\t\t\t\t\tloss = loss_fct(logits, labels)\r\n\t\t\telif self.config.problem_type == \"single_label_classification\":\r\n\t\t\t\tloss_fct = CrossEntropyLoss()\r\n\t\t\t\tloss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\r\n\t\t\telif self.config.problem_type == \"multi_label_classification\":\r\n\t\t\t\tloss_fct = BCEWithLogitsLoss()\r\n\t\t\t\tloss = loss_fct(logits, labels)\r\n\t\tif not return_dict:\r\n\t\t\toutput = (logits,) + outputs[1:]\r\n\t\t\treturn ((loss,) + output) if loss is not None else output\r\n\r\n\t\treturn Seq2SeqSequenceClassifierOutput(\r\n\t\t\tloss=loss,\r\n\t\t\tlogits=logits,\r\n\t\t\tpast_key_values=outputs.past_key_values,\r\n\t\t\tdecoder_hidden_states=outputs.decoder_hidden_states,\r\n\t\t\tdecoder_attentions=outputs.decoder_attentions,\r\n\t\t\tcross_attentions=outputs.cross_attentions,\r\n\t\t\tencoder_last_hidden_state=outputs.encoder_last_hidden_state,\r\n\t\t\tencoder_hidden_states=outputs.encoder_hidden_states,\r\n\t\t\tencoder_attentions=outputs.encoder_attentions,\r\n\t\t)\r\n\r\n\r\n@add_start_docstrings(\r\n\t\"\"\"\r\n\tMBART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\r\n\tlayer on top of the hidden-states output to compute `span start logits` and `span end logits`).\r\n\t\"\"\",\r\n\tMBART_START_DOCSTRING,\r\n)\r\nclass MBartForQuestionAnswering(MBartPreTrainedModel):\r\n\tdef __init__(self, config):\r\n\t\tsuper().__init__(config)\r\n\r\n\t\tconfig.num_labels = 2\r\n\t\tself.num_labels = config.num_labels\r\n\r\n\t\tself.model = MBartModel(config)\r\n\t\tself.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\r\n\r\n\t\tself.model._init_weights(self.qa_outputs)\r\n\r\n\t@add_start_docstrings_to_model_forward(MBART_INPUTS_DOCSTRING)\r\n\t@add_code_sample_docstrings(\r\n\t\tprocessor_class=_TOKENIZER_FOR_DOC,\r\n\t\tcheckpoint=_CHECKPOINT_FOR_QA,\r\n\t\toutput_type=Seq2SeqQuestionAnsweringModelOutput,\r\n\t\tconfig_class=_CONFIG_FOR_DOC,\r\n\t\texpected_loss=_QA_EXPECTED_LOSS,\r\n\t\texpected_output=_QA_EXPECTED_OUTPUT,\r\n\t)\r\n\t# Copied from transformers.models.bart.modeling_bart.BartForQuestionAnswering.forward\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.Tensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\tdecoder_input_ids: Optional[torch.LongTensor] = None,\r\n\t\tdecoder_attention_mask: Optional[torch.LongTensor] = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tdecoder_head_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_head_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_outputs: Optional[List[torch.FloatTensor]] = None,\r\n\t\tstart_positions: Optional[torch.LongTensor] = None,\r\n\t\tend_positions: Optional[torch.LongTensor] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tdecoder_inputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tuse_cache: Optional[bool] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\r\n\t\tr\"\"\"\r\n\t\tstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\r\n\t\t\tLabels for position (index) of the start of the labelled span for computing the token classification loss.\r\n\t\t\tPositions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\r\n\t\t\tare not taken into account for computing the loss.\r\n\t\tend_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\r\n\t\t\tLabels for position (index) of the end of the labelled span for computing the token classification loss.\r\n\t\t\tPositions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\r\n\t\t\tare not taken into account for computing the loss.\r\n\t\t\"\"\"\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\t\tif start_positions is not None and end_positions is not None:\r\n\t\t\tuse_cache = False\r\n\r\n\t\toutputs = self.model(\r\n\t\t\tinput_ids,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\tdecoder_input_ids=decoder_input_ids,\r\n\t\t\tdecoder_attention_mask=decoder_attention_mask,\r\n\t\t\thead_mask=head_mask,\r\n\t\t\tdecoder_head_mask=decoder_head_mask,\r\n\t\t\tcross_attn_head_mask=cross_attn_head_mask,\r\n\t\t\tencoder_outputs=encoder_outputs,\r\n\t\t\tinputs_embeds=inputs_embeds,\r\n\t\t\tdecoder_inputs_embeds=decoder_inputs_embeds,\r\n\t\t\tuse_cache=use_cache,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t\toutput_hidden_states=output_hidden_states,\r\n\t\t\treturn_dict=return_dict,\r\n\t\t)\r\n\r\n\t\tsequence_output = outputs[0]\r\n\r\n\t\tlogits = self.qa_outputs(sequence_output)\r\n\t\tstart_logits, end_logits = logits.split(1, dim=-1)\r\n\t\tstart_logits = start_logits.squeeze(-1).contiguous()\r\n\t\tend_logits = end_logits.squeeze(-1).contiguous()\r\n\r\n\t\ttotal_loss = None\r\n\t\tif start_positions is not None and end_positions is not None:\r\n\t\t\t# If we are on multi-GPU, split add a dimension\r\n\t\t\tif len(start_positions.size()) > 1:\r\n\t\t\t\tstart_positions = start_positions.squeeze(-1)\r\n\t\t\tif len(end_positions.size()) > 1:\r\n\t\t\t\tend_positions = end_positions.squeeze(-1)\r\n\t\t\t# sometimes the start/end positions are outside our model inputs, we ignore these terms\r\n\t\t\tignored_index = start_logits.size(1)\r\n\t\t\tstart_positions = start_positions.clamp(0, ignored_index)\r\n\t\t\tend_positions = end_positions.clamp(0, ignored_index)\r\n\r\n\t\t\tloss_fct = CrossEntropyLoss(ignore_index=ignored_index)\r\n\t\t\tstart_loss = loss_fct(start_logits, start_positions)\r\n\t\t\tend_loss = loss_fct(end_logits, end_positions)\r\n\t\t\ttotal_loss = (start_loss + end_loss) / 2\r\n\r\n\t\tif not return_dict:\r\n\t\t\toutput = (\r\n\t\t\t\tstart_logits,\r\n\t\t\t\tend_logits,\r\n\t\t\t) + outputs[1:]\r\n\t\t\treturn ((total_loss,) + output) if total_loss is not None else output\r\n\r\n\t\treturn Seq2SeqQuestionAnsweringModelOutput(\r\n\t\t\tloss=total_loss,\r\n\t\t\tstart_logits=start_logits,\r\n\t\t\tend_logits=end_logits,\r\n\t\t\tpast_key_values=outputs.past_key_values,\r\n\t\t\tdecoder_hidden_states=outputs.decoder_hidden_states,\r\n\t\t\tdecoder_attentions=outputs.decoder_attentions,\r\n\t\t\tcross_attentions=outputs.cross_attentions,\r\n\t\t\tencoder_last_hidden_state=outputs.encoder_last_hidden_state,\r\n\t\t\tencoder_hidden_states=outputs.encoder_hidden_states,\r\n\t\t\tencoder_attentions=outputs.encoder_attentions,\r\n\t\t)\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->MBart\r\nclass MBartDecoderWrapper(MBartPreTrainedModel):\r\n\t\"\"\"\r\n\tThis wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\r\n\tused in combination with the [`EncoderDecoderModel`] framework.\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, config):\r\n\t\tsuper().__init__(config)\r\n\t\tself.decoder = MBartDecoder(config)\r\n\r\n\tdef forward(self, *args, **kwargs):\r\n\t\treturn self.decoder(*args, **kwargs)\r\n\r\n\r\n# Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->MBart, facebook/bart-base->facebook/mbart-large-cc25\r\nclass MBartForCausalLM(MBartPreTrainedModel):\r\n\tdef __init__(self, config):\r\n\t\tconfig = copy.deepcopy(config)\r\n\t\tconfig.is_decoder = True\r\n\t\tconfig.is_encoder_decoder = False\r\n\t\tsuper().__init__(config)\r\n\t\tself.model = MBartDecoderWrapper(config)\r\n\r\n\t\tself.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\r\n\r\n\t\t# Initialize weights and apply final processing\r\n\t\tself.post_init()\r\n\r\n\tdef get_input_embeddings(self):\r\n\t\treturn self.model.decoder.embed_tokens\r\n\r\n\tdef set_input_embeddings(self, value):\r\n\t\tself.model.decoder.embed_tokens = value\r\n\r\n\tdef get_output_embeddings(self):\r\n\t\treturn self.lm_head\r\n\r\n\tdef set_output_embeddings(self, new_embeddings):\r\n\t\tself.lm_head = new_embeddings\r\n\r\n\tdef set_decoder(self, decoder):\r\n\t\tself.model.decoder = decoder\r\n\r\n\tdef get_decoder(self):\r\n\t\treturn self.model.decoder\r\n\r\n\t@replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\r\n\tdef forward(\r\n\t\tself,\r\n\t\tinput_ids: torch.LongTensor = None,\r\n\t\tattention_mask: Optional[torch.Tensor] = None,\r\n\t\tencoder_hidden_states: Optional[torch.FloatTensor] = None,\r\n\t\tencoder_attention_mask: Optional[torch.FloatTensor] = None,\r\n\t\thead_mask: Optional[torch.Tensor] = None,\r\n\t\tcross_attn_head_mask: Optional[torch.Tensor] = None,\r\n\t\tpast_key_values: Optional[List[torch.FloatTensor]] = None,\r\n\t\tinputs_embeds: Optional[torch.FloatTensor] = None,\r\n\t\tlabels: Optional[torch.LongTensor] = None,\r\n\t\tuse_cache: Optional[bool] = None,\r\n\t\toutput_attentions: Optional[bool] = None,\r\n\t\toutput_hidden_states: Optional[bool] = None,\r\n\t\treturn_dict: Optional[bool] = None,\r\n\t) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\r\n\t\tr\"\"\"\r\n\t\tArgs:\r\n\t\t\tinput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\r\n\t\t\t\tIndices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\r\n\t\t\t\tprovide it.\r\n\r\n\t\t\t\tIndices can be obtained using [`MBartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\r\n\t\t\t\t[`PreTrainedTokenizer.__call__`] for details.\r\n\r\n\t\t\t\t[What are input IDs?](../glossary#input-ids)\r\n\t\t\tattention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\t\tMask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 for tokens that are **not masked**,\r\n\t\t\t\t- 0 for tokens that are **masked**.\r\n\r\n\t\t\t\t[What are attention masks?](../glossary#attention-mask)\r\n\t\t\tencoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\r\n\t\t\t\tSequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\r\n\t\t\t\tif the model is configured as a decoder.\r\n\t\t\tencoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\t\tMask to avoid performing attention on the padding token indices of the encoder input. This mask is used\r\n\t\t\t\tin the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\r\n\t\t\thead_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\r\n\t\t\t\tMask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\t\tcross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\r\n\t\t\t\tMask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\r\n\r\n\t\t\t\t- 1 indicates the head is **not masked**,\r\n\t\t\t\t- 0 indicates the head is **masked**.\r\n\r\n\t\t\tpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\r\n\t\t\t\tTuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\r\n\t\t\t\tshape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\r\n\t\t\t\tshape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\r\n\t\t\t\ttensors are only required when the model is used as a decoder in a Sequence to Sequence model.\r\n\r\n\t\t\t\tContains pre-computed hidden-states (key and values in the self-attention blocks and in the\r\n\t\t\t\tcross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\r\n\r\n\t\t\t\tIf `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\r\n\t\t\t\tthat don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\r\n\t\t\t\tall `decoder_input_ids` of shape `(batch_size, sequence_length)`.\r\n\t\t\tlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n\t\t\t\tLabels for computing the masked language modeling loss. Indices should either be in `[0, ...,\r\n\t\t\t\tconfig.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\r\n\t\t\t\t(masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\r\n\t\t\tuse_cache (`bool`, *optional*):\r\n\t\t\t\tIf set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\r\n\t\t\t\t(see `past_key_values`).\r\n\r\n\t\t\t\t- 1 for tokens that are **not masked**,\r\n\t\t\t\t- 0 for tokens that are **masked**.\r\n\t\t\toutput_attentions (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the attentions tensors of all attention layers. See `attentions` under\r\n\t\t\t\treturned tensors for more detail.\r\n\t\t\toutput_hidden_states (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\r\n\t\t\t\tfor more detail.\r\n\t\t\treturn_dict (`bool`, *optional*):\r\n\t\t\t\tWhether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\r\n\r\n\t\tReturns:\r\n\r\n\t\tExample:\r\n\r\n\t\t```python\r\n\t\t>>> from transformers import MBartTokenizer, MBartForCausalLM\r\n\r\n\t\t>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\r\n\t\t>>> model = MBartForCausalLM.from_pretrained(\"facebook/mbart-large-cc25\", add_cross_attention=False)\r\n\t\t>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\r\n\t\t>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\r\n\t\t>>> outputs = model(**inputs)\r\n\r\n\t\t>>> logits = outputs.logits\r\n\t\t>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\r\n\t\t>>> list(logits.shape) == expected_shape\r\n\t\tTrue\r\n\t\t```\"\"\"\r\n\r\n\t\toutput_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\r\n\t\toutput_hidden_states = (\r\n\t\t\toutput_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\r\n\t\t)\r\n\t\treturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n\r\n\t\t# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\r\n\t\toutputs = self.model.decoder(\r\n\t\t\tinput_ids=input_ids,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\tencoder_hidden_states=encoder_hidden_states,\r\n\t\t\tencoder_attention_mask=encoder_attention_mask,\r\n\t\t\thead_mask=head_mask,\r\n\t\t\tcross_attn_head_mask=cross_attn_head_mask,\r\n\t\t\tpast_key_values=past_key_values,\r\n\t\t\tinputs_embeds=inputs_embeds,\r\n\t\t\tuse_cache=use_cache,\r\n\t\t\toutput_attentions=output_attentions,\r\n\t\t\toutput_hidden_states=output_hidden_states,\r\n\t\t\treturn_dict=return_dict,\r\n\t\t)\r\n\r\n\t\tlogits = self.lm_head(outputs[0])\r\n\r\n\t\tloss = None\r\n\t\tif labels is not None:\r\n\t\t\tloss_fct = CrossEntropyLoss()\r\n\t\t\tloss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\r\n\r\n\t\tif not return_dict:\r\n\t\t\toutput = (logits,) + outputs[1:]\r\n\t\t\treturn (loss,) + output if loss is not None else output\r\n\r\n\t\treturn CausalLMOutputWithCrossAttentions(\r\n\t\t\tloss=loss,\r\n\t\t\tlogits=logits,\r\n\t\t\tpast_key_values=outputs.past_key_values,\r\n\t\t\thidden_states=outputs.hidden_states,\r\n\t\t\tattentions=outputs.attentions,\r\n\t\t\tcross_attentions=outputs.cross_attentions,\r\n\t\t)\r\n\r\n\tdef prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, use_cache=None, **kwargs):\r\n\t\t# if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\r\n\t\tif attention_mask is None:\r\n\t\t\tattention_mask = input_ids.new_ones(input_ids.shape)\r\n\r\n\t\tif past:\r\n\t\t\tinput_ids = input_ids[:, -1:]\r\n\t\t# first step, decoder_cached_states are empty\r\n\t\treturn {\r\n\t\t\t\"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\r\n\t\t\t\"attention_mask\": attention_mask,\r\n\t\t\t\"past_key_values\": past,\r\n\t\t\t\"use_cache\": use_cache,\r\n\t\t}\r\n\r\n\t@staticmethod\r\n\tdef _reorder_cache(past, beam_idx):\r\n\t\treordered_past = ()\r\n\t\tfor layer_past in past:\r\n\t\t\treordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\r\n\t\treturn reordered_past\r\n\r\n```\r\n- Model with Beam Search:\r\n```\r\nimport copy\r\nimport itertools\r\nfrom typing import List, Optional, Tuple\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\nfrom transformers import BartConfig\r\nfrom transformers.generation_utils import GenerationMixin\r\n\r\n\r\ndef _convert_past_list_to_tuple(past_key_values):\r\n    \"\"\"\r\n    In Bart model, the type of past_key_values is tuple(tuple(torch.FloatTensor)) which is not\r\n    TorchScript-compatible. To support this, we have to convert it during the export process.\r\n    This function will convert past values from a list to tuple(tuple(torch.FloatTensor)) for\r\n    the inner decoder.\r\n\r\n    According to the definition of past_key_values, each inner tuple(torch.FloatTensor) has 4 tensors,\r\n    so we convert every 4 elements in the list as a tuple(torch.FloatTensor).\r\n    \"\"\"\r\n    count_of_each_inner_tuple = 4\r\n    results = ()\r\n    temp_result = ()\r\n    count_n = len(past_key_values) // count_of_each_inner_tuple\r\n    for idx in range(count_n):\r\n        real_idx = idx * count_of_each_inner_tuple\r\n        temp_result = tuple(past_key_values[real_idx : real_idx + count_of_each_inner_tuple])\r\n        results += ((temp_result),)\r\n\r\n    return results\r\n\r\n\r\nclass EncoderForONNX(torch.nn.Module):\r\n    def __init__(self, encoder):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n\r\n    def forward(self, input_ids, attention_mask, token_type_ids):\r\n        return self.encoder(\r\n            input_ids=input_ids,\r\n            attention_mask=attention_mask,\r\n            token_type_ids=token_type_ids,\r\n            return_dict=False,\r\n        )\r\n\r\n\r\nclass DecoderForONNX(torch.nn.Module):\r\n    def __init__(self, decoder):\r\n        super().__init__()\r\n        self.decoder = decoder\r\n\r\n    def forward(self, input_ids, encoder_state, attention_mask, past=None):\r\n        all_results = None\r\n        if past is not None:\r\n            all_results = _convert_past_list_to_tuple(past)\r\n            input_ids = input_ids[:, -1:]\r\n\r\n        last_hidden_state, past_key_values = self.decoder(\r\n            input_ids=input_ids,\r\n            encoder_hidden_states=encoder_state,\r\n            encoder_attention_mask=attention_mask,\r\n            past_key_values=all_results,\r\n            return_dict=False,\r\n        )\r\n\r\n        past_values = []\r\n        for past in past_key_values:\r\n            past_values = past_values + list(past)\r\n        return last_hidden_state, past_values\r\n\r\n\r\ndef _create_traced_encoder(encoder, input_ids, attention_mask, token_type_ids):\r\n    encoder_c = copy.deepcopy(encoder)\r\n    encoder_for_onnx = EncoderForONNX(encoder_c)\r\n\r\n    return torch.jit.trace(encoder_for_onnx, (input_ids, attention_mask, token_type_ids))\r\n\r\n\r\ndef _create_traced_decoder(decoder, input_ids, encoder_state, attention_mask, past=None):\r\n    decoder_c = copy.deepcopy(decoder)\r\n    decoder_for_onnx = DecoderForONNX(decoder_c)\r\n    past_values = list(itertools.chain.from_iterable(past or ()))\r\n\r\n    # Do this twice so we got 2 different decoders for further work.\r\n    if past_values:\r\n        return torch.jit.trace(decoder_for_onnx, (input_ids, encoder_state, attention_mask, past_values))\r\n    else:\r\n        return torch.jit.trace(decoder_for_onnx, (input_ids, encoder_state, attention_mask))\r\n\r\n\r\nclass BartConfigTS(BartConfig, torch.nn.Module):\r\n    \"\"\"\r\n    BartConfigTS is a TorchScript-compatible transformers.models.bart.configuration_bart.BartConfig.\r\n    TorchScript only supports sub-classes of torch.nn.Module.\r\n    \"\"\"\r\n\r\n    def __init__(self, config):\r\n        BartConfig.__init__(self, config)\r\n        torch.nn.Module.__init__(self)\r\n\r\n\r\nclass MinLengthLogitsProcessorTS(torch.nn.Module):\r\n    r\"\"\"\r\n    :class:`transformers.LogitsProcessor` enforcing a min-length by setting EOS probability to 0.\r\n\r\n    Args:\r\n        min_length (:obj:`int`):\r\n            The minimum length below which the score of :obj:`eos_token_id` is set to :obj:`-float(\"Inf\")`.\r\n        eos_token_id (:obj:`int`):\r\n            The id of the `end-of-sequence` token.\r\n    \"\"\"\r\n\r\n    def __init__(self, min_length: int, eos_token_id: int):\r\n        super().__init__()\r\n\r\n        if not isinstance(min_length, int) or min_length < 0:\r\n            raise ValueError(f\"`min_length` has to be a positive integer, but is {min_length}\")\r\n\r\n        if not isinstance(eos_token_id, int) or eos_token_id < 0:\r\n            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\r\n\r\n        self.min_length = min_length\r\n        self.eos_token_id = eos_token_id\r\n\r\n    def forward(self, input_ids, scores) -> torch.Tensor:\r\n        cur_len = input_ids.shape[-1]\r\n        if cur_len < self.min_length:\r\n            scores[:, self.eos_token_id] = -float(\"inf\")\r\n        return scores\r\n\r\n\r\nclass UnknownTokenLogitsProcessorTS(torch.nn.Module):\r\n    def __init__(self, unk_token_id: int):\r\n        super().__init__()\r\n        self.unk_token_id = unk_token_id\r\n\r\n    def forward(self, scores) -> torch.Tensor:\r\n        scores[:, self.unk_token_id] = -float(\"inf\")\r\n        return scores\r\n\r\n\r\nclass BARTGenerator(torch.nn.Module, GenerationMixin):\r\n    def __init__(self, model):\r\n        super().__init__()\r\n        self.config = BartConfigTS(model.config)\r\n        self.config.force_bos_token_to_be_generated = False\r\n        self._trace_modules(model)\r\n        self.logits_processor = MinLengthLogitsProcessorTS(self.config.min_length, self.config.eos_token_id)\r\n        self.unk_token_id = 100\r\n        self.unknown_token_processor = UnknownTokenLogitsProcessorTS(self.unk_token_id)\r\n        self.final_logits_weight = model.model.shared.weight\r\n        self.final_logits_bias = model.final_logits_bias\r\n        self.decoder_layers = model.config.decoder_layers\r\n\r\n    def _trace_modules(self, model):\r\n        input_ids = torch.ones((1, 1024), dtype=torch.long, device=model.device)\r\n        attention_mask = torch.tensor(\r\n            [[True] * input_ids.shape[-1]],\r\n            device=model.device,\r\n            dtype=torch.bool,\r\n        )\r\n        token_type_ids = torch.tensor(\r\n            [[0] * 512 + [1] * (input_ids.shape[-1] - 512)],\r\n            device=model.device,\r\n            dtype=torch.long,\r\n        )\r\n        dec_input_ids = torch.ones((1, 100), dtype=torch.long, device=model.device)\r\n        print(input_ids.shape)\r\n        print(attention_mask.shape)\r\n        print(token_type_ids.shape)\r\n        self.encoder = _create_traced_encoder(model.get_encoder(), input_ids, attention_mask, token_type_ids)\r\n        encoder_outputs = model.get_encoder()(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\r\n        decoder = model.model.decoder\r\n        decoder_outputs = decoder(dec_input_ids, encoder_hidden_states=encoder_outputs[\"last_hidden_state\"], encoder_attention_mask=attention_mask)\r\n        self.decoder_no_past = _create_traced_decoder(\r\n            model.model.decoder, dec_input_ids, encoder_outputs[\"last_hidden_state\"], attention_mask \r\n        )\r\n        self.decoder_with_past = _create_traced_decoder(\r\n            model.model.decoder, dec_input_ids, encoder_outputs[\"last_hidden_state\"], attention_mask, decoder_outputs[1]\r\n        )\r\n\r\n    def _encoder_forward(self, input_ids, attention_mask, token_type_ids):\r\n        return self.encoder(input_ids, attention_mask, token_type_ids)[0]\r\n\r\n    @staticmethod\r\n    def _init_sequence_length_for_generation(\r\n        input_ids: torch.LongTensor, max_length: int\r\n    ) -> Tuple[torch.Tensor, torch.Tensor, int]:\r\n        unfinished_sequences = torch.zeros(input_ids.shape[0], dtype=torch.long, device=input_ids.device) + 1\r\n        sequence_lengths = torch.zeros(input_ids.shape[0], dtype=torch.long, device=input_ids.device) + max_length\r\n\r\n        cur_len = input_ids.shape[-1]\r\n        return sequence_lengths, unfinished_sequences, cur_len\r\n\r\n    def _decoder_forward(self, input_ids, encoder_output, attention_mask, past: List[torch.Tensor]):\r\n        # Update here to use different decoder for different values of past.\r\n        if past is None or len(past) == 0:\r\n            decoder_output, past = self.decoder_no_past(\r\n                input_ids=input_ids, encoder_state=encoder_output, attention_mask=attention_mask\r\n            )\r\n        else:\r\n            decoder_output, past = self.decoder_with_past(\r\n                input_ids=input_ids, encoder_state=encoder_output, attention_mask=attention_mask, past=past\r\n            )\r\n\r\n        lm_logits = F.linear(decoder_output, self.final_logits_weight, bias=self.final_logits_bias)\r\n\r\n        return lm_logits, past\r\n\r\n    def greedy_search(\r\n        self, input_ids, encoder_output, attention_mask, max_length, pad_token_id: int, eos_token_id: int\r\n    ):\r\n        # init sequence length tensors\r\n        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(\r\n            input_ids, max_length\r\n        )\r\n\r\n        past: List[torch.Tensor] = []\r\n        while cur_len < max_length:\r\n\r\n            logits, past = self._decoder_forward(input_ids, encoder_output, attention_mask, past)\r\n            next_token_logits = logits[:, -1, :]\r\n\r\n            # pre-process distribution\r\n            scores = self.logits_processor(input_ids, next_token_logits)\r\n\r\n            # argmax\r\n            next_tokens = torch.argmax(scores, dim=-1)\r\n\r\n            # add code that transfomers next_tokens to tokens_to_add\r\n            if eos_token_id is not None:\r\n                assert pad_token_id is not None, \"If eos_token_id is defined, make sure that pad_token_id is defined.\"\r\n                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)\r\n\r\n            # add token and increase length by one\r\n            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\r\n\r\n            # update sequence length\r\n            if eos_token_id is not None:\r\n                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(\r\n                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\r\n                )\r\n\r\n            # stop when there is a </s> in each sentence, or if we exceed the maximul length\r\n            if unfinished_sequences.max() == 0:\r\n                break\r\n\r\n            # increase cur_len\r\n            cur_len = cur_len + 1\r\n\r\n        return input_ids\r\n\r\n    def _prepare_decoder_input_ids_for_generation(\r\n        self,\r\n        input_ids: torch.LongTensor,\r\n        decoder_input_ids: torch.LongTensor,\r\n        bos_token_id: Optional[int] = None,\r\n    ) -> torch.LongTensor:\r\n        # if not decoder_input_ids:\r\n        #     decoder_input_ids = (\r\n        #         torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)\r\n        #         * decoder_start_token_id\r\n        #     )\r\n        return decoder_input_ids\r\n\r\n    def forward(self, input_ids, attention_mask, max_length, decoder_start_token_id):\r\n        pad_token_id = self.config.pad_token_id\r\n        bos_token_id = self.config.bos_token_id\r\n        eos_token_id = self.config.eos_token_id\r\n\r\n        # special case if pad_token_id is not defined\r\n        if pad_token_id is None and eos_token_id is not None:\r\n            # Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\r\n            pad_token_id = eos_token_id\r\n\r\n        encoder_output = self._encoder_forward(input_ids, attention_mask)\r\n\r\n        input_ids = self._prepare_decoder_input_ids_for_generation(\r\n            input_ids,\r\n            decoder_start_token_id=decoder_start_token_id,\r\n            bos_token_id=bos_token_id,\r\n        )\r\n\r\n        return self.greedy_search(\r\n            input_ids,\r\n            encoder_output,\r\n            attention_mask,\r\n            max_length=max_length,\r\n            pad_token_id=pad_token_id,\r\n            eos_token_id=eos_token_id,\r\n        )\r\n\r\n\r\n# TorchScript compatible BeamSearchScorer\r\nclass BeamSearchScorerTS(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.max_length: int = 200\r\n        self.num_beams: int = 3\r\n        self.batch_size: int = 1\r\n        self.length_penalty: float = 1.0\r\n        self.do_early_stopping: bool = True\r\n        self.num_beam_hyps_to_keep: int = 1\r\n        self.num_beam_groups: int = 1\r\n        self.group_size: int = self.num_beams // self.num_beam_groups\r\n        self._done = torch.zeros(self.batch_size, dtype=torch.bool)\r\n        self._beam_hyps_count = torch.zeros(self.batch_size, dtype=torch.long)\r\n        self._beam_hyps_worst_scores = torch.zeros(self.batch_size) + 1e9\r\n        self._beam_hyps_max_length: int = self.max_length - 1\r\n        self._beam_hyps: List[torch.Tensor] = [torch.zeros(2)]  # placeholder for TorchScript compatibility\r\n        self._beam_scores: List[torch.Tensor] = [torch.zeros(2)]  # placeholder for TorchScript compatibility\r\n\r\n    def is_done(self) -> torch.Tensor:\r\n        return self._done.all()\r\n\r\n    def init(\r\n        self,\r\n        batch_size: int,\r\n        max_length: int,\r\n        num_beams: int,\r\n        device: torch.device,\r\n        length_penalty: float = 1.0,\r\n        do_early_stopping: bool = False,\r\n        num_beam_hyps_to_keep: int = 1,\r\n        num_beam_groups: int = 1,\r\n    ):\r\n        self.max_length = max_length\r\n        self.num_beams = num_beams\r\n        self.batch_size = batch_size\r\n        self.length_penalty = length_penalty\r\n        self.do_early_stopping = do_early_stopping\r\n        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\r\n        self.num_beam_groups = num_beam_groups\r\n        self.group_size = self.num_beams // self.num_beam_groups\r\n\r\n        # NOTE: TorchScript does not support List of Modules\r\n        #       Rewritten BeamHypotheses with tensors and list of tensors.\r\n        self._done = torch.zeros(batch_size, dtype=torch.bool, device=device)\r\n        self._beam_hyps_count = torch.zeros(batch_size, dtype=torch.long, device=device)\r\n        self._beam_hyps_worst_scores = torch.zeros(batch_size, device=device) + 1e9\r\n        self._beam_hyps = []\r\n        self._beam_scores = []\r\n\r\n        self._beam_hyps_max_length = max_length - 1  # ignoring bos_token\r\n\r\n        if not isinstance(num_beams, int) or num_beams <= 1:\r\n            raise ValueError(\r\n                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\r\n                \" one should make use of `greedy_search` instead.\"\r\n            )\r\n\r\n        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\r\n            raise ValueError(\r\n                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\r\n                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\r\n            )\r\n\r\n    def hypo_len(self, hypo_idx: int):\r\n        \"\"\"\r\n        Number of hypotheses in the list.\r\n        \"\"\"\r\n        return self._beam_hyps_count[hypo_idx]\r\n\r\n    def hypo_add(self, hyp: torch.Tensor, sum_logprobs: float, hypo_idx: int):\r\n        \"\"\"\r\n        Add a new hypothesis to the list.\r\n        \"\"\"\r\n        score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\r\n        hyps_count = self.hypo_len(hypo_idx)\r\n        if hyps_count < self.num_beams or score > self._beam_hyps_worst_scores[hypo_idx]:\r\n            # NOTE: work around difference of torch.sum(empty_tensor) == 0, while error in onnx.\r\n            # Bug: https://msdata.visualstudio.com/Vienna/_workitems/edit/1486599\r\n            beam_idx = (\r\n                torch.sum(self._beam_hyps_count[:hypo_idx]) if hypo_idx != 0 else torch.tensor(0, dtype=torch.long)\r\n            )\r\n            self._beam_scores.insert(beam_idx, torch.tensor([score]))\r\n            self._beam_hyps.insert(beam_idx, hyp)\r\n            if hyps_count + 1 > self.num_beams:\r\n                sorted_next_scores, sorted_indices = torch.topk(\r\n                    torch.cat(self._beam_scores)[beam_idx : beam_idx + hyps_count + 1], hyps_count + 1, largest=False\r\n                )\r\n                del self._beam_hyps[int((sorted_indices[0] + beam_idx))]\r\n                del self._beam_scores[int((sorted_indices[0] + beam_idx))]\r\n                self._beam_hyps_worst_scores[hypo_idx] = sorted_next_scores[1]\r\n            else:\r\n                self._beam_hyps_worst_scores[hypo_idx] = min(score, self._beam_hyps_worst_scores[hypo_idx])\r\n                self._beam_hyps_count[hypo_idx] = hyps_count + 1\r\n\r\n    def hypo_is_done(self, hypo_idx: int, best_sum_logprobs: float, cur_len: int) -> bool:\r\n        \"\"\"\r\n        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\r\n        one in the heap, then we are done with this sentence.\r\n        \"\"\"\r\n        if self.hypo_len(hypo_idx) < self.num_beams:\r\n            return False\r\n        elif self.do_early_stopping:\r\n            return True\r\n        else:\r\n            cur_score = best_sum_logprobs / cur_len**self.length_penalty\r\n            ret = self._beam_hyps_worst_scores[hypo_idx].item() >= cur_score\r\n            return ret\r\n\r\n    def process(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        next_scores: torch.Tensor,\r\n        next_tokens: torch.Tensor,\r\n        next_indices: torch.Tensor,\r\n        pad_token_id: Optional[int] = None,\r\n        eos_token_id: Optional[int] = None,\r\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\r\n        cur_len = input_ids.shape[-1]\r\n        batch_size = len(self._beam_hyps_count)\r\n        assert batch_size == (input_ids.shape[0] // self.group_size)\r\n\r\n        device = input_ids.device\r\n        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\r\n        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\r\n        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\r\n\r\n        for batch_idx in range(batch_size):\r\n            if self._done[batch_idx]:\r\n                assert (\r\n                    self.hypo_len(batch_idx) >= self.num_beams\r\n                ), \"Batch can only be done if at least {} beams have been generated\".format(self.num_beams)\r\n                assert (\r\n                    eos_token_id is not None and pad_token_id is not None\r\n                ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\r\n                # pad the batch\r\n                next_beam_scores[batch_idx, :] = 0\r\n                next_beam_tokens[batch_idx, :] = pad_token_id\r\n                next_beam_indices[batch_idx, :] = 0\r\n                continue\r\n\r\n            # next tokens for this sentence\r\n            beam_idx = 0\r\n            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\r\n                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\r\n            ):\r\n                batch_beam_idx = batch_idx * self.group_size + next_index\r\n                # add to generated hypotheses if end of sentence\r\n                if (eos_token_id is not None) and (next_token == eos_token_id):\r\n                    # if beam_token does not belong to top num_beams tokens, it should not be added\r\n                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\r\n                    if is_beam_token_worse_than_top_num_beams:\r\n                        continue\r\n                    self.hypo_add(\r\n                        input_ids[batch_beam_idx].clone(),\r\n                        next_score.item(),\r\n                        batch_idx,\r\n                    )\r\n                else:\r\n                    # add next predicted token since it is not eos_token\r\n                    next_beam_scores[batch_idx, beam_idx] = next_score\r\n                    next_beam_tokens[batch_idx, beam_idx] = next_token\r\n                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\r\n                    beam_idx += 1\r\n\r\n                # once the beam for next step is full, don't add more tokens to it.\r\n                if beam_idx == self.group_size:\r\n                    break\r\n\r\n            if beam_idx < self.group_size:\r\n                raise ValueError(\r\n                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\r\n                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\r\n                )\r\n\r\n            # Check if we are done so that we can save a pad step if all(done)\r\n            self._done[batch_idx] = self._done[batch_idx] or self.hypo_is_done(\r\n                batch_idx,\r\n                next_scores[batch_idx].max().item(),\r\n                cur_len,\r\n            )\r\n\r\n        return next_beam_scores.view(-1), next_beam_tokens.view(-1), next_beam_indices.view(-1)\r\n\r\n    def finalize(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        final_beam_scores: torch.Tensor,\r\n        final_beam_tokens: torch.Tensor,\r\n        final_beam_indices: torch.Tensor,\r\n        pad_token_id: int,\r\n        eos_token_id: int,\r\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        batch_size = len(self._beam_hyps_count)\r\n\r\n        # finalize all open beam hypotheses and add to generated hypotheses\r\n        for batch_idx in range(batch_size):\r\n            if self._done[batch_idx]:\r\n                continue\r\n\r\n            # all open beam hypotheses are added to the beam hypothesis\r\n            # beam hypothesis class automatically keeps the best beams\r\n            for beam_id in range(self.num_beams):\r\n                batch_beam_idx = batch_idx * self.num_beams + beam_id\r\n                final_score = final_beam_scores[batch_beam_idx].item()\r\n                final_tokens = input_ids[batch_beam_idx]\r\n                self.hypo_add(final_tokens, final_score, batch_idx)\r\n\r\n        # select the best hypotheses\r\n        # NOTE: torch.Tensor.new_zeros() is not scriptable\r\n        sent_lengths = torch.zeros(batch_size * self.num_beam_hyps_to_keep, dtype=torch.long)\r\n        best = []\r\n        best_scores = torch.zeros(\r\n            batch_size * self.num_beam_hyps_to_keep, device=input_ids.device, dtype=torch.float32\r\n        )\r\n        # retrieve best hypotheses\r\n        for i in range(batch_size):\r\n            # NOTE: lambda is not scriptable\r\n            batch_hypo_start = torch.sum(self._beam_hyps_count[:i]) if i > 0 else torch.tensor(0, dtype=torch.long)\r\n            batch_hypo_end = torch.sum(self._beam_hyps_count[: i + 1])\r\n            beam_scores = torch.cat(self._beam_scores)[batch_hypo_start:batch_hypo_end]\r\n            sorted_next_scores, sorted_indices = torch.topk(beam_scores, len(beam_scores), largest=True)\r\n            for j in range(self.num_beam_hyps_to_keep):\r\n                best_score = beam_scores[sorted_indices[j]]\r\n                best_hyp = self._beam_hyps[batch_hypo_start + sorted_indices[j]]\r\n                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\r\n                # append to lists\r\n                best.append(best_hyp)\r\n                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\r\n\r\n        # prepare for adding eos\r\n        sent_max_len = min(sent_lengths.max() + 1, self.max_length)\r\n        decoded = torch.zeros(batch_size * self.num_beam_hyps_to_keep, sent_max_len, dtype=torch.long)\r\n        # shorter batches are padded if needed\r\n        if sent_lengths.min() != sent_lengths.max():\r\n            assert pad_token_id is not None, \"`pad_token_id` has to be defined\"\r\n            decoded.fill_(pad_token_id)\r\n\r\n        # fill with hypotheses and eos_token_id if the latter fits in\r\n        for i, hypo in enumerate(best):\r\n            decoded[i, : sent_lengths[i]] = hypo\r\n            if sent_lengths[i] < self.max_length:\r\n                decoded[i, sent_lengths[i]] = eos_token_id\r\n\r\n        return decoded, best_scores\r\n\r\n\r\nclass BARTBeamSearchGenerator(BARTGenerator):\r\n    def __init__(self, model):\r\n        super().__init__(model)\r\n        self.beam_scorer = BeamSearchScorerTS()\r\n        self.device = model.device\r\n\r\n    @staticmethod\r\n    def _expand_inputs_for_generation(\r\n        input_ids: torch.Tensor,\r\n        attention_mask: torch.Tensor,\r\n        last_hidden_state: torch.Tensor,\r\n        expand_size: int = 1,\r\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\r\n        expanded_return_idx = (\r\n            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)\r\n        )\r\n        input_ids = input_ids.index_select(0, expanded_return_idx)\r\n\r\n        attention_mask = attention_mask.index_select(0, expanded_return_idx)\r\n\r\n        last_hidden_state = last_hidden_state.index_select(0, expanded_return_idx.to(last_hidden_state.device))\r\n        \r\n        return input_ids, attention_mask, last_hidden_state\r\n\r\n    def adjust_logits_during_generation(self, logits, cur_len: int, max_length: int):\r\n        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\r\n            logits = self._force_token_id_to_be_generated(logits, self.config.bos_token_id)\r\n        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\r\n            logits = self._force_token_id_to_be_generated(logits, self.config.eos_token_id)\r\n        return logits\r\n\r\n    @staticmethod\r\n    def _force_token_id_to_be_generated(scores, token_id: int):\r\n        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float(\"inf\"))\"\"\"\r\n        mask = torch.full_like(scores, 1, dtype=torch.bool)\r\n        mask[:, token_id] = False\r\n        return scores.masked_fill(mask, -float(\"inf\"))\r\n\r\n    def _reorder_cache(self, past: List[torch.Tensor], beam_idx):\r\n        # if decoder past is not included in output\r\n        # speedy decoding is disabled and no need to reorder\r\n        reordered_decoder_past = []\r\n        for state in past:\r\n            reordered_decoder_past.append(state.index_select(0, beam_idx))\r\n        return reordered_decoder_past\r\n\r\n    def beam_search(\r\n        self, input_ids, encoder_output, attention_mask, num_beams, max_length, pad_token_id: int, eos_token_id: int\r\n    ):\r\n\r\n        batch_size = self.beam_scorer.batch_size\r\n\r\n        num_beams = self.beam_scorer.num_beams\r\n        batch_beam_size, cur_len = input_ids.shape\r\n\r\n        assert (\r\n            num_beams * batch_size == batch_beam_size\r\n        ), f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\r\n\r\n        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\r\n        beam_scores[:, 1:] = -1e9\r\n        beam_scores = beam_scores.view((batch_size * num_beams,))\r\n        next_tokens = torch.zeros((batch_size, num_beams), dtype=torch.long, device=input_ids.device)\r\n        next_indices = torch.zeros((batch_size, num_beams), dtype=torch.long, device=input_ids.device)\r\n\r\n        past: List[torch.Tensor] = []\r\n        while cur_len < max_length:\r\n            logits, past = self._decoder_forward(input_ids, encoder_output, attention_mask, past)\r\n            next_token_logits = logits[:, -1, :]\r\n\r\n            # adjust tokens for Bart, *e.g.*\r\n            next_token_logits = self.adjust_logits_during_generation(\r\n                next_token_logits, cur_len=cur_len, max_length=max_length\r\n            )\r\n\r\n            next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\r\n\r\n            # pre-process distribution\r\n            next_token_scores = self.logits_processor(input_ids, next_token_scores)\r\n            next_token_scores = self.unknown_token_processor(next_token_scores)\r\n            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\r\n\r\n            # reshape for beam search\r\n            vocab_size = next_token_scores.shape[-1]\r\n            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\r\n\r\n            next_token_scores, next_tokens = torch.topk(\r\n                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\r\n            )\r\n\r\n            next_indices = next_tokens // vocab_size\r\n            next_tokens = next_tokens % vocab_size\r\n\r\n            beam_scores, beam_next_tokens, beam_idx = self.beam_scorer.process(\r\n                input_ids,\r\n                next_token_scores,\r\n                next_tokens,\r\n                next_indices,\r\n                pad_token_id=pad_token_id,\r\n                eos_token_id=eos_token_id,\r\n            )\r\n\r\n            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\r\n\r\n            cur_len = cur_len + 1\r\n\r\n            if len(past) > 0:\r\n                past = self._reorder_cache(past, beam_idx)\r\n\r\n            if self.beam_scorer.is_done():\r\n                break\r\n\r\n        sequences, sequence_scores = self.beam_scorer.finalize(\r\n            input_ids,\r\n            beam_scores,\r\n            next_tokens,\r\n            next_indices,\r\n            pad_token_id=pad_token_id,\r\n            eos_token_id=eos_token_id,\r\n        )\r\n\r\n        return sequences\r\n\r\n    def forward(self, input_ids, attention_mask, token_type_ids, decoder_input_ids, params, float_params):\r\n        num_beams = params[0]\r\n        max_length = params[1]\r\n        eos_token_id = params[2]\r\n        length_penalty = float_params[0]\r\n        pad_token_id = self.config.pad_token_id\r\n        bos_token_id = self.config.bos_token_id\r\n        # eos_token_id = self.config.eos_token_id\r\n\r\n        # special case if pad_token_id is not defined\r\n        if pad_token_id is None and eos_token_id is not None:\r\n            # logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\r\n            pad_token_id = eos_token_id\r\n\r\n        encoder_output = self._encoder_forward(input_ids, attention_mask, token_type_ids)\r\n\r\n        input_ids = self._prepare_decoder_input_ids_for_generation(\r\n            input_ids,\r\n            decoder_input_ids,\r\n            bos_token_id=bos_token_id,\r\n        )\r\n\r\n        batch_size = input_ids.shape[0]\r\n\r\n        num_return_sequences = self.config.num_return_sequences\r\n        early_stopping = True\r\n\r\n        self.beam_scorer.init(\r\n            batch_size=batch_size,\r\n            max_length=max_length,\r\n            num_beams=num_beams,\r\n            device=self.device,\r\n            length_penalty=length_penalty,\r\n            do_early_stopping=early_stopping,\r\n            num_beam_hyps_to_keep=num_return_sequences,\r\n        )\r\n\r\n        input_ids, attention_mask, encoder_output = self._expand_inputs_for_generation(\r\n            input_ids,\r\n            attention_mask,\r\n            encoder_output,\r\n            expand_size=num_beams,\r\n        )\r\n\r\n        return self.beam_search(\r\n            input_ids=input_ids,\r\n            encoder_output=encoder_output,\r\n            attention_mask=attention_mask,\r\n            num_beams=num_beams,\r\n            max_length=max_length,\r\n            pad_token_id=pad_token_id,\r\n            eos_token_id=eos_token_id,\r\n        )\r\n\r\n```\r\n- Export:\r\n```\r\n#!/usr/bin/env python\r\n# coding=utf-8\r\n# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\"\"\"\r\n\r\n\"\"\"\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nimport numpy as np\r\nimport torch\r\n\r\nimport onnxruntime\r\nimport transformers\r\nfrom bart_onnx.generation_onnx import BARTBeamSearchGenerator\r\nfrom bart_onnx.reduce_onnx_size import remove_dup_initializers\r\nfrom transformers import MBartTokenizer\r\nfrom bart_onnx.modeling_mbart import MBartForConditionalGeneration\r\n\r\n\r\nlogging.basicConfig(\r\n\tformat=\"%(asctime)s | %(levelname)s | %(name)s |  [%(filename)s:%(lineno)d] %(message)s\",\r\n\tdatefmt=\"%Y-%m-%d %H:%M:%S\",\r\n\tlevel=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\r\n\tstream=sys.stdout,\r\n)\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nmodel_dict = {\"/mnt/ked/onnx_convert/mbart-large-cc25\": MBartForConditionalGeneration}\r\ntokenizer_dict = {\"/mnt/ked/onnx_convert/mbart-large-cc25\": MBartTokenizer}\r\n\r\n\r\ndef _strip_prefix_if_present(state_dict, prefix: str) -> None:\r\n\t\"\"\"\r\n\tStrip the prefix in metadata, if any.\r\n\tArgs:\r\n\t\tstate_dict (OrderedDict): a state-dict to be loaded to the model.\r\n\t\tprefix (str): prefix.\r\n\t\"\"\"\r\n\tkeys = sorted(state_dict.keys())\r\n\tif not all(len(key) == 0 or key.startswith(prefix) for key in keys):\r\n\t\treturn\r\n\r\n\tfor key in keys:\r\n\t\tnewkey = key[len(prefix) :]\r\n\t\tstate_dict[newkey] = state_dict.pop(key)\r\n\r\n\t# also strip the prefix in metadata, if any..\r\n\ttry:\r\n\t\tmetadata = state_dict._metadata  # pyre-ignore\r\n\texcept AttributeError:\r\n\t\tpass\r\n\telse:\r\n\t\tfor key in list(metadata.keys()):\r\n\t\t\t# for the metadata dict, the key can be:\r\n\t\t\t# '': for the DDP module, which we want to remove.\r\n\t\t\t# 'module': for the actual model.\r\n\t\t\t# 'module.xx.xx': for the rest.\r\n\r\n\t\t\tif len(key) == 0:\r\n\t\t\t\tcontinue\r\n\t\t\tnewkey = key[len(prefix) :]\r\n\t\t\tmetadata[newkey] = metadata.pop(key)\r\n\r\n\r\ndef parse_args():\r\n\tparser = argparse.ArgumentParser(description=\"Export Bart model + Beam Search to ONNX graph.\")\r\n\tparser.add_argument(\r\n\t\t\"--validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\"\r\n\t)\r\n\tparser.add_argument(\r\n\t\t\"--max_length\",\r\n\t\ttype=int,\r\n\t\tdefault=150,\r\n\t\thelp=\"The maximum total input sequence length after tokenization.\",\r\n\t)\r\n\tparser.add_argument(\r\n\t\t\"--num_beams\",\r\n\t\ttype=int,\r\n\t\tdefault=None,\r\n\t\thelp=(\r\n\t\t\t\"Number of beams to use for evaluation. This argument will be \"\r\n\t\t\t\"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``.\"\r\n\t\t),\r\n\t)\r\n\tparser.add_argument(\r\n\t\t\"--model_name_or_path\",\r\n\t\ttype=str,\r\n\t\thelp=\"Path to pretrained model or model identifier from huggingface.co/models.\",\r\n\t\trequired=True,\r\n\t)\r\n\tparser.add_argument(\r\n\t\t\"--config_name\",\r\n\t\ttype=str,\r\n\t\tdefault=None,\r\n\t\thelp=\"Pretrained config name or path if not the same as model_name\",\r\n\t)\r\n\tparser.add_argument(\r\n\t\t\"--device\",\r\n\t\ttype=str,\r\n\t\tdefault=\"cpu\",\r\n\t\thelp=\"Device where the model will be run\",\r\n\t)\r\n\tparser.add_argument(\"--output_file_path\", type=str, default=None, help=\"Where to store the final ONNX file.\")\r\n\r\n\targs = parser.parse_args()\r\n\r\n\treturn args\r\n\r\n\r\ndef load_model_tokenizer(model_name, device=\"cpu\"):\r\n\thuggingface_model = model_dict[model_name].from_pretrained(model_name).to(device)\r\n\tstate_dict = torch.load(model_name + '/pytorch_model.bin', map_location=device)\r\n\t_strip_prefix_if_present(state_dict, \"module.\")\r\n\thuggingface_model.load_state_dict(state_dict, strict=True)\r\n\ttokenizer = tokenizer_dict[model_name].from_pretrained(model_name)\r\n\r\n\treturn huggingface_model, tokenizer\r\n\r\n\r\ndef export_and_validate_model(model, tokenizer: MBartTokenizer, onnx_file_path, num_beams, max_length, length_penalty):\r\n\tmodel.eval()\r\n\r\n\tort_sess = None\r\n\tbart_script_model = torch.jit.script(BARTBeamSearchGenerator(model))\r\n\r\n\twith torch.no_grad():\r\n\t\tARTICLE_TO_SUMMARIZE = \"183cm19941994121619941994$160,638,8831994199419941993200720082011Brad Pitt19631218  1987 1993  1995  2002B 2005  2007  2008 812010 \"\r\n\t\tcontext = [\"\", \"\", \"\", \"183\", \"\"]\r\n\t\tdata_dict = {\r\n\t\t\t'knowledge': ARTICLE_TO_SUMMARIZE,\r\n\t\t\t'context': context,\r\n\t\t}\r\n\r\n\t\tlang = 'zh_CN'\r\n\t\tknowledge_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data_dict['knowledge']))[:512-2] + [tokenizer.sep_token_id, tokenizer.lang_code_to_id[lang]]\r\n\t\tknowledge_len = len(knowledge_id)\r\n\t\tknowledge_token_type_ids = [2] * 512\r\n\t\tcontext = []\r\n\t\tcontext_len = 0\r\n\t\tcontext_max_len = 512- 1\r\n\t\tcontext_token_type_ids = []\r\n\t\tspeaker = 0\r\n\t\tfor i in range(len(data_dict['context'])-1, -1, -1):\r\n\t\t\tcontext_id_now = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data_dict['context'][i])) + [tokenizer.sep_token_id]\r\n\t\t\tif context_len + len(context_id_now) <= context_max_len:\r\n\t\t\t\tcontext = context_id_now + context\r\n\t\t\t\tcontext_len += len(context_id_now)\r\n\t\t\t\tcontext_token_type_ids = [speaker] * len(context_id_now) + context_token_type_ids\r\n\t\t\t\tspeaker = 1 - speaker\r\n\t\t\telse:\r\n\t\t\t\tbreak\r\n\t\tif len(context) == 0: # if context is empty, add [SEP]\r\n\t\t\tcontext.append(tokenizer.sep_token_id)\r\n\t\t\tcontext_token_type_ids.append(speaker)\r\n\t\t\tcontext_len += 1\r\n\t\tcontext = context + [tokenizer.lang_code_to_id[lang]]\r\n\t\tcontext_token_type_ids = context_token_type_ids + [0]\r\n\t\tcontext_len += 1\r\n\t\tcontext_token_type_ids += [1] * (512 - context_len)\r\n\t\t\r\n\t\tinput_ids = torch.LongTensor([knowledge_id + [tokenizer.pad_token_id] * (512 - knowledge_len) + context + [tokenizer.pad_token_id] * (512 - context_len)])\r\n\t\tattention_mask = torch.LongTensor([[1] * knowledge_len + [0] * (512 - knowledge_len) + [1] * context_len + [0] * (512 - context_len)])\r\n\t\ttoken_type_ids = torch.LongTensor([knowledge_token_type_ids + context_token_type_ids])\r\n\t\tprint(input_ids.shape)\r\n\t\tprint(attention_mask.shape)\r\n\t\tprint(token_type_ids.shape)\r\n\t\tdecoder_input_ids = torch.LongTensor([tokenizer.lang_code_to_id[lang]]).unsqueeze(0).to(model.device)\r\n\t\t\r\n\t\tsummary_ids = model.generate(\r\n\t\t\tinput_ids=input_ids,\r\n\t\t\tattention_mask=attention_mask,\r\n\t\t\ttoken_type_ids=token_type_ids,\r\n\t\t\tdecoder_input_ids=decoder_input_ids,\r\n\t\t\tnum_beams=num_beams,\r\n\t\t\tmax_length=max_length,\r\n\t\t\tmin_length=2,\r\n\t\t\tlength_penalty=length_penalty\r\n\t\t)\r\n\t\tprint(summary_ids)\r\n\t\tprint(tokenizer.decode(summary_ids[0]))\r\n\r\n\t\tparams = torch.LongTensor([num_beams, max_length, tokenizer.eos_token_id])\r\n\t\tfloat_params = torch.DoubleTensor([length_penalty])\r\n\r\n\t\ttorch.onnx.export(\r\n\t\t\tbart_script_model,\r\n\t\t\t(\r\n\t\t\t\tinput_ids,\r\n\t\t\t\tattention_mask,\r\n\t\t\t\ttoken_type_ids,\r\n\t\t\t\tdecoder_input_ids,\r\n\t\t\t\tparams,\r\n\t\t\t\tfloat_params\r\n\t\t\t),\r\n\t\t\tonnx_file_path,\r\n\t\t\topset_version=14,\r\n\t\t\tinput_names=[\"input_ids\", \"attention_mask\", 'token_type_ids', 'decoder_input_ids', \"params\", \"float_params\"],\r\n\t\t\toutput_names=[\"output_ids\"],\r\n\t\t\tdynamic_axes={\r\n\t\t\t\t\"input_ids\": {0: \"batch\", 1: \"seq\"},\r\n\t\t\t\t\"attention_mask\": {0: \"batch\", 1: \"seq\"},\r\n\t\t\t\t\"token_type_ids\": {0: \"batch\", 1: \"seq\"},\r\n\t\t\t\t\"decoder_input_ids\": {0: \"batch\", 1: \"dec_seq_in\"},\r\n\t\t\t\t\"output_ids\": {0: \"batch\", 1: \"seq_out\"},\r\n\t\t\t},\r\n\t\t\t\r\n\t\t\t# example_outputs=summary_ids\r\n\t\t)\r\n\r\n\t\tlogger.info(\"Model exported to {}\".format(onnx_file_path))\r\n\r\n\t\tort_sess = onnxruntime.InferenceSession(onnx_file_path)\r\n\t\tort_out = ort_sess.run(\r\n\t\t\tNone,\r\n\t\t\t{\r\n\t\t\t\t\"input_ids\": input_ids.cpu().numpy(),\r\n\t\t\t\t\"attention_mask\": attention_mask.cpu().numpy(),\r\n\t\t\t\t\"token_type_ids\": token_type_ids.cpu().numpy(),\r\n\t\t\t\t\"decoder_input_ids\": decoder_input_ids.cpu().numpy(),\r\n\t\t\t\t\"params\": np.array([num_beams, max_length, tokenizer.eos_token_id]),\r\n\t\t\t\t\"float_params\": np.array([length_penalty]),\r\n\t\t\t},\r\n\t\t)\r\n\r\n\t\tprint(summary_ids)\r\n\t\tprint(ort_out[0])\r\n\r\n\t\tnp.testing.assert_allclose(summary_ids.cpu().numpy(), ort_out[0], rtol=1e-3, atol=1e-3)\r\n\r\n\t\tlogger.info(\"Model outputs from torch and ONNX Runtime are similar.\")\r\n\t\tlogger.info(\"Success.\")\r\n\r\n\r\ndef main():\r\n\targs = parse_args()\r\n\tmax_length = 150\r\n\tnum_beams = 4\r\n\tlength_penalty = 1.6\r\n\r\n\t# Make one log on every process with the configuration for debugging.\r\n\tlogging.basicConfig(\r\n\t\tformat=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\r\n\t\tdatefmt=\"%m/%d/%Y %H:%M:%S\",\r\n\t\tlevel=logging.INFO,\r\n\t)\r\n\r\n\tlogger.setLevel(logging.INFO)\r\n\ttransformers.utils.logging.set_verbosity_error()\r\n\r\n\tdevice = torch.device(args.device)\r\n\r\n\tmodel, tokenizer = load_model_tokenizer(args.model_name_or_path, device)\r\n\r\n\t# if model.config.decoder_start_token_id is None:\r\n\t\t# raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\r\n\r\n\tmodel.to(device)\r\n\r\n\tif args.max_length:\r\n\t\tmax_length = args.max_length\r\n\r\n\tif args.num_beams:\r\n\t\tnum_beams = args.num_beams\r\n\r\n\tif args.output_file_path:\r\n\t\toutput_name = args.output_file_path\r\n\telse:\r\n\t\toutput_name = \"BART.onnx\"\r\n\r\n\tlogger.info(\"Exporting model to ONNX\")\r\n\texport_and_validate_model(model, tokenizer, output_name, num_beams, max_length, length_penalty)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\tmain()\r\n\r\n```\r\n- Load: (modelPath is the path to \"mbart.onnx\" in export.zip)\r\n```\r\n_inferSession = new InferenceSession(modelPath);\r\n```\r\n\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n- https://microsoftapc-my.sharepoint.com/:u:/g/personal/t-xinhaozhao_microsoft_com/EUL23b7I7ndDv8grmbZdNsEBeai8IZk4zLaeCgGAld2iiQ?e=gYJ0Ft\r\n\r\n**Expected behavior**\r\n1. Export to one file\r\n2. If it must be multiple files, load them when they are in same directory.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n![image](https://user-images.githubusercontent.com/23635588/184584686-707a7ddd-5b85-4c5c-9ce0-213b8702edc6.png)\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12592/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12592/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12593",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12593/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12593/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12593/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12593",
        "id": 1338585807,
        "node_id": "PR_kwDOCVq1mM49KOw_",
        "number": 12593,
        "title": "WIP: [TVM EP] Support zero copying TVM EP output tensor to ONNX Runtime output tensor",
        "user": {
            "login": "vvchernov",
            "id": 28704584,
            "node_id": "MDQ6VXNlcjI4NzA0NTg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28704584?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vvchernov",
            "html_url": "https://github.com/vvchernov",
            "followers_url": "https://api.github.com/users/vvchernov/followers",
            "following_url": "https://api.github.com/users/vvchernov/following{/other_user}",
            "gists_url": "https://api.github.com/users/vvchernov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vvchernov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vvchernov/subscriptions",
            "organizations_url": "https://api.github.com/users/vvchernov/orgs",
            "repos_url": "https://api.github.com/users/vvchernov/repos",
            "events_url": "https://api.github.com/users/vvchernov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vvchernov/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T06:17:51Z",
        "updated_at": "2022-08-25T05:58:21Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12593",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12593",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12593.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12593.patch",
            "merged_at": null
        },
        "body": "**Description**:\r\nSupport new feature of TVM Virtual Machine (method `set_outputs`) on TVM Execution Provider side. It allows to avoid excess copying from TVM EP output tensor to ONNX Runtime one\r\n\r\n**Motivation and Context**\r\nTests with multiple output topologies and big output tensors shows that there is overheads spent on copying from TVM EP to ONNX Runtime. Returning output(s) on preallocated memory for VirtualMachine was implemented on TVM side.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12593/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12593/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12594",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12594/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12594/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12594/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12594",
        "id": 1338966334,
        "node_id": "I_kwDOCVq1mM5PzwE-",
        "number": 12594,
        "title": "Name:'MatMul_32007' Status Message: matmul_helper.h:61 Compute MatMul dimension mismatch",
        "user": {
            "login": "12sf12",
            "id": 67464849,
            "node_id": "MDQ6VXNlcjY3NDY0ODQ5",
            "avatar_url": "https://avatars.githubusercontent.com/u/67464849?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/12sf12",
            "html_url": "https://github.com/12sf12",
            "followers_url": "https://api.github.com/users/12sf12/followers",
            "following_url": "https://api.github.com/users/12sf12/following{/other_user}",
            "gists_url": "https://api.github.com/users/12sf12/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/12sf12/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/12sf12/subscriptions",
            "organizations_url": "https://api.github.com/users/12sf12/orgs",
            "repos_url": "https://api.github.com/users/12sf12/repos",
            "events_url": "https://api.github.com/users/12sf12/events{/privacy}",
            "received_events_url": "https://api.github.com/users/12sf12/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 2014185961,
                "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/core%20runtime",
                "name": "core runtime",
                "color": "006B75",
                "default": false,
                "description": "issues related to core runtime"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-15T12:56:02Z",
        "updated_at": "2022-09-03T02:51:29Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Bug:\r\n\r\nI can convert the pretrained model listed in this repository: https://github.com/salesforce/BLIP, However, at the inference time, I receive this error message: Name:'MatMul_32007' Status Message: matmul_helper.h:61 Compute MatMul dimension mismatch\r\n\r\n**To Reproduce**\r\n\r\nBefore exporting the model, the forward() function starting from line 105 from here (https://github.com/salesforce/BLIP/blob/main/models/blip.py#L105), should change to: \r\n------------------------------------------------\r\n    def forward(self, image):\r\n        \r\n        image_embeds = self.visual_encoder(image)\r\n        image_embeds = image_embeds.repeat_interleave(5,dim=0)\r\n            \r\n        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\r\n        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\r\n        \r\n        prompt = [self.prompt] * image.size(0)\r\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device) \r\n        input_ids[:,0] = self.tokenizer.bos_token_id\r\n        input_ids = input_ids[:, :-1]\r\n        \r\n        ops = self.text_decoder.generate(input_ids=input_ids,\r\n                                              max_length=20,\r\n                                              min_length=5,\r\n                                              num_beams=5,\r\n                                              eos_token_id=self.tokenizer.sep_token_id,\r\n                                              pad_token_id=self.tokenizer.pad_token_id,     \r\n                                              repetition_penalty=1.0,\r\n                                              **model_kwargs)    \r\n        return ops\r\n------------------------------------------------\r\n\r\nTo export the model to ONNX, I used the following code:\r\n------------------------------------------------\r\n  import torch\r\n  from torchvision import transforms\r\n  from torchvision.transforms.functional import InterpolationMode\r\n  from models.blip import blip_decoder\r\n  \r\n  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n  image_size = 384\r\n  transform = transforms.Compose([\r\n      transforms.ToPILImage(),\r\n      transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\r\n      transforms.ToTensor(),\r\n      transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\r\n      ]) \r\n  \r\n  model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth' \r\n  model = blip_decoder(pretrained=model_url, image_size=384, vit='base')\r\n  model.eval()\r\n  print('export started')\r\n  \r\n  batch_size=1\r\n  sf = torch.randn(batch_size, 3, 384, 384)\r\n  model.to('cpu')\r\n  torch.onnx.export(model,              \r\n      sf,                         \r\n      \"BLIP_base.onnx\",   \r\n      export_params=True, \r\n      verbose = True,      \r\n      opset_version=14,         \r\n      do_constant_folding=True,  \r\n      input_names = ['input'],   \r\n      output_names = ['output'], \r\n      dynamic_axes={'input' : {0 : 'batch_size'},    # \r\n                    'output' : {0 : 'batch_size'}})\r\n  \r\n  print('Export ended')\r\n\r\n------------------------------------------------\r\n\r\nUsing these two modifications, the model accepts an image as an input and produces a tensor output having 20 entries. Having done the export, I used the following code to use for inference:\r\n------------------------------------------------\r\nimport torchvision.transforms.functional as F\r\nimport torch.onnx\r\nimport onnx\r\nimport onnxruntime\r\n\r\nimport PIL\r\nimport torchvision.transforms.functional as transform\r\n\r\npath_to_onnx_file=\"BLIP_base.onnx\" # path to the exported model\r\nort_session = onnxruntime.InferenceSession(path_to_onnx_file,providers=['TensorrtExecutionProvider',\r\n                                        'CUDAExecutionProvider',\r\n                                        'CPUExecutionProvider'])\r\n\r\ndef to_numpy(tensor):\r\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\r\n\r\nimage_path='out268.jpg' # change to any image \r\nPIL_image = PIL.Image.open(image_path)\r\n\r\ntensor_image = transform.to_tensor(PIL_image)\r\ntensor_image=tensor_image[None]\r\ntensor_image=F.resize(tensor_image, [384,384])\r\nsf_cpu= {'input': to_numpy(tensor_image)}\r\nort_outs=ort_session.run(None, sf_cpu)\r\n\r\n\r\n------------------------------------------------\r\n\r\nWhen I run the above code, I receive the following error:\r\n\r\n\r\n**Fail: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running MatMul node. Name:'MatMul_32007' Status Message: matmul_helper.h:61 Compute MatMul dimension mismatch**\r\n\r\nI have seen other people try to fix this mismatch issue for the Cuda part, but it did not work for me as I want to run it using CPU mode. It is of great importance to fix the issue ASAP.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n- ONNX Runtime installed from pip command.\r\n- ONNX Runtime version: 1.12.1\r\n- Python version: 3.9\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.3\r\n- GPU model and memory: Nvidia Geforce 3090, 24GB\r\n\r\n\r\n**Expected behavior**\r\nIt should output a tensor list having 20 elements\r\n\r\n**Screenshots**\r\n![sf](https://user-images.githubusercontent.com/67464849/184638601-62facddc-faf3-4c73-b474-94236b2c65de.PNG)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12594/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12594/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12595",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12595/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12595/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12595/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12595",
        "id": 1339184612,
        "node_id": "I_kwDOCVq1mM5P0lXk",
        "number": 12595,
        "title": "RUNTIME_EXCEPTION : Non-zero status code returned while running Mul node. Name:'Mul_5'",
        "user": {
            "login": "SineStriker",
            "id": 55847490,
            "node_id": "MDQ6VXNlcjU1ODQ3NDkw",
            "avatar_url": "https://avatars.githubusercontent.com/u/55847490?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SineStriker",
            "html_url": "https://github.com/SineStriker",
            "followers_url": "https://api.github.com/users/SineStriker/followers",
            "following_url": "https://api.github.com/users/SineStriker/following{/other_user}",
            "gists_url": "https://api.github.com/users/SineStriker/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SineStriker/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SineStriker/subscriptions",
            "organizations_url": "https://api.github.com/users/SineStriker/orgs",
            "repos_url": "https://api.github.com/users/SineStriker/repos",
            "events_url": "https://api.github.com/users/SineStriker/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SineStriker/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T16:09:13Z",
        "updated_at": "2022-08-15T16:57:44Z",
        "closed_at": "2022-08-15T16:57:44Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\nA clear and concise description of what the bug is. To avoid repetition please make sure this is not one of the known issues mentioned on the respective release page.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 19044.1889\r\n- ONNX Runtime installed from (binary):\r\n- ONNX Runtime version: 1.12.1\r\n- Python version: 3.9\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: Nvidia 2070 8G\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n+ Export options\r\n````\r\n# infer_ins is a derived class from torch.nn.Module\r\ninfer_ins.pe.to('cpu')\r\nwith torch.no_grad():\r\n    mel_input = torch.rand(1, 968, 80)\r\n\r\n    torch.onnx.export(\r\n        infer_ins.pe,\r\n        (\r\n            mel_input\r\n        ),\r\n        \"xiaoma_pe.onnx\",\r\n        verbose=True,\r\n        input_names=[\"mel_input\"],\r\n        output_names=[\"f0\"],\r\n        dynamic_axes={\r\n            \"mel_input\": {\r\n                0: \"batch_size\",\r\n                1: \"frames\",\r\n                2: \"num_mel_bin\",\r\n            },\r\n            \"f0\": {\r\n                0: \"batch_size\",\r\n                1: \"frames\"\r\n            }\r\n        },\r\n        opset_version=11,\r\n    )\r\n````\r\n\r\n+ Forward function\r\n````\r\ndef forward(self, mel_input=None):\r\n    ret = {}\r\n    mel_hidden = self.mel_prenet(mel_input)[1]\r\n    if self.conv_layers > 0:\r\n        mel_hidden = self.mel_encoder(mel_hidden)\r\n\r\n    ret['pitch_pred'] = pitch_pred = self.pitch_predictor(mel_hidden)\r\n\r\n    pitch_padding = mel_input.abs().sum(-1) == 0\r\n    use_uv = hparams['pitch_type'] == 'frame' and hparams['use_uv']\r\n\r\n    ret['f0_denorm_pred'] = denorm_f0(\r\n        pitch_pred[:, :, 0], (pitch_pred[:, :, 1] > 0) if use_uv else None,\r\n        hparams, pitch_padding=pitch_padding)\r\n    return ret\r\n````\r\n\r\n+ Use case\r\n````\r\nmel_input = torch.rand(1, 967, 80)\r\npe2 = ort.InferenceSession(\"xiaoma_pe.onnx\")\r\nf0 = self.pe2.run(None, \r\n    {\r\n        'mel_input': to_numpy(mel_out)\r\n    }\r\n)\r\n````\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nNo Error.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n````\r\n2022-08-16 00:07:27.3322084 [E:onnxruntime:, sequential_executor.cc:368 onnxruntime::SequentialExecutor::Execute] Non-zero status code returned while running Mul node. Name:'Mul_5' Status Message: D:\\a\\_work\\1\\s\\onnxruntime\\core/providers/cpu/math/element_wise_ops.h:503 onnxruntime::BroadcastIterator::Init axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 967 by 968\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\truef\\Documents\\GitHub\\DiffSinger\\onnx_test_hifigan_pe.py\", line 99, in <module>\r\n    out = infer_ins.infer_once(c)\r\n  File \"C:\\Users\\truef\\Documents\\GitHub\\DiffSinger\\inference\\svs\\base_svs_infer.py\", line 235, in infer_once\r\n    output = self.forward_model(inp)\r\n  File \"C:\\Users\\truef\\Documents\\GitHub\\DiffSinger\\onnx_test_hifigan_pe.py\", line 68, in forward_model\r\n    f0 = self.pe2.run(None,\r\n  File \"C:\\CodeEnv\\miniconda3\\envs\\dfs\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py\", line 200, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Mul node. Name:'Mul_5' Status Message: D:\\a\\_work\\1\\s\\onnxruntime\\core/providers/cpu/math/element_wise_ops.h:503 onnxruntime::BroadcastIterator::Init axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 967 by 968\r\n````\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12595/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12595/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12596",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12596/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12596/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12596/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12596",
        "id": 1339186708,
        "node_id": "PR_kwDOCVq1mM49MP6k",
        "number": 12596,
        "title": "Add csharp docfx",
        "user": {
            "login": "cassiebreviu",
            "id": 46505951,
            "node_id": "MDQ6VXNlcjQ2NTA1OTUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/46505951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cassiebreviu",
            "html_url": "https://github.com/cassiebreviu",
            "followers_url": "https://api.github.com/users/cassiebreviu/followers",
            "following_url": "https://api.github.com/users/cassiebreviu/following{/other_user}",
            "gists_url": "https://api.github.com/users/cassiebreviu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cassiebreviu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cassiebreviu/subscriptions",
            "organizations_url": "https://api.github.com/users/cassiebreviu/orgs",
            "repos_url": "https://api.github.com/users/cassiebreviu/repos",
            "events_url": "https://api.github.com/users/cassiebreviu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cassiebreviu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2022-08-15T16:11:22Z",
        "updated_at": "2022-08-25T14:51:32Z",
        "closed_at": "2022-08-25T14:51:32Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12596",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12596",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12596.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12596.patch",
            "merged_at": "2022-08-25T14:51:32Z"
        },
        "body": "**Description**: Add ApiDocs project to implement DocFX for C# automatic documentation.\r\n\r\nStaged changes: https://cassiebreviu.github.io/onnxruntime/docs/api/csharp/dest/api/index.html\r\n\r\n**Motivation and Context**\r\n- Better and automatic documentation for csharp onnxruntime API.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12596/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12596/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12597",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12597/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12597/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12597/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12597",
        "id": 1339211188,
        "node_id": "PR_kwDOCVq1mM49MVJZ",
        "number": 12597,
        "title": "Change TunableOp to use a type erased interface",
        "user": {
            "login": "cloudhan",
            "id": 1279292,
            "node_id": "MDQ6VXNlcjEyNzkyOTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1279292?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cloudhan",
            "html_url": "https://github.com/cloudhan",
            "followers_url": "https://api.github.com/users/cloudhan/followers",
            "following_url": "https://api.github.com/users/cloudhan/following{/other_user}",
            "gists_url": "https://api.github.com/users/cloudhan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cloudhan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cloudhan/subscriptions",
            "organizations_url": "https://api.github.com/users/cloudhan/orgs",
            "repos_url": "https://api.github.com/users/cloudhan/repos",
            "events_url": "https://api.github.com/users/cloudhan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cloudhan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T16:34:40Z",
        "updated_at": "2022-08-26T02:46:06Z",
        "closed_at": "2022-08-26T02:46:05Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12597",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12597",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12597.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12597.patch",
            "merged_at": "2022-08-26T02:46:05Z"
        },
        "body": "**Description**: Change the TunableOp to be type erased.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n   Currently, virtual base TunableOp mandate us to implement every kernal launch function by inherite and implement the `Op` interface. This will add alot of boilerplate code and is uncessary. After this change, the kernal launch functions can be implemented as functor class, free function or lambda.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12597/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12597/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12598",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12598/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12598/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12598/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12598",
        "id": 1339214761,
        "node_id": "I_kwDOCVq1mM5P0sup",
        "number": 12598,
        "title": "Build error with LLVM toolchain",
        "user": {
            "login": "hmchen-github",
            "id": 17942794,
            "node_id": "MDQ6VXNlcjE3OTQyNzk0",
            "avatar_url": "https://avatars.githubusercontent.com/u/17942794?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hmchen-github",
            "html_url": "https://github.com/hmchen-github",
            "followers_url": "https://api.github.com/users/hmchen-github/followers",
            "following_url": "https://api.github.com/users/hmchen-github/following{/other_user}",
            "gists_url": "https://api.github.com/users/hmchen-github/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hmchen-github/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hmchen-github/subscriptions",
            "organizations_url": "https://api.github.com/users/hmchen-github/orgs",
            "repos_url": "https://api.github.com/users/hmchen-github/repos",
            "events_url": "https://api.github.com/users/hmchen-github/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hmchen-github/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T16:38:33Z",
        "updated_at": "2022-08-17T17:49:56Z",
        "closed_at": "2022-08-17T17:49:56Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "static assertion failed due to requirement 'sizeof (t) < 0': hash<const char*> would hash the pointer, not the contents of the string, which is almost certainly not what you want. To explicitly hash the pointers, use const void*.\r\n    static_assert(sizeof(t) < 0,\r\n    ^             ~~~~~~~~~~~~~\r\nonnxruntime/include/onnxruntime/core/framework/ortmemoryinfo.h:51:17 in instantiation of function template specialization 'std::hash<const char *>::hash<int>' requested here\r\n    HashCombine(std::hash<const char*>()(name), h);\r\n                ^\r\n1 error generated.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12598/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12598/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12599",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12599/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12599/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12599/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12599",
        "id": 1339217916,
        "node_id": "PR_kwDOCVq1mM49MWkL",
        "number": 12599,
        "title": "Add Tunable GEMM composed from rocblas and composable kernels",
        "user": {
            "login": "cloudhan",
            "id": 1279292,
            "node_id": "MDQ6VXNlcjEyNzkyOTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1279292?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cloudhan",
            "html_url": "https://github.com/cloudhan",
            "followers_url": "https://api.github.com/users/cloudhan/followers",
            "following_url": "https://api.github.com/users/cloudhan/following{/other_user}",
            "gists_url": "https://api.github.com/users/cloudhan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cloudhan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cloudhan/subscriptions",
            "organizations_url": "https://api.github.com/users/cloudhan/orgs",
            "repos_url": "https://api.github.com/users/cloudhan/repos",
            "events_url": "https://api.github.com/users/cloudhan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cloudhan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T16:42:01Z",
        "updated_at": "2022-08-26T06:32:57Z",
        "closed_at": "2022-08-26T06:32:56Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12599",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12599",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12599.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12599.patch",
            "merged_at": "2022-08-26T06:32:56Z"
        },
        "body": "Based on #12597\r\n\r\n**Description**: Introduce Tunable GEMM, which is composed from rocblas and composable kernels\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n   composable kernels' implementations sometimes can outperform rocblas.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12599/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12599/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12600",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12600/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12600/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12600/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12600",
        "id": 1339223747,
        "node_id": "PR_kwDOCVq1mM49MXxS",
        "number": 12600,
        "title": "Fix a build error",
        "user": {
            "login": "hmchen-github",
            "id": 17942794,
            "node_id": "MDQ6VXNlcjE3OTQyNzk0",
            "avatar_url": "https://avatars.githubusercontent.com/u/17942794?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hmchen-github",
            "html_url": "https://github.com/hmchen-github",
            "followers_url": "https://api.github.com/users/hmchen-github/followers",
            "following_url": "https://api.github.com/users/hmchen-github/following{/other_user}",
            "gists_url": "https://api.github.com/users/hmchen-github/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hmchen-github/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hmchen-github/subscriptions",
            "organizations_url": "https://api.github.com/users/hmchen-github/orgs",
            "repos_url": "https://api.github.com/users/hmchen-github/repos",
            "events_url": "https://api.github.com/users/hmchen-github/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hmchen-github/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 10,
        "created_at": "2022-08-15T16:48:13Z",
        "updated_at": "2022-08-17T17:49:55Z",
        "closed_at": "2022-08-17T17:49:55Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12600",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12600",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12600.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12600.patch",
            "merged_at": "2022-08-17T17:49:55Z"
        },
        "body": "LLVM compiler complains the std::hash<const char*> and suggests std::hash<const void*>. But the intention is to hash the name string instead of the pointer. So use std::hash\\<std::string\\> to be explicit.\r\n\r\nFix #12598\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12600/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12600/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12601",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12601/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12601/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12601/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12601",
        "id": 1339489880,
        "node_id": "PR_kwDOCVq1mM49NMoi",
        "number": 12601,
        "title": "release calibrator before deleting temporary files",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T20:44:49Z",
        "updated_at": "2022-08-15T23:03:47Z",
        "closed_at": "2022-08-15T23:03:46Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12601",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12601",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12601.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12601.patch",
            "merged_at": "2022-08-15T23:03:46Z"
        },
        "body": "**Description**: Describe your changes.\r\n\r\nPR #12183 will holds the file until initializer are released. This will prevent temporary directory to delete external initializer file.\r\n\r\nFix: delete calibrator to release InferenceSession before removing temporary directory.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12601/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12601/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12602",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12602/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12602/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12602/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12602",
        "id": 1339510503,
        "node_id": "PR_kwDOCVq1mM49NRCs",
        "number": 12602,
        "title": "Add build option to link TensorRT prebuilt parser",
        "user": {
            "login": "yf711",
            "id": 109183385,
            "node_id": "U_kgDOBoIBmQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/109183385?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yf711",
            "html_url": "https://github.com/yf711",
            "followers_url": "https://api.github.com/users/yf711/followers",
            "following_url": "https://api.github.com/users/yf711/following{/other_user}",
            "gists_url": "https://api.github.com/users/yf711/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yf711/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yf711/subscriptions",
            "organizations_url": "https://api.github.com/users/yf711/orgs",
            "repos_url": "https://api.github.com/users/yf711/repos",
            "events_url": "https://api.github.com/users/yf711/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yf711/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-15T21:07:36Z",
        "updated_at": "2022-08-16T21:09:59Z",
        "closed_at": "2022-08-16T21:09:58Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12602",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12602",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12602.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12602.patch",
            "merged_at": "2022-08-16T21:09:58Z"
        },
        "body": "**Description**: Add new build option \"--use_tensorrt_parser\" in order to directly use TensorRT prebuilt parser lib instead of building parser from open-sourced repo",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12602/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12602/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12603",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12603/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12603/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12603/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12603",
        "id": 1339571599,
        "node_id": "PR_kwDOCVq1mM49NeK6",
        "number": 12603,
        "title": "Use InplaceClipGradNorm for offline processing for on-device training",
        "user": {
            "login": "baijumeswani",
            "id": 12852605,
            "node_id": "MDQ6VXNlcjEyODUyNjA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/12852605?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/baijumeswani",
            "html_url": "https://github.com/baijumeswani",
            "followers_url": "https://api.github.com/users/baijumeswani/followers",
            "following_url": "https://api.github.com/users/baijumeswani/following{/other_user}",
            "gists_url": "https://api.github.com/users/baijumeswani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/baijumeswani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/baijumeswani/subscriptions",
            "organizations_url": "https://api.github.com/users/baijumeswani/orgs",
            "repos_url": "https://api.github.com/users/baijumeswani/repos",
            "events_url": "https://api.github.com/users/baijumeswani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/baijumeswani/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1913759001,
                "node_id": "MDU6TGFiZWwxOTEzNzU5MDAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/training",
                "name": "training",
                "color": "BFD4F2",
                "default": false,
                "description": "issues related to ONNX Runtime training; typically submitted using template"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2022-08-15T22:12:42Z",
        "updated_at": "2022-09-02T14:47:19Z",
        "closed_at": "2022-09-02T14:47:18Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12603",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12603",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12603.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12603.patch",
            "merged_at": "2022-09-02T14:47:18Z"
        },
        "body": "This pull request:\r\n\r\n- Adds the CPU kernel for `InplaceClipGradNorm`\r\n- Updates the offline tooling `ClipGradNorm` to use the new `InplaceClipGradNorm` kernel\r\n- Updates the `Optimizer` class to work with gradients as a `TensorSeq` input.\r\n\r\nThis is an example of how the optimizer model with `AdamWOptimizer` used to look:\r\n![image](https://user-images.githubusercontent.com/12852605/185262789-9ffffd6c-de87-44cc-916e-41e4cecdd579.png)\r\n\r\nAnd here is the same model with the `TensorSeq` inputs for the gradientss:\r\n![image](https://user-images.githubusercontent.com/12852605/185262892-d967aabf-685c-49d4-814c-15f830e0b715.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12603/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12603/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12604",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12604/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12604/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12604/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12604",
        "id": 1339617171,
        "node_id": "PR_kwDOCVq1mM49Nnbn",
        "number": 12604,
        "title": "[Automated]: Update Java API docs",
        "user": {
            "login": "github-actions[bot]",
            "id": 41898282,
            "node_id": "MDM6Qm90NDE4OTgyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/15368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/github-actions%5Bbot%5D",
            "html_url": "https://github.com/apps/github-actions",
            "followers_url": "https://api.github.com/users/github-actions%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/github-actions%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/github-actions%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/github-actions%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/github-actions%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-15T23:05:39Z",
        "updated_at": "2022-08-24T17:08:40Z",
        "closed_at": "2022-08-24T17:08:39Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12604",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12604",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12604.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12604.patch",
            "merged_at": "2022-08-24T17:08:39Z"
        },
        "body": "Automated changes by [create-pull-request](https://github.com/peter-evans/create-pull-request) GitHub action",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12604/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12604/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12605",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12605/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12605/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12605/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12605",
        "id": 1339657685,
        "node_id": "PR_kwDOCVq1mM49NwPK",
        "number": 12605,
        "title": "nightly pipeline build using PTCA image.",
        "user": {
            "login": "AdamLouly",
            "id": 27873459,
            "node_id": "MDQ6VXNlcjI3ODczNDU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/27873459?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/AdamLouly",
            "html_url": "https://github.com/AdamLouly",
            "followers_url": "https://api.github.com/users/AdamLouly/followers",
            "following_url": "https://api.github.com/users/AdamLouly/following{/other_user}",
            "gists_url": "https://api.github.com/users/AdamLouly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/AdamLouly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/AdamLouly/subscriptions",
            "organizations_url": "https://api.github.com/users/AdamLouly/orgs",
            "repos_url": "https://api.github.com/users/AdamLouly/repos",
            "events_url": "https://api.github.com/users/AdamLouly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/AdamLouly/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-16T00:11:28Z",
        "updated_at": "2022-08-24T17:40:56Z",
        "closed_at": "2022-08-24T17:40:56Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12605",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12605",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12605.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12605.patch",
            "merged_at": "2022-08-24T17:40:56Z"
        },
        "body": "**Description**: \r\nNightly pipeline build.\r\n\r\n**Motivation and Context**\r\n- This pipeline will **NOT** get triggered with PRs and will only run when its scheduled.\r\n- The purpose is to catch the failures and errors on the nightly ort version instead of waiting for the stable version, this way we can solve these errors upfront while we have time.\r\n- This pipeline is projected to run on daily basis, but for now It will only run on a weekly basis, because it pulls the PTCA nightly image and this image gets generated every Friday,\r\nThe PTCA team is working on making it get generated on daily basis, until then the pipeline will be scheduled to run once a week (after the PTCA nightly image generation).\r\n- The previous PR (https://github.com/microsoft/onnxruntime/pull/12277) was closed and replaced with this one.\r\n- The pipeline was tested today 15th August 2022 after the PTCA image was generated on Friday, and it ran successfully ( [Pipeline Logs ](https://dev.azure.com/onnxruntime/onnxruntime/_build/results?buildId=722096&view=logs&j=a242bf24-7d40-50e3-91cc-3075088d1dc9&t=a242bf24-7d40-50e3-91cc-3075088d1dc9))",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12605/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12605/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12606",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12606/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12606/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12606/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12606",
        "id": 1339673711,
        "node_id": "PR_kwDOCVq1mM49Nzn1",
        "number": 12606,
        "title": "Add rust bindings",
        "user": {
            "login": "boydjohnson",
            "id": 4340785,
            "node_id": "MDQ6VXNlcjQzNDA3ODU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4340785?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/boydjohnson",
            "html_url": "https://github.com/boydjohnson",
            "followers_url": "https://api.github.com/users/boydjohnson/followers",
            "following_url": "https://api.github.com/users/boydjohnson/following{/other_user}",
            "gists_url": "https://api.github.com/users/boydjohnson/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/boydjohnson/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/boydjohnson/subscriptions",
            "organizations_url": "https://api.github.com/users/boydjohnson/orgs",
            "repos_url": "https://api.github.com/users/boydjohnson/repos",
            "events_url": "https://api.github.com/users/boydjohnson/events{/privacy}",
            "received_events_url": "https://api.github.com/users/boydjohnson/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-16T00:42:15Z",
        "updated_at": "2022-09-12T01:15:17Z",
        "closed_at": null,
        "author_association": "FIRST_TIME_CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12606",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12606",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12606.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12606.patch",
            "merged_at": null
        },
        "body": "This adds updated Rust bindings that have been located at [nbigaouette/onnxruntime-rs](https://github.com/nbigaouette/onnxruntime-rs).\r\n\r\ncheck out the build instructions included in this PR at /rust/BUILD.md.\r\n\r\nChanges to the bindings included in this PR:\r\n- The bindings are generated with the build script on each build\r\n- The onnxruntime shared library is built with ORT_RUST_STRATEGY=compile which is now the default.\r\n- A memory leak was fixed where a call to free wasn't called\r\n- Session is Send but not Sync, Environment is Send + Sync\r\n\r\nSome commits can be squashed, if wanted, but were left unsquashed to show differences between old bindings and new bindings.\r\n\r\nThis PR does not cover packaging nor does it include the Rust bindings withing the build system.\r\n\r\nFor those of you who have previous Rust code based on the bindings, these new bindings\r\ncan be used as a `path` dependency or a `git` dependency (though I have not tested this out).\r\n\r\nThe work addressed in this PR was discussed in #11992\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12606/reactions",
            "total_count": 5,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 4,
            "rocket": 1,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12606/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12607",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12607/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12607/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12607/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12607",
        "id": 1339750921,
        "node_id": "PR_kwDOCVq1mM49ODeT",
        "number": 12607,
        "title": "weight matching",
        "user": {
            "login": "chenfucn",
            "id": 1316708,
            "node_id": "MDQ6VXNlcjEzMTY3MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1316708?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chenfucn",
            "html_url": "https://github.com/chenfucn",
            "followers_url": "https://api.github.com/users/chenfucn/followers",
            "following_url": "https://api.github.com/users/chenfucn/following{/other_user}",
            "gists_url": "https://api.github.com/users/chenfucn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chenfucn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chenfucn/subscriptions",
            "organizations_url": "https://api.github.com/users/chenfucn/orgs",
            "repos_url": "https://api.github.com/users/chenfucn/repos",
            "events_url": "https://api.github.com/users/chenfucn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chenfucn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-16T03:11:23Z",
        "updated_at": "2022-08-18T16:51:41Z",
        "closed_at": "2022-08-17T18:01:11Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12607",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12607",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12607.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12607.patch",
            "merged_at": "2022-08-17T18:01:11Z"
        },
        "body": "**Description**: QDQ loss debug - Weights Matching\r\n\r\n**Motivation and Context**\r\nPart 2 of QDQ loss debugging tool: given a float model and its qdq model, return the matching of all weight tensors and their corresponding dequantized weights from the qdq model.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12607/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12607/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12608",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12608/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12608/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12608/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12608",
        "id": 1339911598,
        "node_id": "I_kwDOCVq1mM5P3W2u",
        "number": 12608,
        "title": "Run the onnx model converted from seq2seq and report an error",
        "user": {
            "login": "Amy234543",
            "id": 107381937,
            "node_id": "U_kgDOBmaEsQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/107381937?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Amy234543",
            "html_url": "https://github.com/Amy234543",
            "followers_url": "https://api.github.com/users/Amy234543/followers",
            "following_url": "https://api.github.com/users/Amy234543/following{/other_user}",
            "gists_url": "https://api.github.com/users/Amy234543/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Amy234543/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Amy234543/subscriptions",
            "organizations_url": "https://api.github.com/users/Amy234543/orgs",
            "repos_url": "https://api.github.com/users/Amy234543/repos",
            "events_url": "https://api.github.com/users/Amy234543/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Amy234543/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2022-08-16T07:09:23Z",
        "updated_at": "2022-08-17T03:11:08Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "error\r\n\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid Feed Input Name:past_key_values_18.1\r\n\r\n\r\nI installed\r\nonnxruntime-gpu    1.12.1\r\nonnx               1.12.0\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12608/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12608/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12609",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12609/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12609/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12609/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12609",
        "id": 1340166805,
        "node_id": "I_kwDOCVq1mM5P4VKV",
        "number": 12609,
        "title": "onnxruntime-web bug : 'return f4' is fast as normal, but 'return f1,f2,f3,f4' is very slow in webgl mode",
        "user": {
            "login": "yufengyao-lingoace",
            "id": 92853025,
            "node_id": "U_kgDOBYjTIQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/92853025?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufengyao-lingoace",
            "html_url": "https://github.com/yufengyao-lingoace",
            "followers_url": "https://api.github.com/users/yufengyao-lingoace/followers",
            "following_url": "https://api.github.com/users/yufengyao-lingoace/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufengyao-lingoace/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufengyao-lingoace/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufengyao-lingoace/subscriptions",
            "organizations_url": "https://api.github.com/users/yufengyao-lingoace/orgs",
            "repos_url": "https://api.github.com/users/yufengyao-lingoace/repos",
            "events_url": "https://api.github.com/users/yufengyao-lingoace/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufengyao-lingoace/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3066979818,
                "node_id": "MDU6TGFiZWwzMDY2OTc5ODE4",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:web",
                "name": "platform:web",
                "color": "FEF2C0",
                "default": false,
                "description": "issues related to ONNX Runtime web; typically submitted using template"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": {
            "login": "shalvamist",
            "id": 94086448,
            "node_id": "U_kgDOBZulMA",
            "avatar_url": "https://avatars.githubusercontent.com/u/94086448?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shalvamist",
            "html_url": "https://github.com/shalvamist",
            "followers_url": "https://api.github.com/users/shalvamist/followers",
            "following_url": "https://api.github.com/users/shalvamist/following{/other_user}",
            "gists_url": "https://api.github.com/users/shalvamist/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shalvamist/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shalvamist/subscriptions",
            "organizations_url": "https://api.github.com/users/shalvamist/orgs",
            "repos_url": "https://api.github.com/users/shalvamist/repos",
            "events_url": "https://api.github.com/users/shalvamist/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shalvamist/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "shalvamist",
                "id": 94086448,
                "node_id": "U_kgDOBZulMA",
                "avatar_url": "https://avatars.githubusercontent.com/u/94086448?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/shalvamist",
                "html_url": "https://github.com/shalvamist",
                "followers_url": "https://api.github.com/users/shalvamist/followers",
                "following_url": "https://api.github.com/users/shalvamist/following{/other_user}",
                "gists_url": "https://api.github.com/users/shalvamist/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/shalvamist/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/shalvamist/subscriptions",
                "organizations_url": "https://api.github.com/users/shalvamist/orgs",
                "repos_url": "https://api.github.com/users/shalvamist/repos",
                "events_url": "https://api.github.com/users/shalvamist/events{/privacy}",
                "received_events_url": "https://api.github.com/users/shalvamist/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 10,
        "created_at": "2022-08-16T10:52:02Z",
        "updated_at": "2022-09-15T05:42:49Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "(1)slow when return f1,f2,f3,f4\r\ndef forward(self, x):\r\n        f1 = self.b1(x)\r\n        f2 = self.b2(f1)  \r\n        f3 = self.b3(f2) \r\n        f4 = self.b4(f3)\r\n        return f1,f2,f3,f4\r\nwhen I return f1,f2,f3,f4  the onnx used in chrome is very slow.  (ort.min.js using  webgl)\r\n\r\n(2) fast when return f4\r\ndef forward(self, x):\r\n        f1 = self.b1(x)\r\n        f2 = self.b2(f1)  \r\n        f3 = self.b3(f2) \r\n        f4 = self.b4(f3)\r\n        return f4\r\nwhen I only return f4  the onnx used in chrome is very fast. (ort.min.js using webgl)\r\nbut in normal , the two way should be the same time\r\nI also find that \r\ndef forward(self, x):\r\n        f1 = self.b1(x)\r\n        f2 = self.b2(f1) \r\n        return f2\r\nis faster than\r\ndef forward(self, x):\r\n        f1 = self.b1(x)\r\n        return f1\r\nSo anyone can tell me why?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12609/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12609/timeline",
        "performed_via_github_app": null,
        "state_reason": "reopened"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12610",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12610/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12610/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12610/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12610",
        "id": 1340670422,
        "node_id": "PR_kwDOCVq1mM49RHqY",
        "number": 12610,
        "title": "Design draft for python training api bindings",
        "user": {
            "login": "AdamLouly",
            "id": 27873459,
            "node_id": "MDQ6VXNlcjI3ODczNDU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/27873459?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/AdamLouly",
            "html_url": "https://github.com/AdamLouly",
            "followers_url": "https://api.github.com/users/AdamLouly/followers",
            "following_url": "https://api.github.com/users/AdamLouly/following{/other_user}",
            "gists_url": "https://api.github.com/users/AdamLouly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/AdamLouly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/AdamLouly/subscriptions",
            "organizations_url": "https://api.github.com/users/AdamLouly/orgs",
            "repos_url": "https://api.github.com/users/AdamLouly/repos",
            "events_url": "https://api.github.com/users/AdamLouly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/AdamLouly/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2022-08-16T17:32:39Z",
        "updated_at": "2022-09-16T00:05:28Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12610",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12610",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12610.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12610.patch",
            "merged_at": null
        },
        "body": "**Description**: **Python API Bindings for on device training. **\r\n**Motivation and Context**\r\n- This PR contains api bindings so python users can perform a whole training loop.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12610/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12610/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12611",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12611/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12611/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12611/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12611",
        "id": 1340692411,
        "node_id": "PR_kwDOCVq1mM49RMLH",
        "number": 12611,
        "title": "[oneDNN] Improve DequantizeLinear operator performance.",
        "user": {
            "login": "eralmual",
            "id": 22269643,
            "node_id": "MDQ6VXNlcjIyMjY5NjQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/22269643?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eralmual",
            "html_url": "https://github.com/eralmual",
            "followers_url": "https://api.github.com/users/eralmual/followers",
            "following_url": "https://api.github.com/users/eralmual/following{/other_user}",
            "gists_url": "https://api.github.com/users/eralmual/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eralmual/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eralmual/subscriptions",
            "organizations_url": "https://api.github.com/users/eralmual/orgs",
            "repos_url": "https://api.github.com/users/eralmual/repos",
            "events_url": "https://api.github.com/users/eralmual/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eralmual/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2022-08-16T17:55:21Z",
        "updated_at": "2022-08-18T18:05:53Z",
        "closed_at": "2022-08-17T19:31:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12611",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12611",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12611.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12611.patch",
            "merged_at": "2022-08-17T19:31:10Z"
        },
        "body": "**Description**: Improved DequantizeLinear operator performance\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\nR/ Multiple models use the DequantizeLinear operator so it's performance is critical for multiple tasks\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12611/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12611/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12612",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12612/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12612/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12612/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12612",
        "id": 1340713296,
        "node_id": "PR_kwDOCVq1mM49RQi9",
        "number": 12612,
        "title": "User/linneamay/ri 8 16",
        "user": {
            "login": "linnealovespie",
            "id": 5448665,
            "node_id": "MDQ6VXNlcjU0NDg2NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5448665?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/linnealovespie",
            "html_url": "https://github.com/linnealovespie",
            "followers_url": "https://api.github.com/users/linnealovespie/followers",
            "following_url": "https://api.github.com/users/linnealovespie/following{/other_user}",
            "gists_url": "https://api.github.com/users/linnealovespie/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/linnealovespie/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/linnealovespie/subscriptions",
            "organizations_url": "https://api.github.com/users/linnealovespie/orgs",
            "repos_url": "https://api.github.com/users/linnealovespie/repos",
            "events_url": "https://api.github.com/users/linnealovespie/events{/privacy}",
            "received_events_url": "https://api.github.com/users/linnealovespie/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-16T18:16:51Z",
        "updated_at": "2022-08-16T20:02:17Z",
        "closed_at": "2022-08-16T19:24:47Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12612",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12612",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12612.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12612.patch",
            "merged_at": "2022-08-16T19:24:47Z"
        },
        "body": "Successful pipeline: [https://dev.azure.com/microsoft/WindowsAI/_build/results?buildId=54359262&view=logs&j=4e3dd845-80f6-5682-7592-d2eda4c1196a&t=b3ed071a-d172-5351-0dfc-02424ddb17a7](https://dev.azure.com/microsoft/WindowsAI/_build/results?buildId=54359262&view=logs&j=4e3dd845-80f6-5682-7592-d2eda4c1196a&t=b3ed071a-d172-5351-0dfc-02424ddb17a7\r\n)\r\n![image](https://user-images.githubusercontent.com/5448665/184950367-83ddbdc0-3f40-4a9a-a4bc-9bb6c17a3042.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12612/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12612/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12613",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12613/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12613/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12613/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12613",
        "id": 1340855011,
        "node_id": "PR_kwDOCVq1mM49RuJq",
        "number": 12613,
        "title": "Pytorch inference",
        "user": {
            "login": "natke",
            "id": 3302433,
            "node_id": "MDQ6VXNlcjMzMDI0MzM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3302433?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/natke",
            "html_url": "https://github.com/natke",
            "followers_url": "https://api.github.com/users/natke/followers",
            "following_url": "https://api.github.com/users/natke/following{/other_user}",
            "gists_url": "https://api.github.com/users/natke/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/natke/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/natke/subscriptions",
            "organizations_url": "https://api.github.com/users/natke/orgs",
            "repos_url": "https://api.github.com/users/natke/repos",
            "events_url": "https://api.github.com/users/natke/events{/privacy}",
            "received_events_url": "https://api.github.com/users/natke/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-16T20:37:48Z",
        "updated_at": "2022-09-03T02:03:22Z",
        "closed_at": "2022-09-03T02:03:22Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12613",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12613",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12613.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12613.patch",
            "merged_at": "2022-09-03T02:03:22Z"
        },
        "body": "New article demonstrating how to perform inference with PyTorch\r\n\r\nStaged here: https://natke.github.io/onnxruntime/docs/tutorials/accelerate-pytorch/pytorch.html",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12613/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12613/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12614",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12614/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12614/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12614/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12614",
        "id": 1340875753,
        "node_id": "PR_kwDOCVq1mM49RyiM",
        "number": 12614,
        "title": "Enabling softmax grad and logsoftmax grad on ORT",
        "user": {
            "login": "abhi-ort",
            "id": 109558862,
            "node_id": "U_kgDOBoe8Tg",
            "avatar_url": "https://avatars.githubusercontent.com/u/109558862?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abhi-ort",
            "html_url": "https://github.com/abhi-ort",
            "followers_url": "https://api.github.com/users/abhi-ort/followers",
            "following_url": "https://api.github.com/users/abhi-ort/following{/other_user}",
            "gists_url": "https://api.github.com/users/abhi-ort/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abhi-ort/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abhi-ort/subscriptions",
            "organizations_url": "https://api.github.com/users/abhi-ort/orgs",
            "repos_url": "https://api.github.com/users/abhi-ort/repos",
            "events_url": "https://api.github.com/users/abhi-ort/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abhi-ort/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2022-08-16T20:59:45Z",
        "updated_at": "2022-08-23T22:49:04Z",
        "closed_at": "2022-08-23T22:49:03Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12614",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12614",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12614.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12614.patch",
            "merged_at": "2022-08-23T22:49:02Z"
        },
        "body": "**Description**: Softmax backward and log softmax backward were falling back to Torch implementation. This PR enables running softmax backward and log softmax backward on ORT backend. Parametrized tests were added for softmax and log softmax backward when running on ort backend. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12614/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12614/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12615",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12615/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12615/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12615/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12615",
        "id": 1340941205,
        "node_id": "PR_kwDOCVq1mM49SAcK",
        "number": 12615,
        "title": "Add instruction of new build option to link TensorRT builtin parser",
        "user": {
            "login": "yf711",
            "id": 109183385,
            "node_id": "U_kgDOBoIBmQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/109183385?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yf711",
            "html_url": "https://github.com/yf711",
            "followers_url": "https://api.github.com/users/yf711/followers",
            "following_url": "https://api.github.com/users/yf711/following{/other_user}",
            "gists_url": "https://api.github.com/users/yf711/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yf711/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yf711/subscriptions",
            "organizations_url": "https://api.github.com/users/yf711/orgs",
            "repos_url": "https://api.github.com/users/yf711/repos",
            "events_url": "https://api.github.com/users/yf711/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yf711/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-16T22:22:43Z",
        "updated_at": "2022-08-17T15:26:18Z",
        "closed_at": "2022-08-17T15:26:17Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12615",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12615",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12615.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12615.patch",
            "merged_at": "2022-08-17T15:26:17Z"
        },
        "body": "**Description**: Add instruction of new build option to link TensorRT builtin parser",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12615/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12615/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12616",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12616/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12616/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12616/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12616",
        "id": 1340970467,
        "node_id": "PR_kwDOCVq1mM49SGgu",
        "number": 12616,
        "title": "Replace to lock_guard as lighter class for locking",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-16T23:12:37Z",
        "updated_at": "2022-08-17T18:08:32Z",
        "closed_at": "2022-08-17T18:08:32Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12616",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12616",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12616.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12616.patch",
            "merged_at": "2022-08-17T18:08:31Z"
        },
        "body": "**Description**: \r\n`std::unique_lock` -> `std::lock_guard`\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12616/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12616/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12617",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12617/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12617/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12617/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12617",
        "id": 1340979066,
        "node_id": "PR_kwDOCVq1mM49SIUr",
        "number": 12617,
        "title": "Try to fix broken link in Java API docs",
        "user": {
            "login": "natke",
            "id": 3302433,
            "node_id": "MDQ6VXNlcjMzMDI0MzM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3302433?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/natke",
            "html_url": "https://github.com/natke",
            "followers_url": "https://api.github.com/users/natke/followers",
            "following_url": "https://api.github.com/users/natke/following{/other_user}",
            "gists_url": "https://api.github.com/users/natke/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/natke/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/natke/subscriptions",
            "organizations_url": "https://api.github.com/users/natke/orgs",
            "repos_url": "https://api.github.com/users/natke/repos",
            "events_url": "https://api.github.com/users/natke/events{/privacy}",
            "received_events_url": "https://api.github.com/users/natke/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-16T23:28:20Z",
        "updated_at": "2022-08-23T17:02:39Z",
        "closed_at": "2022-08-23T17:02:39Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12617",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12617",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12617.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12617.patch",
            "merged_at": null
        },
        "body": "Link broken here: https://github.com/microsoft/onnxruntime/actions/runs/2870936232 \r\n\r\nThe incorrect method signature is being generated by javadoc",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12617/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12617/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12618",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12618/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12618/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12618/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12618",
        "id": 1340982326,
        "node_id": "PR_kwDOCVq1mM49SJAQ",
        "number": 12618,
        "title": "[NNAPI EP] Remove/Refactor shaper inference calculation code",
        "user": {
            "login": "YUNQIUGUO",
            "id": 35738743,
            "node_id": "MDQ6VXNlcjM1NzM4NzQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/35738743?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YUNQIUGUO",
            "html_url": "https://github.com/YUNQIUGUO",
            "followers_url": "https://api.github.com/users/YUNQIUGUO/followers",
            "following_url": "https://api.github.com/users/YUNQIUGUO/following{/other_user}",
            "gists_url": "https://api.github.com/users/YUNQIUGUO/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/YUNQIUGUO/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/YUNQIUGUO/subscriptions",
            "organizations_url": "https://api.github.com/users/YUNQIUGUO/orgs",
            "repos_url": "https://api.github.com/users/YUNQIUGUO/repos",
            "events_url": "https://api.github.com/users/YUNQIUGUO/events{/privacy}",
            "received_events_url": "https://api.github.com/users/YUNQIUGUO/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2022-08-16T23:34:13Z",
        "updated_at": "2022-09-15T23:17:54Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12618",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12618",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12618.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12618.patch",
            "merged_at": null
        },
        "body": "**Description**: Describe your changes.\r\nAs title.\r\n\r\nThe purpose of this pr is to eliminate as much of repetitive shape inference code in nnapi ep shaper struct.\r\n\r\nFor ops (mainly require composed operations) :\r\n-BatchNorm\r\n-Reshape \r\n-Squeeze (in one case of gemm operator)\r\n-BatchMatMul\r\nstill contains some shape calculation impl/logic.\r\n\r\nDynamic shape functions are not touched yet. \r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n\r\nClean up redundant code as cpu shape inference impl for NNAPI EP.\r\nGet rid of the shape inference code in NNAPI EP by using the static shape info in output NodeArg.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12618/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12618/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12621",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12621/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12621/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12621/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12621",
        "id": 1341093487,
        "node_id": "PR_kwDOCVq1mM49SfFH",
        "number": 12621,
        "title": "[Website] Update training install matrix",
        "user": {
            "login": "faxu",
            "id": 20780999,
            "node_id": "MDQ6VXNlcjIwNzgwOTk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/20780999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/faxu",
            "html_url": "https://github.com/faxu",
            "followers_url": "https://api.github.com/users/faxu/followers",
            "following_url": "https://api.github.com/users/faxu/following{/other_user}",
            "gists_url": "https://api.github.com/users/faxu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/faxu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/faxu/subscriptions",
            "organizations_url": "https://api.github.com/users/faxu/orgs",
            "repos_url": "https://api.github.com/users/faxu/repos",
            "events_url": "https://api.github.com/users/faxu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/faxu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-17T02:47:39Z",
        "updated_at": "2022-08-17T16:15:21Z",
        "closed_at": "2022-08-17T16:15:21Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12621",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12621",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12621.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12621.patch",
            "merged_at": "2022-08-17T16:15:21Z"
        },
        "body": "Preview: https://faxu.github.io/onnxruntime/\r\n\r\nChanges:\r\n- Update training installation matrix to reflect currently supported combos",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12621/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12621/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12622",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12622/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12622/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12622/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12622",
        "id": 1341236098,
        "node_id": "I_kwDOCVq1mM5P8aOC",
        "number": 12622,
        "title": " How do I understand the error message such as: 7508808",
        "user": {
            "login": "1093919186",
            "id": 61229971,
            "node_id": "MDQ6VXNlcjYxMjI5OTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/61229971?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/1093919186",
            "html_url": "https://github.com/1093919186",
            "followers_url": "https://api.github.com/users/1093919186/followers",
            "following_url": "https://api.github.com/users/1093919186/following{/other_user}",
            "gists_url": "https://api.github.com/users/1093919186/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/1093919186/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/1093919186/subscriptions",
            "organizations_url": "https://api.github.com/users/1093919186/orgs",
            "repos_url": "https://api.github.com/users/1093919186/repos",
            "events_url": "https://api.github.com/users/1093919186/events{/privacy}",
            "received_events_url": "https://api.github.com/users/1093919186/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3066979818,
                "node_id": "MDU6TGFiZWwzMDY2OTc5ODE4",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:web",
                "name": "platform:web",
                "color": "FEF2C0",
                "default": false,
                "description": "issues related to ONNX Runtime web; typically submitted using template"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": {
            "login": "shalvamist",
            "id": 94086448,
            "node_id": "U_kgDOBZulMA",
            "avatar_url": "https://avatars.githubusercontent.com/u/94086448?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shalvamist",
            "html_url": "https://github.com/shalvamist",
            "followers_url": "https://api.github.com/users/shalvamist/followers",
            "following_url": "https://api.github.com/users/shalvamist/following{/other_user}",
            "gists_url": "https://api.github.com/users/shalvamist/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shalvamist/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shalvamist/subscriptions",
            "organizations_url": "https://api.github.com/users/shalvamist/orgs",
            "repos_url": "https://api.github.com/users/shalvamist/repos",
            "events_url": "https://api.github.com/users/shalvamist/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shalvamist/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "shalvamist",
                "id": 94086448,
                "node_id": "U_kgDOBZulMA",
                "avatar_url": "https://avatars.githubusercontent.com/u/94086448?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/shalvamist",
                "html_url": "https://github.com/shalvamist",
                "followers_url": "https://api.github.com/users/shalvamist/followers",
                "following_url": "https://api.github.com/users/shalvamist/following{/other_user}",
                "gists_url": "https://api.github.com/users/shalvamist/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/shalvamist/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/shalvamist/subscriptions",
                "organizations_url": "https://api.github.com/users/shalvamist/orgs",
                "repos_url": "https://api.github.com/users/shalvamist/repos",
                "events_url": "https://api.github.com/users/shalvamist/events{/privacy}",
                "received_events_url": "https://api.github.com/users/shalvamist/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 15,
        "created_at": "2022-08-17T06:28:22Z",
        "updated_at": "2022-09-14T18:40:26Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "The model I converted with sklearn-onnx and run on onnxruntime-web, the error message is only: 7508808. How should I understand this error message\r\n\r\n**System information**\r\n- 1.11.0",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12622/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12622/timeline",
        "performed_via_github_app": null,
        "state_reason": "reopened"
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12623",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12623/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12623/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12623/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12623",
        "id": 1341251500,
        "node_id": "I_kwDOCVq1mM5P8d-s",
        "number": 12623,
        "title": "Where is the definition of session.Run() in onnxruntime C++ api",
        "user": {
            "login": "abilashravi-ta",
            "id": 53215532,
            "node_id": "MDQ6VXNlcjUzMjE1NTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/53215532?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abilashravi-ta",
            "html_url": "https://github.com/abilashravi-ta",
            "followers_url": "https://api.github.com/users/abilashravi-ta/followers",
            "following_url": "https://api.github.com/users/abilashravi-ta/following{/other_user}",
            "gists_url": "https://api.github.com/users/abilashravi-ta/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abilashravi-ta/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abilashravi-ta/subscriptions",
            "organizations_url": "https://api.github.com/users/abilashravi-ta/orgs",
            "repos_url": "https://api.github.com/users/abilashravi-ta/repos",
            "events_url": "https://api.github.com/users/abilashravi-ta/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abilashravi-ta/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2022-08-17T06:38:02Z",
        "updated_at": "2022-08-19T17:48:41Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Is your feature request related to a problem? Please describe.**\r\nWhere can I find the complete definition of the `Run()` function (the one used for onnx model _inferencing_ in `Ort::Session` in **C++**).\r\nI was able to find this definition (https://github.com/microsoft/onnxruntime/blob/v1.11.0/server/executor.cc#L82) but this in turn calls another `session.Run()` for which I couldn't find the definition. I have searched through the source code multiple times but couldn't figure out where it is.\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): v1.11.0\r\n\r\n**Describe the solution you'd like**\r\nA pointer to the base `Run()` function definition. Would also like to know how this functions calls the subsequent operations like `conv`, `gemm`, etc.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12623/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12623/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12625",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12625/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12625/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12625/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12625",
        "id": 1341407970,
        "node_id": "I_kwDOCVq1mM5P9ELi",
        "number": 12625,
        "title": "Customized allocator from EP is overwritten by default one in SessionState::SetupAllocators()",
        "user": {
            "login": "vvchernov",
            "id": 28704584,
            "node_id": "MDQ6VXNlcjI4NzA0NTg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28704584?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vvchernov",
            "html_url": "https://github.com/vvchernov",
            "followers_url": "https://api.github.com/users/vvchernov/followers",
            "following_url": "https://api.github.com/users/vvchernov/following{/other_user}",
            "gists_url": "https://api.github.com/users/vvchernov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vvchernov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vvchernov/subscriptions",
            "organizations_url": "https://api.github.com/users/vvchernov/orgs",
            "repos_url": "https://api.github.com/users/vvchernov/repos",
            "events_url": "https://api.github.com/users/vvchernov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vvchernov/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 2014185961,
                "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/core%20runtime",
                "name": "core runtime",
                "color": "006B75",
                "default": false,
                "description": "issues related to core runtime"
            },
            {
                "id": 4023383529,
                "node_id": "LA_kwDOCVq1mM7vz_Xp",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:tvm",
                "name": "ep:tvm",
                "color": "0052CC",
                "default": false,
                "description": "issues related to TVM execution provider"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-17T08:52:31Z",
        "updated_at": "2022-08-18T16:54:22Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "**Is your feature request related to a problem? Please describe.**\r\nONNX Runtime provides possibility to customize Allocator for specific Execution Provider. In my understanding it is base feature and if allocator is customized and specific EP is used the allocator should be used with it independently on target or platform. But it does not so. On the highest-level where InferenceSession is initialized the customized allocator can be changed by default one from CPU, GPU or CUDA EP.\r\nComments and TODOs in[ SessionState::SetupAllocators()](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/core/framework/session_state.cc#L27) method show that I'm right in my understanding. But why was the design done so but its problem hidden in comments?\r\nParticular case of the problem. I use TVM EP which assums that alignment of allocated memory is 128, but its allocator changed by CPU EP's (default) allocator and its alignment is defined by MLAS and usually smaller than 128 that is unacceptably.\r\n\r\n**System information**\r\n- ONNX Runtime version: 1.13.0\r\n\r\n**Describe the solution you'd like**\r\nSolution requires ORT design updating. I propose to discuss it here. Conceptually my view is the following. All allocators from EPs should be basically preferable. Problem related to multi-threading allocations and CPU, CUDA, ROCM and others EPs interconnection is particular case which should not define general behavior of the system.\r\n\r\n**Describe alternatives you've considered**\r\nCurrently I work with TVM EP and plan to fix it by condition extension. but I do not see quick and solid solution due to it requires strong design refactoring.\r\n\r\n**Additional context**\r\nProblem is concentrated in SessionState::SetupAllocators() method which is used by SessionState constructor.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12625/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12625/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12626",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12626/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12626/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12626/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12626",
        "id": 1341417260,
        "node_id": "PR_kwDOCVq1mM49Th1b",
        "number": 12626,
        "title": "Update composable kernel and enable experimental inter wave scheduling",
        "user": {
            "login": "cloudhan",
            "id": 1279292,
            "node_id": "MDQ6VXNlcjEyNzkyOTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1279292?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cloudhan",
            "html_url": "https://github.com/cloudhan",
            "followers_url": "https://api.github.com/users/cloudhan/followers",
            "following_url": "https://api.github.com/users/cloudhan/following{/other_user}",
            "gists_url": "https://api.github.com/users/cloudhan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cloudhan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cloudhan/subscriptions",
            "organizations_url": "https://api.github.com/users/cloudhan/orgs",
            "repos_url": "https://api.github.com/users/cloudhan/repos",
            "events_url": "https://api.github.com/users/cloudhan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cloudhan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2022-08-17T09:00:27Z",
        "updated_at": "2022-08-26T05:19:42Z",
        "closed_at": "2022-08-26T05:19:42Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12626",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12626",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12626.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12626.patch",
            "merged_at": "2022-08-26T05:19:41Z"
        },
        "body": "**Description**: Use the latest develop branch of composable kernel.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n   Track composable kernel upstream bug fix and perf improvement. Enable the experimental inter wave schedualing feature will improve the performance.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12626/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12626/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12627",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12627/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12627/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12627/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12627",
        "id": 1341438445,
        "node_id": "PR_kwDOCVq1mM49TmaG",
        "number": 12627,
        "title": "[TVM EP] fix issue #12625 for TVM EP",
        "user": {
            "login": "vvchernov",
            "id": 28704584,
            "node_id": "MDQ6VXNlcjI4NzA0NTg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28704584?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vvchernov",
            "html_url": "https://github.com/vvchernov",
            "followers_url": "https://api.github.com/users/vvchernov/followers",
            "following_url": "https://api.github.com/users/vvchernov/following{/other_user}",
            "gists_url": "https://api.github.com/users/vvchernov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vvchernov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vvchernov/subscriptions",
            "organizations_url": "https://api.github.com/users/vvchernov/orgs",
            "repos_url": "https://api.github.com/users/vvchernov/repos",
            "events_url": "https://api.github.com/users/vvchernov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vvchernov/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4023383529,
                "node_id": "LA_kwDOCVq1mM7vz_Xp",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:tvm",
                "name": "ep:tvm",
                "color": "0052CC",
                "default": false,
                "description": "issues related to TVM execution provider"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 12,
        "created_at": "2022-08-17T09:17:52Z",
        "updated_at": "2022-09-08T20:04:18Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12627",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12627",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12627.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12627.patch",
            "merged_at": null
        },
        "body": "**Description**:\r\nUpdate conditions in SessionState::SetupAllocators()\r\nThis is particular pacth which does not solve general problem for all EPs (see #12625) only for TVM EP\r\n\r\n**Motivation and Context**\r\nCustomized TVM EP Allocator is overwritten by default Allocator from CPU EP on high-level which leads to problem with memory allocation.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12627/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12627/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12628",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12628/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12628/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12628/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12628",
        "id": 1341853048,
        "node_id": "PR_kwDOCVq1mM49VA7f",
        "number": 12628,
        "title": "ppc64le: mlas: fix both MaximumFloat and MinimumFloat to return NAN",
        "user": {
            "login": "maxiwell",
            "id": 2990132,
            "node_id": "MDQ6VXNlcjI5OTAxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2990132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/maxiwell",
            "html_url": "https://github.com/maxiwell",
            "followers_url": "https://api.github.com/users/maxiwell/followers",
            "following_url": "https://api.github.com/users/maxiwell/following{/other_user}",
            "gists_url": "https://api.github.com/users/maxiwell/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/maxiwell/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/maxiwell/subscriptions",
            "organizations_url": "https://api.github.com/users/maxiwell/orgs",
            "repos_url": "https://api.github.com/users/maxiwell/repos",
            "events_url": "https://api.github.com/users/maxiwell/events{/privacy}",
            "received_events_url": "https://api.github.com/users/maxiwell/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2022-08-17T14:28:04Z",
        "updated_at": "2022-09-05T21:16:30Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12628",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12628",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12628.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12628.patch",
            "merged_at": null
        },
        "body": "Avoid using vec_max/vec_min because their behaviors are undefined if one of\r\nthe elements is NAN. The Power Vector Intrinsic Programming Reference says:\r\n\r\n\"For floating-point types, if both source elements contain signed\r\nzeros, or if either source element contains a NaN, it is\r\nundefined which of the two source elements is copied into\r\nthe corresponding result element.\"\r\n\r\nAs the unittest Activation.ShortExecute expects NAN, this patch uses\r\nvec_sel and vec_cmpgt to return NAN if one of the elements is NAN.\r\n\r\nhttps://git.openpower.foundation/systemsoftware/Programming-Guides/src/branch/master/Intrinsics_Reference/ch_vec_reference.xml#L26808\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12628/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12628/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12629",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12629/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12629/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12629/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12629",
        "id": 1341878123,
        "node_id": "I_kwDOCVq1mM5P-29r",
        "number": 12629,
        "title": "Scikit-learn model converted to ONNX results in different output shapes between Python and Java environments",
        "user": {
            "login": "dan-kur",
            "id": 95598696,
            "node_id": "U_kgDOBbK4aA",
            "avatar_url": "https://avatars.githubusercontent.com/u/95598696?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dan-kur",
            "html_url": "https://github.com/dan-kur",
            "followers_url": "https://api.github.com/users/dan-kur/followers",
            "following_url": "https://api.github.com/users/dan-kur/following{/other_user}",
            "gists_url": "https://api.github.com/users/dan-kur/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dan-kur/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dan-kur/subscriptions",
            "organizations_url": "https://api.github.com/users/dan-kur/orgs",
            "repos_url": "https://api.github.com/users/dan-kur/repos",
            "events_url": "https://api.github.com/users/dan-kur/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dan-kur/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2022-08-17T14:42:50Z",
        "updated_at": "2022-08-19T20:48:52Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\nI was trying to train a scikit-learn model in Python, export it to ONNX and then use the model for prediction in a Java environment. The scikit-learn model was converted to ONNX using the `skl2onnx` Python package and loaded using `ai.onnxruntime `in Java.\r\n\r\nHowever, the output was not complete when making predictions in Java. The predicted probabilities did not contain the entire probability array but only the first index of the array. When testing the ONNX model output in Python, the probability array was complete.\r\n\r\nThis issue persisted with different models (SVM, Logistic Regression, MLP) as well as different scikit-learn objects (a single classifier, a classifier as part of a Pipeline).\r\n\r\n**System information**\r\n- macOS 11.6.6\r\n- JDK version: 11.0\r\n- Python version: 3.9.5\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: 1.10.0\r\n- ONNX version: 1.11.0\r\n- skl2onnx version: 1.11.0\r\n\r\n**To Reproduce**\r\n- Export a scikit-learn model to ONNX using skl2onnx in Python\r\n- Load ONNX model in Java using ai.onnxruntime\r\n- Make a prediction \r\n\r\n**Expected behavior**\r\n- The expected output is a list of length 2\r\n- The first index is the predicted class \r\n- The second index should be the array of predicted probabilities for every class in the target variable. However, in Java, only the first index of the probability array is returned  \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12629/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12629/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12630",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12630/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12630/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12630/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12630",
        "id": 1341911969,
        "node_id": "PR_kwDOCVq1mM49VNlP",
        "number": 12630,
        "title": "Add BuildError for --gen_doc and --enable_training",
        "user": {
            "login": "thiagocrepaldi",
            "id": 5469809,
            "node_id": "MDQ6VXNlcjU0Njk4MDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5469809?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thiagocrepaldi",
            "html_url": "https://github.com/thiagocrepaldi",
            "followers_url": "https://api.github.com/users/thiagocrepaldi/followers",
            "following_url": "https://api.github.com/users/thiagocrepaldi/following{/other_user}",
            "gists_url": "https://api.github.com/users/thiagocrepaldi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thiagocrepaldi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thiagocrepaldi/subscriptions",
            "organizations_url": "https://api.github.com/users/thiagocrepaldi/orgs",
            "repos_url": "https://api.github.com/users/thiagocrepaldi/repos",
            "events_url": "https://api.github.com/users/thiagocrepaldi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thiagocrepaldi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1311608287,
                "node_id": "MDU6TGFiZWwxMzExNjA4Mjg3",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/documentation",
                "name": "documentation",
                "color": "1D76DB",
                "default": true,
                "description": "improvements or additions to documentation; typically submitted using template"
            },
            {
                "id": 1913759001,
                "node_id": "MDU6TGFiZWwxOTEzNzU5MDAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/training",
                "name": "training",
                "color": "BFD4F2",
                "default": false,
                "description": "issues related to ONNX Runtime training; typically submitted using template"
            },
            {
                "id": 2159809301,
                "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/build",
                "name": "build",
                "color": "D93F0B",
                "default": false,
                "description": "build issues; typically submitted using template"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-17T15:04:23Z",
        "updated_at": "2022-08-17T18:18:38Z",
        "closed_at": "2022-08-17T18:18:37Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12630",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12630",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12630.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12630.patch",
            "merged_at": "2022-08-17T18:18:37Z"
        },
        "body": "Currently, when building ORT with both --gen_doc --enable_training, the document generation fails with missing `schemadef` definition.\r\n\r\nThat happens because only inference builds add python bindings for it. https://github.com/microsoft/onnxruntime/pull/10980 tried to address sch limitation, but it wasn't accepted\r\n\r\nThis PR adds a build validation error to warn the user about such limitation in the hope of saving debug time for an unsupported scenario",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12630/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12630/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12631",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12631/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12631/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12631/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12631",
        "id": 1341955140,
        "node_id": "PR_kwDOCVq1mM49VXAT",
        "number": 12631,
        "title": "Convert more quantized weights to uint8",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2022-08-17T15:35:56Z",
        "updated_at": "2022-09-05T08:19:16Z",
        "closed_at": null,
        "author_association": "FIRST_TIME_CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12631",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12631",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12631.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12631.patch",
            "merged_at": null
        },
        "body": "**Description**: Optimization after static quantization (QDQ) with `QLinearAdd` was not possible because `QLinearAdd` expects uint8 / uint8 as input types, and the weights from a bias followed by a DequantizeLinear were in int8.\r\n\r\nThese changes should allow to convert the output from DequantizeLinear of bias weights to uint8, allowing to insert a `QLinearAdd`.\r\n\r\nThis should fix https://github.com/microsoft/onnxruntime/issues/12487\r\n\r\nThanks @yufenglee for your help, is my understanding correct? It's the first time I code in C++ so I hope it is not too ugly. Let me know if you would like me to add tests or change something.\r\n\r\n**Motivation and Context**\r\nBefore:\r\n\r\n![image](https://user-images.githubusercontent.com/9808326/185181218-c4f26122-e453-4d91-9363-b5bba7b209ff.png)\r\n\r\nAfter:\r\n\r\n![image](https://user-images.githubusercontent.com/9808326/185181321-991a7a0b-3bdf-40f1-8964-cd13400c44f8.png)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12631/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12631/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12632",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12632/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12632/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12632/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12632",
        "id": 1342051205,
        "node_id": "PR_kwDOCVq1mM49VrZQ",
        "number": 12632,
        "title": "QDQ debugger - Adding Error Calculator",
        "user": {
            "login": "chenfucn",
            "id": 1316708,
            "node_id": "MDQ6VXNlcjEzMTY3MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1316708?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chenfucn",
            "html_url": "https://github.com/chenfucn",
            "followers_url": "https://api.github.com/users/chenfucn/followers",
            "following_url": "https://api.github.com/users/chenfucn/following{/other_user}",
            "gists_url": "https://api.github.com/users/chenfucn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chenfucn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chenfucn/subscriptions",
            "organizations_url": "https://api.github.com/users/chenfucn/orgs",
            "repos_url": "https://api.github.com/users/chenfucn/repos",
            "events_url": "https://api.github.com/users/chenfucn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chenfucn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-17T17:03:29Z",
        "updated_at": "2022-08-18T16:30:44Z",
        "closed_at": "2022-08-18T16:30:43Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12632",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12632",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12632.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12632.patch",
            "merged_at": "2022-08-18T16:30:43Z"
        },
        "body": "**Description**: Describe your changes.\r\n\r\n**Motivation and Context**\r\n- Why is this change required? What problem does it solve?\r\n- If it fixes an open issue, please link to the issue here.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12632/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12632/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12633",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12633/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12633/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12633/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12633",
        "id": 1342172107,
        "node_id": "PR_kwDOCVq1mM49WE9T",
        "number": 12633,
        "title": "CAPI for ort-training checkpoint property",
        "user": {
            "login": "baijumeswani",
            "id": 12852605,
            "node_id": "MDQ6VXNlcjEyODUyNjA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/12852605?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/baijumeswani",
            "html_url": "https://github.com/baijumeswani",
            "followers_url": "https://api.github.com/users/baijumeswani/followers",
            "following_url": "https://api.github.com/users/baijumeswani/following{/other_user}",
            "gists_url": "https://api.github.com/users/baijumeswani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/baijumeswani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/baijumeswani/subscriptions",
            "organizations_url": "https://api.github.com/users/baijumeswani/orgs",
            "repos_url": "https://api.github.com/users/baijumeswani/repos",
            "events_url": "https://api.github.com/users/baijumeswani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/baijumeswani/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 1913759001,
                "node_id": "MDU6TGFiZWwxOTEzNzU5MDAx",
                "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/training",
                "name": "training",
                "color": "BFD4F2",
                "default": false,
                "description": "issues related to ONNX Runtime training; typically submitted using template"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2022-08-17T19:16:49Z",
        "updated_at": "2022-08-23T17:15:22Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12633",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12633",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12633.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12633.patch",
            "merged_at": null
        },
        "body": "This pull request\r\n- Fixes a warning that was being emitted by the compiler: ```warning: no parameter has pointer type```\r\n- Introduces C apis for setting and getting checkpoint properties.\r\n\r\nTo use the C apis to set and get the checkpoint properties:\r\n\r\n```cc\r\n// Int value\r\nint64_t epoch = 987;\r\nORT_RETURN_ON_ERROR(g_ort_training_api->SetCheckpointProperty(\r\n    checkpoint_state, \"epoch\", OrtCheckpointPropertyType::IntProperty, &epoch));\r\n\r\nint64_t saved_int_value;\r\nORT_RETURN_ON_ERROR(g_ort_training_api->GetCheckpointProperty(\r\n    checkpoint_state, \"epoch\", OrtCheckpointPropertyType::IntProperty, &saved_int_value));\r\n\r\n\r\n// Float value\r\nfloat learning_rate = 0.001f;\r\nORT_RETURN_ON_ERROR(g_ort_training_api->SetCheckpointProperty(\r\n    checkpoint_state, \"lr\", OrtCheckpointPropertyType::FloatProperty, &learning_rate));\r\n\r\nfloat saved_float_value;\r\nORT_RETURN_ON_ERROR(g_ort_training_api->GetCheckpointProperty(\r\n    checkpoint_state, \"lr\", OrtCheckpointPropertyType::FloatProperty, &saved_float_value));\r\n\r\n\r\n// String value\r\nchar model_name[] = \"model name\";\r\nORT_RETURN_ON_ERROR(g_ort_training_api->SetCheckpointProperty(\r\n    checkpoint_state, \"model_name\", OrtCheckpointPropertyType::StringProperty, model_name));\r\n\r\nchar* saved_str_value;\r\nORT_RETURN_ON_ERROR(g_ort_training_api->GetCheckpointProperty(\r\n    checkpoint_state, \"model_name\", OrtCheckpointPropertyType::StringProperty, &saved_str_value));\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12633/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12633/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12634",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12634/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12634/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12634/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12634",
        "id": 1342368665,
        "node_id": "PR_kwDOCVq1mM49Wu9z",
        "number": 12634,
        "title": "Refactor profiling logics in sequential executor",
        "user": {
            "login": "RandySheriffH",
            "id": 48490400,
            "node_id": "MDQ6VXNlcjQ4NDkwNDAw",
            "avatar_url": "https://avatars.githubusercontent.com/u/48490400?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/RandySheriffH",
            "html_url": "https://github.com/RandySheriffH",
            "followers_url": "https://api.github.com/users/RandySheriffH/followers",
            "following_url": "https://api.github.com/users/RandySheriffH/following{/other_user}",
            "gists_url": "https://api.github.com/users/RandySheriffH/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/RandySheriffH/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/RandySheriffH/subscriptions",
            "organizations_url": "https://api.github.com/users/RandySheriffH/orgs",
            "repos_url": "https://api.github.com/users/RandySheriffH/repos",
            "events_url": "https://api.github.com/users/RandySheriffH/events{/privacy}",
            "received_events_url": "https://api.github.com/users/RandySheriffH/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-17T23:02:30Z",
        "updated_at": "2022-08-18T00:46:45Z",
        "closed_at": "2022-08-17T23:04:22Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12634",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12634",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12634.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12634.patch",
            "merged_at": null
        },
        "body": "Refactor a few things in sequential executor:\r\n\r\n1. NTVX\r\n2. Concurrency\r\n3. Profiler\r\n4. Tracing\r\n5. Instrumentation\r\n6. ...\r\n\r\nStill in draft for more details.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12634/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12634/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12635",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12635/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12635/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12635/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12635",
        "id": 1342371305,
        "node_id": "PR_kwDOCVq1mM49WvUT",
        "number": 12635,
        "title": "Refactor sequential executor",
        "user": {
            "login": "RandySheriffH",
            "id": 48490400,
            "node_id": "MDQ6VXNlcjQ4NDkwNDAw",
            "avatar_url": "https://avatars.githubusercontent.com/u/48490400?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/RandySheriffH",
            "html_url": "https://github.com/RandySheriffH",
            "followers_url": "https://api.github.com/users/RandySheriffH/followers",
            "following_url": "https://api.github.com/users/RandySheriffH/following{/other_user}",
            "gists_url": "https://api.github.com/users/RandySheriffH/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/RandySheriffH/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/RandySheriffH/subscriptions",
            "organizations_url": "https://api.github.com/users/RandySheriffH/orgs",
            "repos_url": "https://api.github.com/users/RandySheriffH/repos",
            "events_url": "https://api.github.com/users/RandySheriffH/events{/privacy}",
            "received_events_url": "https://api.github.com/users/RandySheriffH/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 10,
        "created_at": "2022-08-17T23:05:42Z",
        "updated_at": "2022-09-13T23:53:14Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": true,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12635",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12635",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12635.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12635.patch",
            "merged_at": null
        },
        "body": "Move profiling logics out of executor to facilitate future reuse.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12635/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12635/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12636",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12636/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12636/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12636/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12636",
        "id": 1342698959,
        "node_id": "I_kwDOCVq1mM5QB_XP",
        "number": 12636,
        "title": "cuda_provider_options.h include non existing file?",
        "user": {
            "login": "mortenwp",
            "id": 54076102,
            "node_id": "MDQ6VXNlcjU0MDc2MTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/54076102?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mortenwp",
            "html_url": "https://github.com/mortenwp",
            "followers_url": "https://api.github.com/users/mortenwp/followers",
            "following_url": "https://api.github.com/users/mortenwp/following{/other_user}",
            "gists_url": "https://api.github.com/users/mortenwp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mortenwp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mortenwp/subscriptions",
            "organizations_url": "https://api.github.com/users/mortenwp/orgs",
            "repos_url": "https://api.github.com/users/mortenwp/repos",
            "events_url": "https://api.github.com/users/mortenwp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mortenwp/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2022-08-18T07:53:01Z",
        "updated_at": "2022-08-25T10:57:12Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "cuda_providers_options.h include \"core/framework/arena_extend_strategy.h\" which I cannot find in that location.\r\nI have checked out v1.12.1, but cannot see file in master either. Am I missing something?\r\n\r\nI assume that the file should contain the definition of the enum onnxruntime::ArenaExtendStrategy.\r\nEarlier I think the arena extend strategy were just an int with value 0 or 1.\r\n\r\nbrgds\r\n-Morten\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/81b128b5e97284f576ed8aaf18404980cb45fe34/include/onnxruntime/core/providers/cuda/cuda_provider_options.h#L7\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/81b128b5e97284f576ed8aaf18404980cb45fe34/include/onnxruntime/core/providers/cuda/cuda_provider_options.h#L24",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12636/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12636/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12637",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12637/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12637/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12637/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12637",
        "id": 1342897389,
        "node_id": "I_kwDOCVq1mM5QCvzt",
        "number": 12637,
        "title": "when the model support dynamic batch the input shape [ -1,-1,80],  how can  warm up?  because of the dynamic batch , I do know the warmup batchsize number ,can it use min_batchsize and max _batchsize to warmup?",
        "user": {
            "login": "liroda",
            "id": 24436690,
            "node_id": "MDQ6VXNlcjI0NDM2Njkw",
            "avatar_url": "https://avatars.githubusercontent.com/u/24436690?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/liroda",
            "html_url": "https://github.com/liroda",
            "followers_url": "https://api.github.com/users/liroda/followers",
            "following_url": "https://api.github.com/users/liroda/following{/other_user}",
            "gists_url": "https://api.github.com/users/liroda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/liroda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/liroda/subscriptions",
            "organizations_url": "https://api.github.com/users/liroda/orgs",
            "repos_url": "https://api.github.com/users/liroda/repos",
            "events_url": "https://api.github.com/users/liroda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/liroda/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2022-08-18T10:27:57Z",
        "updated_at": "2022-08-18T17:28:36Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "when the model support dynamic batch the input shape [ -1,-1,80],  how can  warm up?  because of the dynamic batch , I do know the warmup batchsize number ,can it use min_batchsize and max _batchsize to warmup?\r\n\r\n_Originally posted by @liroda in https://github.com/microsoft/onnxruntime/issues/12120#issuecomment-1218914542_",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12637/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12637/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12638",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12638/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12638/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12638/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12638",
        "id": 1342958013,
        "node_id": "I_kwDOCVq1mM5QC-m9",
        "number": 12638,
        "title": "The quantization model reduces the accuracy compared to the TRT",
        "user": {
            "login": "yeliang2258",
            "id": 30516196,
            "node_id": "MDQ6VXNlcjMwNTE2MTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/30516196?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yeliang2258",
            "html_url": "https://github.com/yeliang2258",
            "followers_url": "https://api.github.com/users/yeliang2258/followers",
            "following_url": "https://api.github.com/users/yeliang2258/following{/other_user}",
            "gists_url": "https://api.github.com/users/yeliang2258/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yeliang2258/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yeliang2258/subscriptions",
            "organizations_url": "https://api.github.com/users/yeliang2258/orgs",
            "repos_url": "https://api.github.com/users/yeliang2258/repos",
            "events_url": "https://api.github.com/users/yeliang2258/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yeliang2258/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2022-08-18T11:22:59Z",
        "updated_at": "2022-09-01T11:04:37Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Describe the bug**\r\n\r\nI have a quantited YOLOV5 model. After extracting the quantize information in the model into a quantization table, the inference accuracy of using TensorRT INT8 is normal, but using ORT to infer this quantitative model, the accuracy drops by about 4%. What may be the reason ? \r\n\r\n**Urgency**\r\nIf there are particular important use cases blocked by this or strict project-related timelines, please share more information and dates. If there are no hard deadlines, please specify none.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- ONNX Runtime installed from (source or binary):binary\r\n- ONNX Runtime version:1.11.1\r\n- Python version:3.7\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12638/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12638/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12639",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12639/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12639/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12639/events",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12639",
        "id": 1343118875,
        "node_id": "I_kwDOCVq1mM5QDl4b",
        "number": 12639,
        "title": "Failed to create TensorrtExecutionProvider using onnxruntime-gpu",
        "user": {
            "login": "stentll",
            "id": 89965725,
            "node_id": "MDQ6VXNlcjg5OTY1NzI1",
            "avatar_url": "https://avatars.githubusercontent.com/u/89965725?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stentll",
            "html_url": "https://github.com/stentll",
            "followers_url": "https://api.github.com/users/stentll/followers",
            "following_url": "https://api.github.com/users/stentll/following{/other_user}",
            "gists_url": "https://api.github.com/users/stentll/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stentll/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stentll/subscriptions",
            "organizations_url": "https://api.github.com/users/stentll/orgs",
            "repos_url": "https://api.github.com/users/stentll/repos",
            "events_url": "https://api.github.com/users/stentll/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stentll/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2022-08-18T13:39:51Z",
        "updated_at": "2022-08-31T18:02:04Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am cannot use `TensorRT` execution provider for `onnxruntime-gpu` inferencing. \r\n\r\n\r\n**Urgency**\r\nI would like to solve this within 3 weeks.\r\n\r\n**System information**\r\n- OS Platform and Distribution: debian 10\r\n- ONNX Runtime installed from: pip \r\n- ONNX Runtime version: onnxruntime-gpu version is 1.11.0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.4.4 / 8.2.4\r\n- GPU model and memory: NVIDIA Quadro RTX 5000 16GB\r\n\r\n**To Reproduce**\r\nI am initializing the `session` like this:\r\n\r\n```\r\nimport onnxruntime as ort\r\n\r\nproviders = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\"]\r\nort_sess = ort.InferenceSession(model_path, providers=providers)\r\n\r\n```\r\nand getting this error:\r\n\r\n>[W:onnxruntime:Default, onnxruntime_pybind_state.cc:509 CreateExecutionProviderInstance] Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.\r\n\r\n\r\n`ort.get_available_providers()` outputs \r\n>['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\r\n\r\nand `ort_sess.get_providers()` outputs\r\n> ['CUDAExecutionProvider', 'CPUExecutionProvider']\r\n\r\n\r\nI understand that the `error message` points to `version mismatch` but I didn't manage to get it right.\r\n\r\nAccording to `onnxruntime TensorRT` compatibility table I must have one of the following combinations (https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html):\r\n\r\n| ONNX Runtime | TensorRT | CUDA |\r\n|--------------|----------|------|\r\n| master       | 8.4      | 11.4 |\r\n| 1.12         | 8.4      | 11.4 |\r\n| 1.11         | 8.2      | 11.4 |\r\n| 1.10         | 8.0      | 11.4 |\r\n| 1.9          | 8.0      | 11.4 |\r\n\r\n\r\nAnd according to `CUDA` requirements table (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements) I must have one of these combinations:\r\n\r\n| ONNX Runtime | CUDA |               cuDNN               |                                                          Notes                                                         |   |\r\n|:------------:|:----:|:---------------------------------:|:----------------------------------------------------------------------------------------------------------------------:|---|\r\n| 1.12, 1.11   | 11.4 | 8.2.4 (Linux), 8.2.2.26 (Windows) | libcudart 11.4.43, libcufft 10.5.2.100, libcurand 10.2.5.120, libcublasLt 11.6.5.2, libcublas 11.6.5.2, libcudnn 8.2.4 |   |\r\n\r\n\r\nI have installed all `NVIDIA-related packages` via `tar` installation method from `Nvidia` docs.\r\nHere are my versions:\r\n\r\n\r\ncuda:   \r\n`cat /usr/local/cuda/version.json`:\r\n```\r\n   \"cuda\" : {\r\n      \"name\" : \"CUDA SDK\",\r\n      \"version\" : \"11.4.4\"\r\n   }\r\n```\r\n\r\ncudnn:   \r\n`cat /usr/local/cuda/version.json`:\r\n```\r\n#define CUDNN_MAJOR 8\r\n#define CUDNN_MINOR 2\r\n#define CUDNN_PATCHLEVEL 4\r\n```\r\n\r\nTensorRT:   \r\n`pip list | grep tensorrt`   \r\n```\r\ntensorrt                          8.2.3.0\r\n```\r\n\r\nMy onnxruntime-gpu version is **1.11.0** because of the compatibility table above.\r\n\r\nI am using TensorRT version 8.2.3.0 because (https://onnxruntime.ai/docs/build/eps.html#tensorrt) \r\n> The TensorRT execution provider for ONNX Runtime is built and tested with TensorRT 8.2.3.0. \r\n\r\nThere is, however, another page that states the following (https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html):\r\n> The TensorRT execution provider for ONNX Runtime is built and tested with TensorRT 8.4.\r\n\r\nSo I also tried another combo with TensorRT version TensorRT-8.4.1.5, onnxruntime-gpu==1.12.0, cuda-11.4, cudnn-8.2.4 but got the same error.\r\n\r\nI'm using Debian 10.\r\n\r\n**Expected behavior**\r\nI would expect onnxruntime to allow me to infer with TensorRT execution provider and not give any error messages and not falling back to CUDA EP.\r\n\r\n\r\nWhat am I doing wrong? Has anyone nailed the version matching and could share the version combination? If so, did you also manage to install tensorrt via `nvidia pypi` index (this would make things more comfortable for me)?\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12639/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12639/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12640",
        "repository_url": "https://api.github.com/repos/microsoft/onnxruntime",
        "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12640/labels{/name}",
        "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12640/comments",
        "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12640/events",
        "html_url": "https://github.com/microsoft/onnxruntime/pull/12640",
        "id": 1343416867,
        "node_id": "PR_kwDOCVq1mM49aLal",
        "number": 12640,
        "title": "Dynamic QDQ Quantization",
        "user": {
            "login": "acid-space-cowboy",
            "id": 54558678,
            "node_id": "MDQ6VXNlcjU0NTU4Njc4",
            "avatar_url": "https://avatars.githubusercontent.com/u/54558678?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/acid-space-cowboy",
            "html_url": "https://github.com/acid-space-cowboy",
            "followers_url": "https://api.github.com/users/acid-space-cowboy/followers",
            "following_url": "https://api.github.com/users/acid-space-cowboy/following{/other_user}",
            "gists_url": "https://api.github.com/users/acid-space-cowboy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/acid-space-cowboy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/acid-space-cowboy/subscriptions",
            "organizations_url": "https://api.github.com/users/acid-space-cowboy/orgs",
            "repos_url": "https://api.github.com/users/acid-space-cowboy/repos",
            "events_url": "https://api.github.com/users/acid-space-cowboy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/acid-space-cowboy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 16,
        "created_at": "2022-08-18T17:46:56Z",
        "updated_at": "2022-08-23T16:30:06Z",
        "closed_at": "2022-08-23T16:30:05Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/pulls/12640",
            "html_url": "https://github.com/microsoft/onnxruntime/pull/12640",
            "diff_url": "https://github.com/microsoft/onnxruntime/pull/12640.diff",
            "patch_url": "https://github.com/microsoft/onnxruntime/pull/12640.patch",
            "merged_at": "2022-08-23T16:30:05Z"
        },
        "body": "**Description**: Added Dynamic QDQ Quantization feature to qdq_quantizer.py\r\n\r\n**Motivation and Context**\r\n- Allows users to make use of dynamic QDQ Quantization\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12640/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12640/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    }
]