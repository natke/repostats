[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480630204",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1480630204",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1480630204,
        "node_id": "IC_kwDOCVq1mM5YQJ-8",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T05:37:53Z",
        "updated_at": "2023-03-23T05:37:53Z",
        "author_association": "MEMBER",
        "body": "The time for creating an InferenceSession is meaningless as it is the time to load and optimize the model and prepare it for execution.\r\n\r\nWhat matters is the performance of running the model, and you should be doing a warmup call to `run` before measuring if you want accurate results.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480630204/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480709237",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1480709237",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1480709237,
        "node_id": "IC_kwDOCVq1mM5YQdR1",
        "user": {
            "login": "ysq151944",
            "id": 59116110,
            "node_id": "MDQ6VXNlcjU5MTE2MTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/59116110?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ysq151944",
            "html_url": "https://github.com/ysq151944",
            "followers_url": "https://api.github.com/users/ysq151944/followers",
            "following_url": "https://api.github.com/users/ysq151944/following{/other_user}",
            "gists_url": "https://api.github.com/users/ysq151944/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ysq151944/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ysq151944/subscriptions",
            "organizations_url": "https://api.github.com/users/ysq151944/orgs",
            "repos_url": "https://api.github.com/users/ysq151944/repos",
            "events_url": "https://api.github.com/users/ysq151944/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ysq151944/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T07:21:40Z",
        "updated_at": "2023-03-23T07:21:40Z",
        "author_association": "NONE",
        "body": "> The time for creating an InferenceSession is meaningless as it is the time to load and optimize the model and prepare it for execution.\r\n> \r\n> What matters is the performance of running the model, and you should be doing a warmup call to `run` before measuring if you want accurate results.\r\n\r\nThanks for your reply, yet I think the loading time matters in my situation:\r\n\r\n1. the model predicts a sample every 3 minutes\r\n2. the model should be updated every day\r\n3. Inter-Process Communication is last thing I would try\r\n\r\n\r\nMaybe what dosen't matter is the compiling time, not the loading time?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480709237/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480716191",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1480716191",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1480716191,
        "node_id": "IC_kwDOCVq1mM5YQe-f",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T07:30:20Z",
        "updated_at": "2023-03-23T07:30:20Z",
        "author_association": "MEMBER",
        "body": "The expected usage is to create an InferenceSession and re-use it when you want to run the model to generate an inference, as loading the model is an expensive operation. \r\n\r\ni.e. create the inference session once and every 3 minutes when you need to make a prediction call `InferenceSession.run`.\r\n\r\nIf you need to update the model, create a new InferenceSession with the updated model, as the model is immutable once the InferenceSession has been initialized. \r\n\r\nMaybe take a look at some of the examples.\r\n\r\nhttps://onnxruntime.ai/docs/get-started/with-python.html#scikit-learn-cv\r\nhttps://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/inference_demos/simple_onnxruntime_inference.ipynb\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480716191/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480747674",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1480747674",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1480747674,
        "node_id": "IC_kwDOCVq1mM5YQmqa",
        "user": {
            "login": "ysq151944",
            "id": 59116110,
            "node_id": "MDQ6VXNlcjU5MTE2MTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/59116110?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ysq151944",
            "html_url": "https://github.com/ysq151944",
            "followers_url": "https://api.github.com/users/ysq151944/followers",
            "following_url": "https://api.github.com/users/ysq151944/following{/other_user}",
            "gists_url": "https://api.github.com/users/ysq151944/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ysq151944/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ysq151944/subscriptions",
            "organizations_url": "https://api.github.com/users/ysq151944/orgs",
            "repos_url": "https://api.github.com/users/ysq151944/repos",
            "events_url": "https://api.github.com/users/ysq151944/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ysq151944/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T08:03:41Z",
        "updated_at": "2023-03-23T08:03:41Z",
        "author_association": "NONE",
        "body": "> The expected usage is to create an InferenceSession and re-use it when you want to run the model to generate an inference, as loading the model is an expensive operation.\r\n> \r\n> i.e. create the inference session once and every 3 minutes when you need to make a prediction call `InferenceSession.run`.\r\n> \r\n> If you need to update the model, create a new InferenceSession with the updated model, as the model is immutable once the InferenceSession has been initialized.\r\n> \r\n> Maybe take a look at some of the examples.\r\n> \r\n> https://onnxruntime.ai/docs/get-started/with-python.html#scikit-learn-cv https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/inference_demos/simple_onnxruntime_inference.ipynb\r\n\r\nThanks, I've read the doc, and I'am not worry about the inferencing duration, onnxruntime performs perfect in that part in my case(from 100ms to 16us on Ubuntu, 66us on Windows).\r\n\r\nHowever, I can't \"create a new InferenceSession with the updated model \" unless stopping the active process or claiming new compute resource(which I would not like to do).\r\n\r\nAs you said once, \"When ORT loads the model we run all our optimizers, convert to ORT internal data types, potentially do pre-packing of weights, create the allocation plan, etc.\".\r\n\r\nSo, would it possible that you split the onnxruntime.InferenceSession into separate parts:\r\n1. do all your optimization\r\n2. let user saves the optimized model on disk\r\n3. load optimized model directly\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480747674/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480877680",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1480877680",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1480877680,
        "node_id": "IC_kwDOCVq1mM5YRGZw",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T09:41:08Z",
        "updated_at": "2023-03-23T09:41:08Z",
        "author_association": "MEMBER",
        "body": "https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.SessionOptions.optimized_model_filepath saves the model after optimization. Optimization is just one part of loading though so whilst pre-optimization will help, creating the InferenceSession is still a heavy operation.\r\n\r\nSomething looks massively off with your linux numbers though. The same C++ code is used during model loading on Linux and Windows, so the time should be pretty much equivalent if you take into account hardware difference like size of memory and processor cores/speed.\r\n\r\nSide note: intra/inter op thread counts are mainly relevant to InferenceSession::run and have nearly nothing to do with model loading.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480877680/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1481148688",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1481148688",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1481148688,
        "node_id": "IC_kwDOCVq1mM5YSIkQ",
        "user": {
            "login": "ysq151944",
            "id": 59116110,
            "node_id": "MDQ6VXNlcjU5MTE2MTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/59116110?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ysq151944",
            "html_url": "https://github.com/ysq151944",
            "followers_url": "https://api.github.com/users/ysq151944/followers",
            "following_url": "https://api.github.com/users/ysq151944/following{/other_user}",
            "gists_url": "https://api.github.com/users/ysq151944/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ysq151944/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ysq151944/subscriptions",
            "organizations_url": "https://api.github.com/users/ysq151944/orgs",
            "repos_url": "https://api.github.com/users/ysq151944/repos",
            "events_url": "https://api.github.com/users/ysq151944/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ysq151944/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T12:57:12Z",
        "updated_at": "2023-03-23T12:57:12Z",
        "author_association": "NONE",
        "body": "> https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.SessionOptions.optimized_model_filepath saves the model after optimization. Optimization is just one part of loading though so whilst pre-optimization will help, creating the InferenceSession is still a heavy operation.\r\n> \r\n> Something looks massively off with your linux numbers though. The same C++ code is used during model loading on Linux and Windows, so the time should be pretty much equivalent if you take into account hardware difference like size of memory and processor cores/speed.\r\n> \r\n> Side note: intra/inter op thread counts are mainly relevant to InferenceSession::run and have nearly nothing to do with model loading.\r\n\r\nIndeed loading optimized model helps a little(277s -> 264s). \r\n\r\nBeside, I checked another machine with Ubuntu18.04, model loading is still quite slow.\r\n\r\nCould you download my model and reproduce the big loading time diff between windows and ubuntu? I just want to check whether it's my problem, thanks in advance.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1481148688/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1481288669",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1481288669",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1481288669,
        "node_id": "IC_kwDOCVq1mM5YSqvd",
        "user": {
            "login": "xadupre",
            "id": 22452781,
            "node_id": "MDQ6VXNlcjIyNDUyNzgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/22452781?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xadupre",
            "html_url": "https://github.com/xadupre",
            "followers_url": "https://api.github.com/users/xadupre/followers",
            "following_url": "https://api.github.com/users/xadupre/following{/other_user}",
            "gists_url": "https://api.github.com/users/xadupre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xadupre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xadupre/subscriptions",
            "organizations_url": "https://api.github.com/users/xadupre/orgs",
            "repos_url": "https://api.github.com/users/xadupre/repos",
            "events_url": "https://api.github.com/users/xadupre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xadupre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T14:21:35Z",
        "updated_at": "2023-03-23T14:21:35Z",
        "author_association": "MEMBER",
        "body": "The model contains one TreeEnsembleClassifier (you could remove the ZipMap operator: https://onnx.ai/sklearn-onnx/parameterized.html#zipmap). The optimizrs do not change the model. You can disable them. However, onnxruntime creates the tree structure every time the session is loaded. The cost should be similar to the one used to deserialize the model. This one cannot disappear and your model is 200 Mb. I don't know if protobuf behaves differently on Windows and Ubuntu. onnxruntime cannot be pickled so the loaded time cannot be avoided with onnxruntime. But it could probably with a python runtime. loading time + inference time would probably be less then 9s with a picklable python runtime you can unpickle at every call.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1481288669/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1482149450",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1482149450",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1482149450,
        "node_id": "IC_kwDOCVq1mM5YV85K",
        "user": {
            "login": "ysq151944",
            "id": 59116110,
            "node_id": "MDQ6VXNlcjU5MTE2MTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/59116110?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ysq151944",
            "html_url": "https://github.com/ysq151944",
            "followers_url": "https://api.github.com/users/ysq151944/followers",
            "following_url": "https://api.github.com/users/ysq151944/following{/other_user}",
            "gists_url": "https://api.github.com/users/ysq151944/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ysq151944/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ysq151944/subscriptions",
            "organizations_url": "https://api.github.com/users/ysq151944/orgs",
            "repos_url": "https://api.github.com/users/ysq151944/repos",
            "events_url": "https://api.github.com/users/ysq151944/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ysq151944/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-24T02:04:58Z",
        "updated_at": "2023-03-24T02:04:58Z",
        "author_association": "NONE",
        "body": "> The model contains one TreeEnsembleClassifier (you could remove the ZipMap operator: https://onnx.ai/sklearn-onnx/parameterized.html#zipmap). The optimizrs do not change the model. You can disable them. However, onnxruntime creates the tree structure every time the session is loaded. The cost should be similar to the one used to deserialize the model. This one cannot disappear and your model is 200 Mb. I don't know if protobuf behaves differently on Windows and Ubuntu. onnxruntime cannot be pickled so the loaded time cannot be avoided with onnxruntime. But it could probably with a python runtime. loading time + inference time would probably be less then 9s with a picklable python runtime you can unpickle at every call.\r\n\r\nThank you! The python runtime workes!\r\n\r\nI not sure whether \"pyhton runtime\" means mlprodict.onnxrt.OnnxInference('*.onnx', runtime='python_compiled')? Anyway it works for me:\r\n\r\n1. the loading time drops to 9.06(quicker than 11.13s on windows)\r\n2. inference time now 2 times to onnxruntime on Ubuntu(33us / 16us)\r\n3. on windows the inference time now 0.83 times to onnxruntime(55us / 66us)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1482149450/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1482345776",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1482345776",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1482345776,
        "node_id": "IC_kwDOCVq1mM5YWs0w",
        "user": {
            "login": "xadupre",
            "id": 22452781,
            "node_id": "MDQ6VXNlcjIyNDUyNzgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/22452781?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xadupre",
            "html_url": "https://github.com/xadupre",
            "followers_url": "https://api.github.com/users/xadupre/followers",
            "following_url": "https://api.github.com/users/xadupre/following{/other_user}",
            "gists_url": "https://api.github.com/users/xadupre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xadupre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xadupre/subscriptions",
            "organizations_url": "https://api.github.com/users/xadupre/orgs",
            "repos_url": "https://api.github.com/users/xadupre/repos",
            "events_url": "https://api.github.com/users/xadupre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xadupre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-24T07:03:47Z",
        "updated_at": "2023-03-24T07:03:47Z",
        "author_association": "MEMBER",
        "body": "That's an option. Most of this python runtime moved to onnx.reference.ReferenceEvaluator except the c++ implementations. For the time being, in your case, OnnxInference is probably the best option.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1482345776/reactions",
            "total_count": 2,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 1,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1482372302",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15165#issuecomment-1482372302",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15165",
        "id": 1482372302,
        "node_id": "IC_kwDOCVq1mM5YWzTO",
        "user": {
            "login": "ysq151944",
            "id": 59116110,
            "node_id": "MDQ6VXNlcjU5MTE2MTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/59116110?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ysq151944",
            "html_url": "https://github.com/ysq151944",
            "followers_url": "https://api.github.com/users/ysq151944/followers",
            "following_url": "https://api.github.com/users/ysq151944/following{/other_user}",
            "gists_url": "https://api.github.com/users/ysq151944/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ysq151944/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ysq151944/subscriptions",
            "organizations_url": "https://api.github.com/users/ysq151944/orgs",
            "repos_url": "https://api.github.com/users/ysq151944/repos",
            "events_url": "https://api.github.com/users/ysq151944/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ysq151944/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-24T07:31:15Z",
        "updated_at": "2023-03-24T07:31:15Z",
        "author_association": "NONE",
        "body": "Yes, surely OnnxInference meets all my needs. \r\n\r\nThanks again! I'll close this issue.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1482372302/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]