[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002258322",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1002258322",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1002258322,
        "node_id": "IC_kwDOCVq1mM47vT-S",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-28T19:51:57Z",
        "updated_at": "2021-12-28T19:55:32Z",
        "author_association": "MEMBER",
        "body": "The ability to take advantage of GPU depends on two factors 1) the way ONNX Runtime is used (is GPU provider enabled) via ONNX Runtime API 2) whether the resulting model contains references to kernels that have GPU implementation.\r\n\r\nThe above code is a very high level of another Microsoft ML framework that invokes ONNX Runtime somewhere deep underneath. It may be an ML model that may only refer to non-standard nodes that have no GPU implementation. You can visualize the model using https://netron.app/. \r\n\r\nIf the model is a mix of GPU supported and unsupported nodes, inference would incur a cost of transferring data back and forth rather than staying on GPU for the whole inference time.\r\n\r\nStanard ONNX ops are documented [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md).\r\n\r\nSupported ML extensions to facilitate MS ML execution are documented [here](https://github.com/onnx/onnx/blob/master/docs/Operators-ml.md). I think most of them, if not all, are CPU only.\r\n\r\nOfficial docs [here ](https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/object-detection-onnx)refer to ONNX models we know are fully supported by ONNX Runtime.\r\n\r\nFiling an issue [here](https://github.com/microsoft/AcademicContent/blob/main/Labs/AI%20and%20Machine%20Learning/Custom%20Vision%20Service/Custom%20Vision%20Service.md) might help.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002258322/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002304713",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1002304713",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1002304713,
        "node_id": "IC_kwDOCVq1mM47vfTJ",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-28T22:22:08Z",
        "updated_at": "2021-12-28T22:22:08Z",
        "author_association": "MEMBER",
        "body": "If it's an image processing model it's highly likely all operators are supported on GPU, however the GPU execution provider (EP) is not registered by default - you have to explicitly do that and I don't know what ML.Net does under the covers. \r\n\r\nI believe there's also a requirement to have CUDA and cuDNN installed otherwise the CUDA EP won't load. There's some older ML.Net documentation about this linked in [this](https://stackoverflow.com/questions/64426308/using-ml-net-with-an-onnx-model-and-gpu) StackOverflow question. The CUDA/cuDNN versions ORT requires is listed [here](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html).\r\n\r\nYou could alternatively call ORT directly but would need to add code to do the pre/post processing. As you just seem to have an image resize on the pre-processing [this example code](https://devblogs.microsoft.com/xamarin/machine-learning-in-xamarin-forms-with-onnx-runtime/#preprocessing) may help. There are other libraries that can be used for the resize depending on your requirements - see [here](https://devblogs.microsoft.com/dotnet/net-core-image-processing/) for a comparison of the most popular ones.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002304713/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1003756385",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1003756385",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1003756385,
        "node_id": "IC_kwDOCVq1mM471Bth",
        "user": {
            "login": "noumanqaiser",
            "id": 5542052,
            "node_id": "MDQ6VXNlcjU1NDIwNTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5542052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/noumanqaiser",
            "html_url": "https://github.com/noumanqaiser",
            "followers_url": "https://api.github.com/users/noumanqaiser/followers",
            "following_url": "https://api.github.com/users/noumanqaiser/following{/other_user}",
            "gists_url": "https://api.github.com/users/noumanqaiser/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/noumanqaiser/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/noumanqaiser/subscriptions",
            "organizations_url": "https://api.github.com/users/noumanqaiser/orgs",
            "repos_url": "https://api.github.com/users/noumanqaiser/repos",
            "events_url": "https://api.github.com/users/noumanqaiser/events{/privacy}",
            "received_events_url": "https://api.github.com/users/noumanqaiser/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-02T18:28:37Z",
        "updated_at": "2022-01-02T18:28:37Z",
        "author_association": "NONE",
        "body": "The onnx model can be accessed at the link below, \r\n\r\nhttps://1drv.ms/u/s!AoCxHIRfqNffinv_ECpvMGDY4jbl?e=aLBuMm\r\n\r\nMy understanding is, the instructions/Opset should not be an issue, but I have very limited understanding of opsets and I would appreciate if you could take a look of the onnx file and confirm.\r\n\r\nReferring to @skottmckay , I have tried with various combinations of CUDA and CudNN, I couldn't find any specific document explaining what Cuda/Cudnn version combination would allow GPU inferencing with ML.NET, but to use ML.NET for model training requires Cuda 10.1 with CudNN 7.6.4 as explained here( https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/install-gpu-model-builder), However there is no mention if impacts ML.NET's inference or only training.\r\n\r\nOn the OnnxRuntime website. A table of compatible Cuda & CudNN versions is given, (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), based on this Cuda 11.4 with CudNN 8.2.2.26 should help exploit GPU when consumed with Microsoft.ML.OnnxRuntime.Gpu Nuget package. \r\n\r\nI have tried out both these combinations and in none of the cases, the inference performance shows any improvement. \r\n\r\nThis leaves me with the final option of utilizing external libraries for preprocessing and calling ORT directly, which sort of takes ML.NET entirely out of the equation. In Mike's article, I see that he hasnt tried it out with OnnxRuntime Cuda or DirectML Execution providers, but this may be worth trying out.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1003756385/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004292803",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1004292803",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1004292803,
        "node_id": "IC_kwDOCVq1mM473ErD",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-03T19:02:56Z",
        "updated_at": "2022-01-03T19:02:56Z",
        "author_association": "MEMBER",
        "body": "I have contacted @michaelsharp from ML.NET",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004292803/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004558789",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1004558789",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1004558789,
        "node_id": "IC_kwDOCVq1mM474FnF",
        "user": {
            "login": "noumanqaiser",
            "id": 5542052,
            "node_id": "MDQ6VXNlcjU1NDIwNTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5542052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/noumanqaiser",
            "html_url": "https://github.com/noumanqaiser",
            "followers_url": "https://api.github.com/users/noumanqaiser/followers",
            "following_url": "https://api.github.com/users/noumanqaiser/following{/other_user}",
            "gists_url": "https://api.github.com/users/noumanqaiser/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/noumanqaiser/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/noumanqaiser/subscriptions",
            "organizations_url": "https://api.github.com/users/noumanqaiser/orgs",
            "repos_url": "https://api.github.com/users/noumanqaiser/repos",
            "events_url": "https://api.github.com/users/noumanqaiser/events{/privacy}",
            "received_events_url": "https://api.github.com/users/noumanqaiser/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-04T06:30:48Z",
        "updated_at": "2022-01-04T06:30:48Z",
        "author_association": "NONE",
        "body": "@michaelgsharp @skottmckay @yuslepukhin \r\n\r\nAs scott suggested, I created another class in the project which consumes ONNX Runtime directly and image pre-processing(resizing and Tensor loading) is managed externally.  Unfortunately, I see no performance gains. \r\n\r\nFor this benchmarking, I am using the following environment:\r\nWindows 10 Home 21H1, Dell Inspiron 5406, Core i7 1165G7, 16GB RAM with Nvidia MX330 2GB GPU\r\nProgram is written in C#, .NET 5, Console App\r\nVisual Studio 2022 v17.0.4\r\nCUDA/CudNN version: CUDA Tooklit 11.4.3 , CudNN 8.2.2.26 for Cuda 11.4\r\nGPU model and memory: Nvidia MX330 with 2GB Memory\r\n\r\nType of Model: Image Classification\r\nModel trained on: Microsoft CustomVision and exported as Onnx Model, Model Details are below:\r\n![image](https://user-images.githubusercontent.com/5542052/148014229-2395e87a-471a-4e9a-a65d-8fc6b5359e4a.png)\r\n\r\nIn all experiments, I deleted the bin folder entirely and rebuilt solution to ensure no mixup of dlls. Same image data set of around 100 images was used for comparison.\r\n\r\nBenchmark 1: \r\nImage classification, using ML.NET and Onnxruntime 1.10 and Microsoft.ML version 1.7\r\n\r\nCode shared Earlier in the first post: \r\nAverage Analysis time: 71.6ms\r\nAverage Processor % during inferencing: ~50%\r\nAverage Memory Usage: ~300MB\r\n![image](https://user-images.githubusercontent.com/5542052/148014671-da270528-1840-4886-98c6-573cbdd09152.png)\r\n\r\nBenchmark 2:\r\nImage classification,  Onnxruntime 1.10 and resizing/tensor loading managed externally, \r\n\r\n\r\nHere is the code I used for Inferencing:\r\n`\r\nDictionary<int, string> ModelLabels = new Dictionary<int, string>();\r\n        List<double> inferenceTimes = new List<double>();\r\n        List<double> ResizingTimes = new List<double>();\r\n        List<double> ImagetoTensorConversionTimes = new List<double>();\r\n        public void GetMasspredictions(string samplesfolder, string modelFolderPath)\r\n        {\r\n            inferenceTimes.Clear();\r\n            ResizingTimes.Clear();\r\n            ImagetoTensorConversionTimes.Clear();\r\n\r\n            string[] inputfiles = Directory.GetFiles(samplesfolder);\r\n            string modelPath = modelFolderPath + @\"model.onnx\";\r\n            ModelLabels.Clear();\r\n            string[] labels = File.ReadAllText(modelFolderPath + @\"labels.txt\").Split('\\n');\r\n\r\n            int i = 0;\r\n            foreach (var label in labels)\r\n            {\r\n                ModelLabels.Add(i, label);\r\n                i++;\r\n            }\r\n\r\n            \r\n             i = 0;\r\n\r\n\r\n            using (var session = new InferenceSession(modelPath))//, SessionOptions.MakeSessionOptionWithCudaProvider()))\r\n            {\r\n                foreach (var fl in inputfiles)\r\n                {\r\n\r\n                    Bitmap bitmap = new Bitmap(fl);\r\n                    Stopwatch sw = new Stopwatch();\r\n\r\n\r\n                    var inputs = GetModelInput(bitmap);\r\n                    sw.Start();\r\n\r\n                    // Run the inference\r\n                    using (var results = session.Run(inputs))\r\n                    {\r\n                        // Get the results\r\n                        foreach (var r in results)\r\n                        {\r\n\r\n                            int prediction = MaxProbability(r.AsTensor<float>());\r\n                            Console.WriteLine(\"Prediction: \" + ModelLabels[prediction].ToString());\r\n\r\n\r\n\r\n                        }\r\n                    }\r\n\r\n\r\n                    sw.Stop();\r\n\r\n\r\n                    Console.WriteLine($\"Inference Time(ms): {sw.ElapsedMilliseconds}\");\r\n                    i++;\r\n                    if (i > 2)  //avoiding the initial samples form stats as they take up much longer\r\n                        inferenceTimes.Add(sw.ElapsedMilliseconds);\r\n                }\r\n\r\n            }\r\n\r\n\r\n            if (inferenceTimes.Count() > 0)\r\n            {\r\n                Console.WriteLine($\"Average Inference Time(ms): {inferenceTimes.Average()}\");\r\n                Console.WriteLine($\"Average Resizing Time(ms): {ResizingTimes.Average()}\");\r\n                Console.WriteLine($\"Average TensorLoading Time(ms): {ImagetoTensorConversionTimes.Average()}\");\r\n                Console.WriteLine($\"Average Total Time(ms): {ImagetoTensorConversionTimes.Average() + ResizingTimes.Average()+ inferenceTimes.Average() }\");\r\n            }\r\n        }\r\n\r\n        public List<NamedOnnxValue> GetModelInput(Bitmap FullImage)\r\n        {\r\n            Stopwatch sw = new Stopwatch();\r\n            sw.Start();\r\n            var inputImage = ResizeBitmap(FullImage, 300, 300);\r\n            sw.Stop();\r\n            ResizingTimes.Add(sw.ElapsedMilliseconds);\r\n            \r\n            sw.Reset();\r\n\r\n            sw.Start();\r\n\r\n            //Image to tensor conversion OPTION 1\r\n            //an unsafe method to convert imamge to float, saves around 50ms compared to the one below.\r\n            Tensor<float> input =  ConvertImageToFloatTensorUnsafe(inputImage);\r\n            \r\n            \r\n            //Image to Tensor Converstion OPTION 2\r\n            //for a 300x300, can take around 80ms.\r\n            /*\r\n            Tensor<float> input = new DenseTensor<float>(new[] { 1, 3, 300, 300 });\r\n            var mean = new float[] { 0, 0, 0 };\r\n            for (int y = 0; y < inputImage.Height; y++)\r\n            {\r\n               \r\n                for (int x = 0; x < inputImage.Width; x++)\r\n                {\r\n                    var pixel = inputImage.GetPixel(x,y);\r\n                    input[0, 0, y, x] = (pixel.R - mean[0]);\r\n                    input[0, 1, y, x] = (pixel.G - mean[1]);\r\n                    input[0, 2, y, x] = (pixel.B - mean[2]);\r\n                }\r\n            }\r\n            */\r\n            // Setup inputs and outputs\r\n            var inputs = new List<NamedOnnxValue>\r\n            {\r\n                NamedOnnxValue.CreateFromTensor<float>(\"data\",input)\r\n            };\r\n            sw.Stop();\r\n            ImagetoTensorConversionTimes.Add(sw.ElapsedMilliseconds);\r\n\r\n            return inputs;\r\n\r\n        }\r\n\r\n        public Tensor<float> ConvertImageToFloatTensorUnsafe(Bitmap image)\r\n        {\r\n            // Create the Tensor with the appropiate dimensions  for the NN\r\n            Tensor<float> data = new DenseTensor<float>(new[] { 1,3, image.Width, image.Height });\r\n\r\n            BitmapData bmd = image.LockBits(new System.Drawing.Rectangle(0, 0, image.Width, image.Height), System.Drawing.Imaging.ImageLockMode.ReadOnly, image.PixelFormat);\r\n            int PixelSize = 3;\r\n\r\n            unsafe\r\n            {\r\n                for (int y = 0; y < bmd.Height; y++)\r\n                {\r\n                    // row is a pointer to a full row of data with each of its colors\r\n                    byte* row = (byte*)bmd.Scan0 + (y * bmd.Stride);\r\n                    for (int x = 0; x < bmd.Width; x++)\r\n                    {\r\n                        // note the order of colors is BGR\r\n                        data[0, 0,y, x] = row[x * PixelSize + 0];// / (float)255.0;\r\n                        data[0,1, y, x] = row[x * PixelSize + 1];// / (float)255.0;\r\n                        data[0,2, y, x] = row[x * PixelSize + 2];// / (float)255.0;\r\n                    }\r\n                }\r\n\r\n                image.UnlockBits(bmd);\r\n            }\r\n            return data;\r\n        }\r\n\r\n\r\n      \r\n\r\n        public Bitmap ResizeBitmap(Bitmap bmp, int width, int height)\r\n        {\r\n            Bitmap result = new Bitmap(width, height);\r\n            using (Graphics g = Graphics.FromImage(result))\r\n            {\r\n                g.DrawImage(bmp, 0, 0, width, height);\r\n            }\r\n\r\n            return result;\r\n        }\r\n\r\n        static int MaxProbability(Tensor<float> probabilities)\r\n        {\r\n            float max = -9999.9F;\r\n            int maxIndex = -1;\r\n            for (int i = 0; i < probabilities.Length; ++i)\r\n            {\r\n                float prob = probabilities.GetValue(i);\r\n                if (prob > max)\r\n                {\r\n                    max = prob;\r\n                    maxIndex = i;\r\n                }\r\n            }\r\n            return maxIndex;\r\n\r\n        }\r\n\r\n`\r\n\r\nResults:\r\n\r\nAverage Inference Time(ms): 59.6\r\nAverage Resizing Time(ms): 10.629032258064516\r\nAverage TensorLoading Time(ms): 55.70967741935484\r\nAverage Total Time(ms): 125.93870967741935\r\n\r\nAverage Memory Usage: 173 MB\r\nAverage Processor % = ~50%\r\n![image](https://user-images.githubusercontent.com/5542052/148015065-427dde5a-2e7e-4bc3-a707-d4a4fae42363.png)\r\n\r\nBenchmark 3:\r\nImage classification,  Onnxruntime.GPU 1.10 and resizing/tensor loading managed externally, \r\n\r\nJust one change, when creating the session, I use the following line instead:\r\n` using (var session = new InferenceSession(modelPath ,SessionOptions.MakeSessionOptionWithCudaProvider()))`\r\n\r\nAverage Inference Time(ms): 37.483333333333334\r\nAverage Resizing Time(ms): 6.225806451612903\r\nAverage TensorLoading Time(ms): 29.870967741935484\r\nAverage Total Time(ms): 73.58010752688172\r\n\r\nAverage Memory Usage: 3.2 GB !!!\r\nAverage Processor Usage: 13%\r\n![image](https://user-images.githubusercontent.com/5542052/148015381-a9d1e33e-5e48-4530-99bc-2dd60527bbd7.png)\r\n\r\nI did another benchmark with Onnxruntime.GPU but with the session being created without GPU:\r\n`using (var session = new InferenceSession(modelPath))`\r\n\r\nIn this case, the results are almost same as benchmark 2, Hence I believe the GPU doesnt even come into action.\r\n\r\nBenchmark 4:\r\nImage classification,  Onnxruntime.GPU 1.10 with ML.NET used for transformation \r\n\r\nPipeline created using this code:\r\n` \r\n ```\r\nvar pipeline = mlContext.Transforms\r\n                                .ResizeImages(\"image\", modelprops.CustomVisionPreprocessTargetWidth, modelprops.CustomVisionPreprocessTargetHeight, nameof(ImageInputData.Image), ImageResizingEstimator.ResizingKind.Fill)\r\n                                .Append(mlContext.Transforms.ExtractPixels(\"data\", \"image\"))\r\n                                .Append(mlContext.Transforms.ApplyOnnxModel(\"model_output\", \"data\", modelFolderPath + @\"model.onnx\"));\r\n```\r\n`\r\nAverage analysis time = 70.8 ms\r\nProcessor usage: ~50%\r\nMemory: 287 MB.\r\n\r\nBenchmark 5:\r\nImage classification,  Onnxruntime.GPU 1.10 with ML.NET used for transformation, clearly proving device ID when creating pipeline\r\n\r\nPipeline created using this code:\r\n` var pipeline = mlContext.Transforms\r\n                                .ResizeImages(\"image\", modelprops.CustomVisionPreprocessTargetWidth, modelprops.CustomVisionPreprocessTargetHeight, nameof(ImageInputData.Image), ImageResizingEstimator.ResizingKind.Fill)\r\n                                .Append(mlContext.Transforms.ExtractPixels(\"data\", \"image\"))\r\n                                .Append(mlContext.Transforms.ApplyOnnxModel(\"model_output\", \"data\", modelFolderPath + @\"model.onnx\", 0));`\r\n\r\n\r\nAverage analysis time = varies from 120 to 150ms\r\nProcessor usage: ~13%\r\nMemory: 3.2GB.\r\n\r\n![image](https://user-images.githubusercontent.com/5542052/148016947-b38e3dd9-506b-44f5-9062-98c9e9f7ad36.png)\r\n\r\nusing any other device ID other than 0 results in an exception when the pipeline is created saying\r\n`{\"Error initializing model :Microsoft.ML.OnnxRuntime.OnnxRuntimeException: [ErrorCode:Fail] D:\\\\a\\\\_work\\\\1\\\\s\\\\onnxruntime\\\\core\\\\providers\\\\cuda\\\\cuda_call.cc:122 onnxruntime::CudaCall D:\\\\a\\\\_work\\\\1\\\\s\\\\onnxruntime\\\\core\\\\providers\\\\cuda\\\\cuda_call.cc:116 onnxruntime::CudaCall CUDA failure 101: invalid device ordinal ; GPU=0 ; hostname=DESKTOP-IC179BD ; expr=cudaSetDevice(info_.device_id); \\n\\n\\r\\n   at Microsoft.ML.OnnxRuntime.NativeApiStatus.VerifySuccess(IntPtr nativeStatus)\\r\\n   at Microsoft.ML.OnnxRuntime.InferenceSession.Init(String modelPath, SessionOptions options, PrePackedWeightsContainer prepackedWeightsContainer)\\r\\n   at Microsoft.ML.OnnxRuntime.InferenceSession..ctor(String modelPath, SessionOptions options)\\r\\n   at Microsoft.ML.Transforms.Onnx.OnnxModel..ctor(String modelFile, Nullable`1 gpuDeviceId, Boolean fallbackToCpu, Boolean ownModelFile, IDictionary`2 shapeDictionary, Int32 recursionLimit, Nullable`1 interOpNumThreads, Nullable`1 intraOpNumThreads)\\r\\n   at Microsoft.ML.Transforms.Onnx.OnnxTransformer..ctor(IHostEnvironment env, Options options, Byte[] modelBytes)\"}`\r\n\r\nNot providing the device ID at all leads to same results as Benchmark 1. Which means GPU is not used.\r\n\r\nBenchmark 6\r\nUsing Onnxruntime.DirectML v1.10 with ML.NET, no device ID given during pipeline initiation\r\n\r\nSame results as benchmark 1\r\n\r\nBenchmark 7\r\nUsing Onnxruntime.DirectML v1.10 with ML.NET,  device ID 0 given during pipeline initiation\r\n\r\nCode does not execute and leads to exception during pipeline creation:\r\n`{\"Unable to find an entry point named 'OrtSessionOptionsAppendExecutionProvider_CUDA' in DLL 'onnxruntime'.\":\"\"}`\r\n\r\n\r\n**Conclusion:**\r\n\r\n- Calling OnnxRuntime with GPU support leads to a much higher utilization of Process Memory(>3GB), while saving on the processor usage. There are hardly any noticable performance gains. \r\n- It seems ML.NET handles image transformation operation pretty well, and for that reason, any attempts to call ORT directly and utilizing external logic to resize image/load tensor offsets any performance gains. \r\n- Not specifying Device ID during ML.NET pipeline creation or not specifying CUDA during ORT Session creation always leads to CPU being used for inferencing. Hence its key to specify these if GPU usage is intended.\r\n- OnnxRuntime.DirectML does not seem to be able bring GPU in action. My understanding was that DirectML could make good use of a generic GPU(Intel's built in, Nvidia) but so far that does not seem to be the case. @fdwr  we were talking about this in a seperate post, but I guess we can consolidate all experiments with Onnxruntime Hardware acceleration here.\r\n\r\nThe question is, what could be done to bring down this inference time drastically. So far nothing that i did seems to be helping bring the inference time to less than 15ms.\r\n\r\nTo take this forward and have you guys try out all scenarios, I am willing to share the full project with onnx model and sample images so further benchmarking can be done.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004558789/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004621480",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1004621480",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1004621480,
        "node_id": "IC_kwDOCVq1mM474U6o",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-04T08:44:49Z",
        "updated_at": "2022-01-04T08:44:49Z",
        "author_association": "MEMBER",
        "body": "Can you please tighten up how you're measuring time? Including something like Console.Writeline can greatly skew the numbers for a run. Things like the tensor loading time in benchmark 2 vs 3 is really suspicious. Shouldn't that always be pretty equal given it has nothing to do with how the data is executed? \r\n\r\nFor the discussion of whether ORT is using GPU or not, it will be simpler to just focus on the Run call time - i.e start/stop of the stopwatch for a single line that calls Run.\r\n\r\nThe CUDA operator kernels are rather large, so I think the much larger memory usage is a good signal that the CUDA execution provider is being loaded.\r\n\r\nBenchmark 3 was 37.5ms vs 59.6 with CPU for benchmark 2. That seems significantly better to me and a good signal that the CUDA execution provider is being used.\r\n\r\nThere's also the device copy overhead to copy input/output between CPU and CUDA to be able to run the model and retrieve the results to take into account. It's possible to avoid that but doing so is only meaningful if your input starts on GPU (won't do given you have pre-processing) or output will be used on GPU (generally not the case unless you're feeding the output into another model).\r\n\r\nI'm not quite sure what your expectations are regarding performance. Where does the 15ms target come from? \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004621480/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004624067",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1004624067",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1004624067,
        "node_id": "IC_kwDOCVq1mM474VjD",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-04T08:49:03Z",
        "updated_at": "2022-01-04T08:49:03Z",
        "author_association": "MEMBER",
        "body": "One other thing you could do is use the NVIDIA Control Panel to see GPU utilization when running the model. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004624067/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004638597",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1004638597",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1004638597,
        "node_id": "IC_kwDOCVq1mM474ZGF",
        "user": {
            "login": "noumanqaiser",
            "id": 5542052,
            "node_id": "MDQ6VXNlcjU1NDIwNTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5542052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/noumanqaiser",
            "html_url": "https://github.com/noumanqaiser",
            "followers_url": "https://api.github.com/users/noumanqaiser/followers",
            "following_url": "https://api.github.com/users/noumanqaiser/following{/other_user}",
            "gists_url": "https://api.github.com/users/noumanqaiser/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/noumanqaiser/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/noumanqaiser/subscriptions",
            "organizations_url": "https://api.github.com/users/noumanqaiser/orgs",
            "repos_url": "https://api.github.com/users/noumanqaiser/repos",
            "events_url": "https://api.github.com/users/noumanqaiser/events{/privacy}",
            "received_events_url": "https://api.github.com/users/noumanqaiser/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-04T09:11:38Z",
        "updated_at": "2022-01-04T09:11:38Z",
        "author_association": "NONE",
        "body": "@skottmckay The entire discussion stems from an evaluation where my team is evaluating if Microsoft CustomVision could be used as a generic Model training platform, and models eventually deployed in manufacturing to run high speed inferencing on images obtained using Machine vision cameras, so the idea was if Microsoft CustomVision+OnnxRuntime+ML.NET could be a high performance solution for defect analysis. On high speed manufacturing lines, inference performance is key to reaching a decision if the product being analyzed should be rejected or production line stopped.\r\n\r\nNow in the case above, what I takeaway is, there is hardly much performance difference in Benchmark 1(ML.NET+OnnxRuntime) vs Benchmark 3(OnnxRuntime.GPU+External Preprocessing). The added value from Cuda hardware acceleration is being offset elsewhere. Do you think Benchmark 1 is the best possible solution in this case or we still have some options?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004638597/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004713131",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1004713131",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1004713131,
        "node_id": "IC_kwDOCVq1mM474rSr",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-04T11:01:26Z",
        "updated_at": "2022-01-04T22:09:10Z",
        "author_association": "MEMBER",
        "body": "At this point I would have said the numbers weren't reliable enough and I'd be looking to re-measure without any extraneous code (especially Console.WriteLine calls within timed regions). \r\n\r\nSomething like the Image pre/post processing times should be consistent. Without achieving consistency or understanding why you can't, I would have a low level of confidence in the numbers in general. Using averages is also potentially flawed. One slow result (e.g. Console.WriteLine blocks) could overly affect the values, so understanding the distribution of the latency measurements may be important (depending on what percentile you measure production latency at of course).\r\n\r\nIs it possible to take all the pre/post-processing out of the measurements and just measure the Run/predict call. Hopefully there's a way to call ML.NET with your manually pre-processed image as input to be able to do that for the ML.NET values.\r\n\r\nNot sure if CustomVision has an option to move the pre-processing into the model post-training. We're currently implementing that sort of capability. It would be run as a one-off via a python script to update the model. That way the resize and channel/layout transpose would be handled within the InferenceSession.Run by optimized code that is parallelized where applicable (including running that processing on GPU if that's enabled). I would also guess that ML.NET has pretty optimized implementations of these common pre-processing steps, hence the overhead of doing that via their pipeline is likely a lot lower than the primitive C# code from our examples. e.g. instead of setting individual pixels you can block copy sections depending on what the transpose needs to do. \r\n\r\nDoes using ML.NET let you take advantage of ORT's support for concurrent calls and batching? What is the relative importance of the the per-Run latency vs the throughput as concurrent requests and/or batching would help with throughput.\r\n\r\nFor the device copy overhead to use CUDA, batching may amortize some of that. How much that copy costs though is probably device dependent. Testing on a laptop may have a completely different overhead to hardware that is purely focused on processing an image using the GPU. That said, your measurement for inferencing time includes this device copy overhead and benchmark 3 was significantly faster for this than benchmark 2 (the call to Run is handling the copy to/from CUDA). \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1004713131/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1005302523",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1005302523",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1005302523,
        "node_id": "IC_kwDOCVq1mM4767L7",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-05T01:22:48Z",
        "updated_at": "2022-01-05T01:22:48Z",
        "author_association": "MEMBER",
        "body": "One other way to find out if the CUDA execution provider is being exercised is to turn on verbose logging and look for \"Node placements\" in the logs. This would tell you which nodes (if not all) were placed on CUDA.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1005302523/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014333912",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1014333912",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1014333912,
        "node_id": "IC_kwDOCVq1mM48dYHY",
        "user": {
            "login": "rvdinter",
            "id": 25580585,
            "node_id": "MDQ6VXNlcjI1NTgwNTg1",
            "avatar_url": "https://avatars.githubusercontent.com/u/25580585?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rvdinter",
            "html_url": "https://github.com/rvdinter",
            "followers_url": "https://api.github.com/users/rvdinter/followers",
            "following_url": "https://api.github.com/users/rvdinter/following{/other_user}",
            "gists_url": "https://api.github.com/users/rvdinter/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rvdinter/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rvdinter/subscriptions",
            "organizations_url": "https://api.github.com/users/rvdinter/orgs",
            "repos_url": "https://api.github.com/users/rvdinter/repos",
            "events_url": "https://api.github.com/users/rvdinter/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rvdinter/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-17T09:53:58Z",
        "updated_at": "2022-01-17T09:53:58Z",
        "author_association": "NONE",
        "body": "I am experiencing the same issue when first loading the ONNX model, saving it to .zip, and later loading this model. The GPU does not seem to use more VRAM, nor processing power. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014333912/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015644451",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1015644451",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1015644451,
        "node_id": "IC_kwDOCVq1mM48iYEj",
        "user": {
            "login": "michaelgsharp",
            "id": 51342856,
            "node_id": "MDQ6VXNlcjUxMzQyODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/51342856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/michaelgsharp",
            "html_url": "https://github.com/michaelgsharp",
            "followers_url": "https://api.github.com/users/michaelgsharp/followers",
            "following_url": "https://api.github.com/users/michaelgsharp/following{/other_user}",
            "gists_url": "https://api.github.com/users/michaelgsharp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/michaelgsharp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/michaelgsharp/subscriptions",
            "organizations_url": "https://api.github.com/users/michaelgsharp/orgs",
            "repos_url": "https://api.github.com/users/michaelgsharp/repos",
            "events_url": "https://api.github.com/users/michaelgsharp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/michaelgsharp/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-18T17:27:00Z",
        "updated_at": "2022-01-18T17:27:00Z",
        "author_association": "MEMBER",
        "body": "Yeah, I'm one of the devs on the ML.NET side of things. This is actually an issue on our side. After you save/load a model, it no longer uses the GPU and the user doesn't have a way of forcing it to use the GPU. We didn't realize this was an issue until this github issue was made, but we are currently working on a fix for it.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015644451/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1023987173",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1023987173",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1023987173,
        "node_id": "IC_kwDOCVq1mM49CM3l",
        "user": {
            "login": "mmayer-lgtm",
            "id": 98581529,
            "node_id": "U_kgDOBeA8GQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/98581529?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mmayer-lgtm",
            "html_url": "https://github.com/mmayer-lgtm",
            "followers_url": "https://api.github.com/users/mmayer-lgtm/followers",
            "following_url": "https://api.github.com/users/mmayer-lgtm/following{/other_user}",
            "gists_url": "https://api.github.com/users/mmayer-lgtm/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mmayer-lgtm/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mmayer-lgtm/subscriptions",
            "organizations_url": "https://api.github.com/users/mmayer-lgtm/orgs",
            "repos_url": "https://api.github.com/users/mmayer-lgtm/repos",
            "events_url": "https://api.github.com/users/mmayer-lgtm/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mmayer-lgtm/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-28T08:22:39Z",
        "updated_at": "2022-01-28T08:22:39Z",
        "author_association": "NONE",
        "body": "@michaelgsharp \r\nSorry for asking; but is there any news on this one? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1023987173/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024381463",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1024381463",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1024381463,
        "node_id": "IC_kwDOCVq1mM49DtIX",
        "user": {
            "login": "noumanqaiser",
            "id": 5542052,
            "node_id": "MDQ6VXNlcjU1NDIwNTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5542052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/noumanqaiser",
            "html_url": "https://github.com/noumanqaiser",
            "followers_url": "https://api.github.com/users/noumanqaiser/followers",
            "following_url": "https://api.github.com/users/noumanqaiser/following{/other_user}",
            "gists_url": "https://api.github.com/users/noumanqaiser/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/noumanqaiser/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/noumanqaiser/subscriptions",
            "organizations_url": "https://api.github.com/users/noumanqaiser/orgs",
            "repos_url": "https://api.github.com/users/noumanqaiser/repos",
            "events_url": "https://api.github.com/users/noumanqaiser/events{/privacy}",
            "received_events_url": "https://api.github.com/users/noumanqaiser/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-28T16:27:26Z",
        "updated_at": "2022-01-28T16:27:26Z",
        "author_association": "NONE",
        "body": "@michaelgsharp Can you confirm that the GPU was not being utilized even in benchmark 3 when the session was initiated using:\r\n\r\nusing (var session = new InferenceSession(modelPath ,SessionOptions.MakeSessionOptionWithCudaProvider()))\r\n\r\nand the initialization alone occupied a good deal of Ram? If yes, any idea how early this issue can be solved so we can exploit the maximum benefits from cuda acceleration?\r\n\r\nOn a seperate thread, I have raised another issue where onnxruntime.DirectMl is not resulting in any performance gain either. Is the issue you have found generic and preventing all types of hardware acceleration in ML.net(both Cuda and DirectML).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1024381463/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1027317329",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1027317329",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1027317329,
        "node_id": "IC_kwDOCVq1mM49O55R",
        "user": {
            "login": "mmayer-lgtm",
            "id": 98581529,
            "node_id": "U_kgDOBeA8GQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/98581529?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mmayer-lgtm",
            "html_url": "https://github.com/mmayer-lgtm",
            "followers_url": "https://api.github.com/users/mmayer-lgtm/followers",
            "following_url": "https://api.github.com/users/mmayer-lgtm/following{/other_user}",
            "gists_url": "https://api.github.com/users/mmayer-lgtm/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mmayer-lgtm/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mmayer-lgtm/subscriptions",
            "organizations_url": "https://api.github.com/users/mmayer-lgtm/orgs",
            "repos_url": "https://api.github.com/users/mmayer-lgtm/repos",
            "events_url": "https://api.github.com/users/mmayer-lgtm/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mmayer-lgtm/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-01T21:45:11Z",
        "updated_at": "2022-02-01T21:45:11Z",
        "author_association": "NONE",
        "body": "@noumanqaiser \r\n\r\nJust one comment about this; not sure if this is helpful.\r\n\r\nWith an object detection model which is created and trained through Model Builder in Visual Studio (and Azure), the Inference is at a factor of ~5 faster with CUDA than with just CPU running on my noteboox (Nvidia Quadro T2000) (~130ms vs. ~700ms for session.run() step)\r\n\r\nOf course (as you already said) the Execution Provider for CUDA needs to added:\r\n\r\n             SessionOptions options = new SessionOptions();\r\n            options.LogSeverityLevel = OrtLoggingLevel.ORT_LOGGING_LEVEL_WARNING;//_VERBOSE\r\n            options.AppendExecutionProvider_CUDA(0);\r\n\r\n            var session = new InferenceSession(@\"D:\\MLNET\\Project2cpp\\MLModel1.onnx\",options);\r\n            using IDisposableReadOnlyCollection<DisposableNamedOnnxValue> results = session.Run(inputs);\r\n\r\nThe installed Cuda and cuDnn Versions are the ones mentioned here for version 1.10 of the Onnxruntime: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/98581529/152055236-c7ac887d-f0de-4cc7-b04c-bf14e7774f07.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1027317329/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1027319639",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1027319639",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1027319639,
        "node_id": "IC_kwDOCVq1mM49O6dX",
        "user": {
            "login": "michaelgsharp",
            "id": 51342856,
            "node_id": "MDQ6VXNlcjUxMzQyODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/51342856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/michaelgsharp",
            "html_url": "https://github.com/michaelgsharp",
            "followers_url": "https://api.github.com/users/michaelgsharp/followers",
            "following_url": "https://api.github.com/users/michaelgsharp/following{/other_user}",
            "gists_url": "https://api.github.com/users/michaelgsharp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/michaelgsharp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/michaelgsharp/subscriptions",
            "organizations_url": "https://api.github.com/users/michaelgsharp/orgs",
            "repos_url": "https://api.github.com/users/michaelgsharp/repos",
            "events_url": "https://api.github.com/users/michaelgsharp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/michaelgsharp/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-01T21:48:43Z",
        "updated_at": "2022-02-01T21:48:43Z",
        "author_association": "MEMBER",
        "body": "You should be able to see benefit using the GPU in ML.NET until the model has been saved. After the saving is where the current issue comes into play where you cannot specify running on the GPU.\r\n\r\nThat appears to be what is happening as @mmayer-lgtm is seeing the benefit when using model builder (which is before the ML.NET model has been saved/loaded.). Loading the model after its been trained by model builder should then see only the CPU being used.\r\n\r\nWe are working on a fix for this on the ML.NET side.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1027319639/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1030628195",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1030628195",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1030628195,
        "node_id": "IC_kwDOCVq1mM49biNj",
        "user": {
            "login": "noumanqaiser",
            "id": 5542052,
            "node_id": "MDQ6VXNlcjU1NDIwNTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5542052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/noumanqaiser",
            "html_url": "https://github.com/noumanqaiser",
            "followers_url": "https://api.github.com/users/noumanqaiser/followers",
            "following_url": "https://api.github.com/users/noumanqaiser/following{/other_user}",
            "gists_url": "https://api.github.com/users/noumanqaiser/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/noumanqaiser/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/noumanqaiser/subscriptions",
            "organizations_url": "https://api.github.com/users/noumanqaiser/orgs",
            "repos_url": "https://api.github.com/users/noumanqaiser/repos",
            "events_url": "https://api.github.com/users/noumanqaiser/events{/privacy}",
            "received_events_url": "https://api.github.com/users/noumanqaiser/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-05T13:45:57Z",
        "updated_at": "2022-02-05T13:45:57Z",
        "author_association": "NONE",
        "body": "@mmayer-lgtm As you said, calling OnnxRuntime.GPU directly and initiating a session with CUDA execution provider helps you achieve improvement in inference time, but I noticed that this benefit is eaten by inefficient image preprocessing methods(resizing the bitmap and transposing it pixel by pixel into format expected by the model). I tried out various methods but it seems ML.NET is almost always offering superior performance for this preprocessing process.\r\n\r\nGiven this, it would be optimal to use ML.NET to optimize overall execution time(preprocessing+inferencing). \r\n\r\n@michaelgsharp  I got it, In my case I would always start with a saved model, and hence the observation. Any idea by when this issue can be closed? Would you be releasing a new Nuget package for this fix?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1030628195/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100581279",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1100581279",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1100581279,
        "node_id": "IC_kwDOCVq1mM5BmYmf",
        "user": {
            "login": "stale[bot]",
            "id": 26384082,
            "node_id": "MDM6Qm90MjYzODQwODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/1724?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stale%5Bbot%5D",
            "html_url": "https://github.com/apps/stale",
            "followers_url": "https://api.github.com/users/stale%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/stale%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/stale%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stale%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stale%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/stale%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/stale%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/stale%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stale%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "created_at": "2022-04-16T05:54:57Z",
        "updated_at": "2022-04-16T05:54:57Z",
        "author_association": "NONE",
        "body": "This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100581279/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100588503",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10142#issuecomment-1100588503",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10142",
        "id": 1100588503,
        "node_id": "IC_kwDOCVq1mM5BmaXX",
        "user": {
            "login": "noumanqaiser",
            "id": 5542052,
            "node_id": "MDQ6VXNlcjU1NDIwNTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5542052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/noumanqaiser",
            "html_url": "https://github.com/noumanqaiser",
            "followers_url": "https://api.github.com/users/noumanqaiser/followers",
            "following_url": "https://api.github.com/users/noumanqaiser/following{/other_user}",
            "gists_url": "https://api.github.com/users/noumanqaiser/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/noumanqaiser/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/noumanqaiser/subscriptions",
            "organizations_url": "https://api.github.com/users/noumanqaiser/orgs",
            "repos_url": "https://api.github.com/users/noumanqaiser/repos",
            "events_url": "https://api.github.com/users/noumanqaiser/events{/privacy}",
            "received_events_url": "https://api.github.com/users/noumanqaiser/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-04-16T06:48:40Z",
        "updated_at": "2022-04-16T06:48:40Z",
        "author_association": "NONE",
        "body": "> You should be able to see benefit using the GPU in ML.NET until the model has been saved. After the saving is where the current issue comes into play where you cannot specify running on the GPU.\r\n> \r\n> That appears to be what is happening as @mmayer-lgtm is seeing the benefit when using model builder (which is before the ML.NET model has been saved/loaded.). Loading the model after its been trained by model builder should then see only the CPU being used.\r\n> \r\n> We are working on a fix for this on the ML.NET side.\r\n\r\nHi @michaelgsharp  Can you confirm if this fix has been rolled out in ML.net?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100588503/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]