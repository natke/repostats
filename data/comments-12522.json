[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1209399760",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12522#issuecomment-1209399760",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12522",
        "id": 1209399760,
        "node_id": "IC_kwDOCVq1mM5IFfnQ",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-09T13:42:36Z",
        "updated_at": "2022-08-09T13:43:16Z",
        "author_association": "CONTRIBUTOR",
        "body": "Running the same script on an AWS EC2 c6i instance gives:\r\n\r\n```\r\ncpu\r\n['CPUExecutionProvider']\r\n\r\n--- BATCH SIZE 1 ---\r\nPyTorch: 1.99 s\r\nONNX Runtime: 5.56 s\r\nORT 179.97 % slower than PT\r\n\r\n--- BATCH SIZE 2 ---\r\nPyTorch: 2.61 s\r\nONNX Runtime: 10.81 s\r\nORT 313.96 % slower than PT\r\n\r\n--- BATCH SIZE 4 ---\r\nPyTorch: 3.67 s\r\nONNX Runtime: 19.81 s\r\nORT 439.57 % slower than PT\r\n\r\n--- BATCH SIZE 8 ---\r\nPyTorch: 6.62 s\r\nONNX Runtime: 37.55 s\r\nORT 466.96 % slower than PT\r\n```\r\n\r\nlscpu gives\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         46 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\r\n    CPU family:          6\r\n    Model:               106\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            6\r\n    BogoMIPS:            5799.98\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm\r\n                          constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2\r\n                         apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb st\r\n                         ibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb a\r\n                         vx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmu\r\n                         lqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities\r\nVirtualization features: \r\n  Hypervisor vendor:     KVM\r\n  Virtualization type:   full\r\nCaches (sum of all):     \r\n  L1d:                   384 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    10 MiB (8 instances)\r\n  L3:                    54 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:         \r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1209399760/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1209455794",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12522#issuecomment-1209455794",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12522",
        "id": 1209455794,
        "node_id": "IC_kwDOCVq1mM5IFtSy",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-09T14:28:06Z",
        "updated_at": "2022-08-09T14:35:49Z",
        "author_association": "CONTRIBUTOR",
        "body": "To follow up on this, I ran the onnxruntime profiler on my model (on my laptop) to see what is taking so much time. Here's my finding, with batch size = 4:\r\n\r\n![image](https://user-images.githubusercontent.com/9808326/183674376-aac3ed85-8a85-4260-a482-e0f55b3b996a.png)\r\n\r\nSo it seems this issue is indeed related to https://github.com/microsoft/onnxruntime/issues/12130\r\n\r\nWhen profiling the PyTorch model with FX, it is clearly not the batchnorm taking most of the time: https://pastebin.com/CxmXYbY7\r\n\r\nScript:\r\n\r\n```python\r\nimport onnxruntime\r\nimport time\r\nimport torch\r\nimport numpy as np\r\nimport json\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\nmodel_path = \"/path/to/model.onnx\"\r\n\r\noptions = onnxruntime.SessionOptions()\r\noptions.enable_profiling = True\r\n\r\nort_session = onnxruntime.InferenceSession(\r\n    model_path,\r\n    sess_options=options,\r\n    providers=['CPUExecutionProvider']\r\n)\r\n\r\nbatch_size = 4\r\nprint(f\"\\n--- BATCH SIZE {batch_size} ---\")\r\npt_inputs = dict()\r\npt_inputs[\"pixel_values\"] = torch.ones(batch_size, 3, 224, 224, dtype=torch.float32)\r\n\r\nonnx_inputs = {\r\n    \"pixel_values\": pt_inputs[\"pixel_values\"].cpu().detach().numpy(),\r\n}\r\n\r\nfor i in range(200):\r\n    res = ort_session.run(None, onnx_inputs)\r\n\r\nprof = ort_session.end_profiling()\r\nprint(prof)\r\n\r\njson_path = f\"/path/to/{prof}\"\r\nwith open(json_path, \"r\") as f:\r\n    js = json.load(f)\r\n\r\ndef process_profiling(js):\r\n    \"\"\"\r\n    Flattens json returned by onnxruntime profiling.\r\n    :param js: json\r\n    :return: list of dictionaries\r\n    \"\"\"\r\n    rows = []\r\n    for row in js:\r\n        if 'args' in row and isinstance(row['args'], dict):\r\n            for k, v in row['args'].items():\r\n                row[f'args_{k}'] = v\r\n            del row['args']\r\n        rows.append(row)\r\n    return rows\r\n\r\ndf = pd.DataFrame(process_profiling(js))\r\n\r\ngr_dur = df[['dur', \"args_op_name\"]].groupby(\r\n    \"args_op_name\").sum().sort_values('dur')\r\ngr_n = df[['dur', \"args_op_name\"]].groupby(\r\n    \"args_op_name\").count().sort_values('dur')\r\ngr_n = gr_n.loc[gr_dur.index, :]\r\n\r\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\r\ngr_dur.plot.barh(ax=ax[0])\r\ngr_n.plot.barh(ax=ax[1])\r\nax[0].set_title(\"duration\")\r\nax[1].set_title(\"n occurences\")\r\n\r\nplt.show()\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1209455794/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1209618944",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12522#issuecomment-1209618944",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12522",
        "id": 1209618944,
        "node_id": "IC_kwDOCVq1mM5IGVIA",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-09T16:41:43Z",
        "updated_at": "2022-08-09T16:41:43Z",
        "author_association": "MEMBER",
        "body": "BatchNormalization itself is not optimized specifically because it is usually following Conv and can be fused into Conv.\r\nFor this model, BatchNormalization can be fused together with MatMul into an extended Gemm or a MatMul + Bias. \r\n\r\n![image](https://user-images.githubusercontent.com/30486710/183708684-b23230f1-b79a-4882-8b73-4bee9c417052.png)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1209618944/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1210337825",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12522#issuecomment-1210337825",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12522",
        "id": 1210337825,
        "node_id": "IC_kwDOCVq1mM5IJEoh",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-10T08:29:05Z",
        "updated_at": "2022-08-11T10:21:57Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks a lot for your help. Should I do the fusing by hand or is this an optimization proposed by onnxruntime? I could not find ressource on this in the documentation.\r\n\r\nEdit: note I am using an exotic model where there is this flatten inbetween MatMul and BatchNorm.\r\nEdit2: Can confirm BatchNorm2d is folded into Conv2d when converting from PyTorch. Here it's an exotic case hence no automatic folding.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1210337825/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]