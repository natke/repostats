[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001716478",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1001716478",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1001716478,
        "node_id": "IC_kwDOCVq1mM47tPr-",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-27T19:43:19Z",
        "updated_at": "2021-12-28T00:39:58Z",
        "author_association": "MEMBER",
        "body": "ORT is designed to give you the best performance at inference time. So we try to do as much work as possible when we load it once, optimized it and then you can execute it multiple times.\r\nThat is why we read the model (does not matter if the bytes are on disk or already in memory, we still have to deserialize it), sort the graph in topological order, resolve it (find appropriate kernels - implementations for each graph node that take into account versioning and kernel signatures), apply optimizations and do many other things to make execution possible.\r\nYour question is very broad. Read the documentation and if you want to know more details, read the code.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001716478/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001871974",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1001871974",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1001871974,
        "node_id": "IC_kwDOCVq1mM47t1pm",
        "user": {
            "login": "igaloly",
            "id": 38460810,
            "node_id": "MDQ6VXNlcjM4NDYwODEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/38460810?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igaloly",
            "html_url": "https://github.com/igaloly",
            "followers_url": "https://api.github.com/users/igaloly/followers",
            "following_url": "https://api.github.com/users/igaloly/following{/other_user}",
            "gists_url": "https://api.github.com/users/igaloly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igaloly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igaloly/subscriptions",
            "organizations_url": "https://api.github.com/users/igaloly/orgs",
            "repos_url": "https://api.github.com/users/igaloly/repos",
            "events_url": "https://api.github.com/users/igaloly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igaloly/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-28T05:07:35Z",
        "updated_at": "2021-12-28T05:07:35Z",
        "author_association": "NONE",
        "body": "@yuslepukhin \r\nWhy some of the procedures that are running in Inference Time were not implemented during the build time (conversion to the ONNX model)?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001871974/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002358217",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1002358217",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1002358217,
        "node_id": "IC_kwDOCVq1mM47vsXJ",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-29T01:54:10Z",
        "updated_at": "2021-12-29T01:54:10Z",
        "author_association": "MEMBER",
        "body": "Model loading involves conversion from the ONNX format to the internal ORT data structures. Whether the ONNX model comes from bytes or a file on disk doesn't affect the need for this conversion.\r\n\r\nDuring the creation of the session we do this conversion, run any applicable optimizations (which take into account the execution providers that are enabled and the current hardware), create plans for execution and memory allocation, move initializers to the device they're required on, and various other things. This is all one-off work though, and what generally matters the most is the execution time to run the model, as an InferenceSession instance supports multiple concurrent requests. i.e. you should re-use the InferenceSession and not create a new one for each request.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002358217/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002403886",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1002403886",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1002403886,
        "node_id": "IC_kwDOCVq1mM47v3gu",
        "user": {
            "login": "igaloly",
            "id": 38460810,
            "node_id": "MDQ6VXNlcjM4NDYwODEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/38460810?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igaloly",
            "html_url": "https://github.com/igaloly",
            "followers_url": "https://api.github.com/users/igaloly/followers",
            "following_url": "https://api.github.com/users/igaloly/following{/other_user}",
            "gists_url": "https://api.github.com/users/igaloly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igaloly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igaloly/subscriptions",
            "organizations_url": "https://api.github.com/users/igaloly/orgs",
            "repos_url": "https://api.github.com/users/igaloly/repos",
            "events_url": "https://api.github.com/users/igaloly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igaloly/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-29T05:27:27Z",
        "updated_at": "2021-12-29T05:27:27Z",
        "author_association": "NONE",
        "body": "@skottmckay \r\nThe extra details about the `InferenceSession` init are great!\r\n\r\nFrom what you said, I assume there aren't many procedures that can be made in build time, right?\r\n\r\nMoreover, I think your idea of re-using the `InferenseSession` instance is great.\r\nUnfortunately, I don't see how it is possible currently.\r\n\r\nFor example, I have a web server that returns a prediction on request.\r\nFor each request, I need to: Fetch the model -> Create an `InferenceSession` instance -> Predict.\r\nYou can see that the instance initialization happens on each request.\r\n\r\nI can cache the instance on the current server, but if my infrastructure is horizontally scaled, and because of the lack of serialization of the instance, the user might end up reaching a different server, that doesn't have the instance in memory.\r\n\r\nMoreover, if I have a lot of models, saving all of them always in memory is not efficient.\r\n\r\nI would love to hear more of your thoughts! :)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002403886/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002433984",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1002433984",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1002433984,
        "node_id": "IC_kwDOCVq1mM47v-3A",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-29T07:27:38Z",
        "updated_at": "2021-12-29T07:27:38Z",
        "author_association": "MEMBER",
        "body": "If you save the optimized model there's not much else you can do to reduce the cost of creating the inference session.\r\n\r\nIf the cost of caching all the models on all servers is too high, I'd suggest the selection of the server for a request should take into account the model involved so that you limit the number of models an individual server would need to cache. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002433984/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1003124826",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1003124826",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1003124826,
        "node_id": "IC_kwDOCVq1mM47ynha",
        "user": {
            "login": "igaloly",
            "id": 38460810,
            "node_id": "MDQ6VXNlcjM4NDYwODEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/38460810?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igaloly",
            "html_url": "https://github.com/igaloly",
            "followers_url": "https://api.github.com/users/igaloly/followers",
            "following_url": "https://api.github.com/users/igaloly/following{/other_user}",
            "gists_url": "https://api.github.com/users/igaloly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igaloly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igaloly/subscriptions",
            "organizations_url": "https://api.github.com/users/igaloly/orgs",
            "repos_url": "https://api.github.com/users/igaloly/repos",
            "events_url": "https://api.github.com/users/igaloly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igaloly/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-30T17:46:49Z",
        "updated_at": "2021-12-30T17:46:49Z",
        "author_association": "NONE",
        "body": "Got it.\r\nThank you for all of your help, @skottmckay \r\nI really appreciate it!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1003124826/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1212019541",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1212019541",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1212019541,
        "node_id": "IC_kwDOCVq1mM5IPfNV",
        "user": {
            "login": "AnakTeka",
            "id": 3157407,
            "node_id": "MDQ6VXNlcjMxNTc0MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3157407?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/AnakTeka",
            "html_url": "https://github.com/AnakTeka",
            "followers_url": "https://api.github.com/users/AnakTeka/followers",
            "following_url": "https://api.github.com/users/AnakTeka/following{/other_user}",
            "gists_url": "https://api.github.com/users/AnakTeka/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/AnakTeka/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/AnakTeka/subscriptions",
            "organizations_url": "https://api.github.com/users/AnakTeka/orgs",
            "repos_url": "https://api.github.com/users/AnakTeka/repos",
            "events_url": "https://api.github.com/users/AnakTeka/events{/privacy}",
            "received_events_url": "https://api.github.com/users/AnakTeka/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-08-11T13:51:43Z",
        "updated_at": "2022-08-11T13:51:43Z",
        "author_association": "NONE",
        "body": "@igagoly\r\n\r\n> What's the most time-consuming process in python's `InferenceSession` class init? By profiling using `cProfile`, I have noticed that the `_create_inference_session` which runs in `InferenceSession`'s init is the most time-consuming.\r\n> \r\n> I consulted with the great @xadupre which explained to me that the process of `InferenceSession` is generally as follows: Model Loading -> Graph Optimization -> Data Structure Modifications.\r\n> \r\n> Xavier recommended that saving and using the optimized graph **may** save time. I've tried it, and even though a bit of time was saved, the change wasn't significant. You can try it yourself in this Colab notebook: https://colab.research.google.com/drive/1KwtH90g2NPC8P_bmTT31GdytV2OlS8Y7?usp=sharing\r\n> \r\n> If the Graph Optimization isn't the bottleneck, then I may assume that `Model Loading` or `Data Structure Modifications` is.\r\n> \r\n> I want to understand a bit more about the Model Loading process. If for example, instead of passing a string (file path) to `InferenceSession`, I'd pass model bytes buffer, will there still be a Model Loading process? I assume No because the model is already loaded in the memory and the CPP code should just get the pointer to the memory location.\r\n> \r\n> Also, I want to understand a bit better the Data Structure Modifying process. Why is it needed and what's happening in the process?\r\n\r\n@igaloly do you still have the Colab notebook? I'm trying to pickle the `InferenceSession objects` to no avail, would be great if you have any pointer to pickle the `InferenceSession objects` object.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1212019541/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]