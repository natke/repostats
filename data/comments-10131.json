[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001716478",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1001716478",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1001716478,
        "node_id": "IC_kwDOCVq1mM47tPr-",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-27T19:43:19Z",
        "updated_at": "2021-12-28T00:39:58Z",
        "author_association": "MEMBER",
        "body": "ORT is designed to give you the best performance at inference time. So we try to do as much work as possible when we load it once, optimized it and then you can execute it multiple times.\r\nThat is why we read the model (does not matter if the bytes are on disk or already in memory, we still have to deserialize it), sort the graph in topological order, resolve it (find appropriate kernels - implementations for each graph node that take into account versioning and kernel signatures), apply optimizations and do many other things to make execution possible.\r\nYour question is very broad. Read the documentation and if you want to know more details, read the code.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001716478/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001871974",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1001871974",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1001871974,
        "node_id": "IC_kwDOCVq1mM47t1pm",
        "user": {
            "login": "igaloly",
            "id": 38460810,
            "node_id": "MDQ6VXNlcjM4NDYwODEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/38460810?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igaloly",
            "html_url": "https://github.com/igaloly",
            "followers_url": "https://api.github.com/users/igaloly/followers",
            "following_url": "https://api.github.com/users/igaloly/following{/other_user}",
            "gists_url": "https://api.github.com/users/igaloly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igaloly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igaloly/subscriptions",
            "organizations_url": "https://api.github.com/users/igaloly/orgs",
            "repos_url": "https://api.github.com/users/igaloly/repos",
            "events_url": "https://api.github.com/users/igaloly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igaloly/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-28T05:07:35Z",
        "updated_at": "2021-12-28T05:07:35Z",
        "author_association": "NONE",
        "body": "@yuslepukhin \r\nWhy some of the procedures that are running in Inference Time were not implemented during the build time (conversion to the ONNX model)?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1001871974/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002358217",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1002358217",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1002358217,
        "node_id": "IC_kwDOCVq1mM47vsXJ",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-29T01:54:10Z",
        "updated_at": "2021-12-29T01:54:10Z",
        "author_association": "MEMBER",
        "body": "Model loading involves conversion from the ONNX format to the internal ORT data structures. Whether the ONNX model comes from bytes or a file on disk doesn't affect the need for this conversion.\r\n\r\nDuring the creation of the session we do this conversion, run any applicable optimizations (which take into account the execution providers that are enabled and the current hardware), create plans for execution and memory allocation, move initializers to the device they're required on, and various other things. This is all one-off work though, and what generally matters the most is the execution time to run the model, as an InferenceSession instance supports multiple concurrent requests. i.e. you should re-use the InferenceSession and not create a new one for each request.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002358217/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002403886",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1002403886",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1002403886,
        "node_id": "IC_kwDOCVq1mM47v3gu",
        "user": {
            "login": "igaloly",
            "id": 38460810,
            "node_id": "MDQ6VXNlcjM4NDYwODEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/38460810?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igaloly",
            "html_url": "https://github.com/igaloly",
            "followers_url": "https://api.github.com/users/igaloly/followers",
            "following_url": "https://api.github.com/users/igaloly/following{/other_user}",
            "gists_url": "https://api.github.com/users/igaloly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igaloly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igaloly/subscriptions",
            "organizations_url": "https://api.github.com/users/igaloly/orgs",
            "repos_url": "https://api.github.com/users/igaloly/repos",
            "events_url": "https://api.github.com/users/igaloly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igaloly/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-29T05:27:27Z",
        "updated_at": "2021-12-29T05:27:27Z",
        "author_association": "NONE",
        "body": "@skottmckay \r\nThe extra details about the `InferenceSession` init are great!\r\n\r\nFrom what you said, I assume there aren't many procedures that can be made in build time, right?\r\n\r\nMoreover, I think your idea of re-using the `InferenseSession` instance is great.\r\nUnfortunately, I don't see how it is possible currently.\r\n\r\nFor example, I have a web server that returns a prediction on request.\r\nFor each request, I need to: Fetch the model -> Create an `InferenceSession` instance -> Predict.\r\nYou can see that the instance initialization happens on each request.\r\n\r\nI can cache the instance on the current server, but if my infrastructure is horizontally scaled, and because of the lack of serialization of the instance, the user might end up reaching a different server, that doesn't have the instance in memory.\r\n\r\nMoreover, if I have a lot of models, saving all of them always in memory is not efficient.\r\n\r\nI would love to hear more of your thoughts! :)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002403886/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002433984",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1002433984",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1002433984,
        "node_id": "IC_kwDOCVq1mM47v-3A",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-29T07:27:38Z",
        "updated_at": "2021-12-29T07:27:38Z",
        "author_association": "MEMBER",
        "body": "If you save the optimized model there's not much else you can do to reduce the cost of creating the inference session.\r\n\r\nIf the cost of caching all the models on all servers is too high, I'd suggest the selection of the server for a request should take into account the model involved so that you limit the number of models an individual server would need to cache. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1002433984/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1003124826",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10131#issuecomment-1003124826",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10131",
        "id": 1003124826,
        "node_id": "IC_kwDOCVq1mM47ynha",
        "user": {
            "login": "igaloly",
            "id": 38460810,
            "node_id": "MDQ6VXNlcjM4NDYwODEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/38460810?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igaloly",
            "html_url": "https://github.com/igaloly",
            "followers_url": "https://api.github.com/users/igaloly/followers",
            "following_url": "https://api.github.com/users/igaloly/following{/other_user}",
            "gists_url": "https://api.github.com/users/igaloly/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igaloly/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igaloly/subscriptions",
            "organizations_url": "https://api.github.com/users/igaloly/orgs",
            "repos_url": "https://api.github.com/users/igaloly/repos",
            "events_url": "https://api.github.com/users/igaloly/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igaloly/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-30T17:46:49Z",
        "updated_at": "2021-12-30T17:46:49Z",
        "author_association": "NONE",
        "body": "Got it.\r\nThank you for all of your help, @skottmckay \r\nI really appreciate it!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1003124826/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]