[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/813177019",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-813177019",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 813177019,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxMzE3NzAxOQ==",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-04-05T04:38:50Z",
        "updated_at": "2021-04-05T04:38:50Z",
        "author_association": "MEMBER",
        "body": "@stevenlix can you please take a look? Thx.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/813177019/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/813184403",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-813184403",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 813184403,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxMzE4NDQwMw==",
        "user": {
            "login": "stevenlix",
            "id": 38092805,
            "node_id": "MDQ6VXNlcjM4MDkyODA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/38092805?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stevenlix",
            "html_url": "https://github.com/stevenlix",
            "followers_url": "https://api.github.com/users/stevenlix/followers",
            "following_url": "https://api.github.com/users/stevenlix/following{/other_user}",
            "gists_url": "https://api.github.com/users/stevenlix/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stevenlix/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stevenlix/subscriptions",
            "organizations_url": "https://api.github.com/users/stevenlix/orgs",
            "repos_url": "https://api.github.com/users/stevenlix/repos",
            "events_url": "https://api.github.com/users/stevenlix/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stevenlix/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-04-05T05:04:17Z",
        "updated_at": "2021-04-05T05:04:17Z",
        "author_association": "MEMBER",
        "body": "TensorRT usually takes much longer time to build engine. Please add a warmup session (i.e. call sess.run once) before inference, so that engine can be built in advance. \r\nAnother thing is if the model can't run as a whole in TensorRT, you may add CUDA as a fallback, for example, sess = rt.InferenceSession(str(model_pth), opt, providers=[\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\"])",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/813184403/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/813445376",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-813445376",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 813445376,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxMzQ0NTM3Ng==",
        "user": {
            "login": "oborchers",
            "id": 26734737,
            "node_id": "MDQ6VXNlcjI2NzM0NzM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/26734737?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/oborchers",
            "html_url": "https://github.com/oborchers",
            "followers_url": "https://api.github.com/users/oborchers/followers",
            "following_url": "https://api.github.com/users/oborchers/following{/other_user}",
            "gists_url": "https://api.github.com/users/oborchers/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/oborchers/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/oborchers/subscriptions",
            "organizations_url": "https://api.github.com/users/oborchers/orgs",
            "repos_url": "https://api.github.com/users/oborchers/repos",
            "events_url": "https://api.github.com/users/oborchers/events{/privacy}",
            "received_events_url": "https://api.github.com/users/oborchers/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-04-05T15:11:25Z",
        "updated_at": "2021-04-05T15:25:48Z",
        "author_association": "NONE",
        "body": "@stevenlix : Thank you for your comments! I have gone through all of those possibilities and the results are exactly the same. Furthermore, I provided a notebook for exact reproduction of the behavior here:\r\nhttps://github.com/oborchers/Medium_Repo/blob/master/onnxruntime-issues/TensorRT%20Slow.ipynb\r\nDockerfile can be found here: https://github.com/oborchers/Medium_Repo/blob/master/onnxruntime-issues/Dockerfile\r\n\r\nThe speeds look as follows:\r\n\r\n- Torch: 95 sentences/s\r\n- CUDA + CPU: 384 sentences/s\r\n- CUDA: 370 sentences/s\r\n- Tensorrt + CUDA + CPU: 91 sentences/s\r\n- Tensorrt: 91 sentences/s\r\n\r\nAdditional Info:\r\n- Tensorrt runs have been performed with 50 warmup iterations\r\n- GPU usage via `nvidia-smi`\r\n  - Torch: fluctuates around 25%\r\n  - CUDA+CPU: fluctuates around 65-75%\r\n  -  Tensorrt: wildly varies between 0 to 30-40%, and maxing at 70% while going back to 0%. Rinse, repeat.\r\n  \r\nThis is also something I have experience half a year ago while trying this, but postponed investigation as I assumed I just did something wrong. But now that I am able replicate the exact same problem in Docker, I can safely discard a problem on the side on our master server configuration.\r\n\r\nGPUs otherwise not occupied.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/813445376/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/814619248",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-814619248",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 814619248,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxNDYxOTI0OA==",
        "user": {
            "login": "stevenlix",
            "id": 38092805,
            "node_id": "MDQ6VXNlcjM4MDkyODA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/38092805?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stevenlix",
            "html_url": "https://github.com/stevenlix",
            "followers_url": "https://api.github.com/users/stevenlix/followers",
            "following_url": "https://api.github.com/users/stevenlix/following{/other_user}",
            "gists_url": "https://api.github.com/users/stevenlix/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stevenlix/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stevenlix/subscriptions",
            "organizations_url": "https://api.github.com/users/stevenlix/orgs",
            "repos_url": "https://api.github.com/users/stevenlix/repos",
            "events_url": "https://api.github.com/users/stevenlix/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stevenlix/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-04-07T05:44:41Z",
        "updated_at": "2021-04-07T05:47:18Z",
        "author_association": "MEMBER",
        "body": "If sequence length of the model is dynamic, you may need to build an engine in warmup to cover the shape range, otherwise TensorRT will rebuild engine during inference every time when new shape inputs are coming. For example, if sequence length varies from A to B, in warmup you may call session.run once with input shape[1,A] and call another session.run with input shape[1, B], then the engine will be used for any sequence length between A and B, and will not be rebuilt during inference.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/814619248/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/816967842",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-816967842",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 816967842,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgxNjk2Nzg0Mg==",
        "user": {
            "login": "oborchers",
            "id": 26734737,
            "node_id": "MDQ6VXNlcjI2NzM0NzM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/26734737?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/oborchers",
            "html_url": "https://github.com/oborchers",
            "followers_url": "https://api.github.com/users/oborchers/followers",
            "following_url": "https://api.github.com/users/oborchers/following{/other_user}",
            "gists_url": "https://api.github.com/users/oborchers/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/oborchers/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/oborchers/subscriptions",
            "organizations_url": "https://api.github.com/users/oborchers/orgs",
            "repos_url": "https://api.github.com/users/oborchers/repos",
            "events_url": "https://api.github.com/users/oborchers/events{/privacy}",
            "received_events_url": "https://api.github.com/users/oborchers/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-04-09T20:57:37Z",
        "updated_at": "2021-04-09T20:57:37Z",
        "author_association": "NONE",
        "body": "@stevenlix:\r\n\r\nThis partially addresses the issue. I tried to do exactly that, and this has the side effect that the GPU usage now remains constantly at 85-90%. Encoding 10,000 sentences took 38s at 263 sentences/s. However, I am still nowhere near to beating base CUDA, which is residing at around 400-450 sentences/s.\r\n\r\nNext I tried to pad all inputs to the maximum length and sub-lengths. The following padding sizes result in the following performance:\r\n\r\n- 128: 90% GPU usage @ 192 sentences/s\r\n- 256: 92% GPU usage @  128 sentences/s\r\n- 512: 95% GPU usage @ 75 sentences/s\r\n\r\nI also played around with the environment variables, but this does not seem to have any effect:\r\n```\r\nexport ORT_TENSORRT_MAX_BATCH_SIZE=1\r\nexport ORT_TENSORRT_MAX_WORKSPACE_SIZE=8_589_934_592\r\nexport ORT_TENSORRT_MAX_PARTITION_ITERATIONS=40\r\nexport ORT_TENSORRT_MIN_SUBGRAPH_SIZE=1\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/816967842/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/835188828",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-835188828",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 835188828,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgzNTE4ODgyOA==",
        "user": {
            "login": "mudong0419",
            "id": 24379054,
            "node_id": "MDQ6VXNlcjI0Mzc5MDU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/24379054?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mudong0419",
            "html_url": "https://github.com/mudong0419",
            "followers_url": "https://api.github.com/users/mudong0419/followers",
            "following_url": "https://api.github.com/users/mudong0419/following{/other_user}",
            "gists_url": "https://api.github.com/users/mudong0419/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mudong0419/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mudong0419/subscriptions",
            "organizations_url": "https://api.github.com/users/mudong0419/orgs",
            "repos_url": "https://api.github.com/users/mudong0419/repos",
            "events_url": "https://api.github.com/users/mudong0419/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mudong0419/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-05-08T07:57:17Z",
        "updated_at": "2021-05-08T07:57:17Z",
        "author_association": "NONE",
        "body": "I have the same problem. When inference with Bert(HuggingFace), TensorrtExecutionProvider is slower than CUDAExecutionProvider.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/835188828/reactions",
            "total_count": 4,
            "+1": 4,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/990663354",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-990663354",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 990663354,
        "node_id": "IC_kwDOCVq1mM47DFK6",
        "user": {
            "login": "LeeJiangWei",
            "id": 49602584,
            "node_id": "MDQ6VXNlcjQ5NjAyNTg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/49602584?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LeeJiangWei",
            "html_url": "https://github.com/LeeJiangWei",
            "followers_url": "https://api.github.com/users/LeeJiangWei/followers",
            "following_url": "https://api.github.com/users/LeeJiangWei/following{/other_user}",
            "gists_url": "https://api.github.com/users/LeeJiangWei/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LeeJiangWei/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LeeJiangWei/subscriptions",
            "organizations_url": "https://api.github.com/users/LeeJiangWei/orgs",
            "repos_url": "https://api.github.com/users/LeeJiangWei/repos",
            "events_url": "https://api.github.com/users/LeeJiangWei/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LeeJiangWei/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-10T06:51:45Z",
        "updated_at": "2021-12-10T07:28:29Z",
        "author_association": "NONE",
        "body": "> I have the same problem. When inference with Bert(HuggingFace), TensorrtExecutionProvider is slower than CUDAExecutionProvider.\r\n\r\nSame here using HugginceFace models. It's a warm-up issue as @stevenlix said, and it will be much faster the second time you call sess.run().",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/990663354/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100843586",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7230#issuecomment-1100843586",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7230",
        "id": 1100843586,
        "node_id": "IC_kwDOCVq1mM5BnYpC",
        "user": {
            "login": "stale[bot]",
            "id": 26384082,
            "node_id": "MDM6Qm90MjYzODQwODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/1724?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stale%5Bbot%5D",
            "html_url": "https://github.com/apps/stale",
            "followers_url": "https://api.github.com/users/stale%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/stale%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/stale%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stale%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stale%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/stale%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/stale%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/stale%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stale%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "created_at": "2022-04-17T09:53:59Z",
        "updated_at": "2022-04-17T09:53:59Z",
        "author_association": "NONE",
        "body": "This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100843586/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]