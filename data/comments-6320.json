[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/772993538",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6320#issuecomment-772993538",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6320",
        "id": 772993538,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc3Mjk5MzUzOA==",
        "user": {
            "login": "tracysh",
            "id": 42477615,
            "node_id": "MDQ6VXNlcjQyNDc3NjE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42477615?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tracysh",
            "html_url": "https://github.com/tracysh",
            "followers_url": "https://api.github.com/users/tracysh/followers",
            "following_url": "https://api.github.com/users/tracysh/following{/other_user}",
            "gists_url": "https://api.github.com/users/tracysh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tracysh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tracysh/subscriptions",
            "organizations_url": "https://api.github.com/users/tracysh/orgs",
            "repos_url": "https://api.github.com/users/tracysh/repos",
            "events_url": "https://api.github.com/users/tracysh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tracysh/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-04T03:09:52Z",
        "updated_at": "2021-02-04T03:09:52Z",
        "author_association": "CONTRIBUTOR",
        "body": "@xzhu1900: Sorry for the delayed response. ONNX Runtime recently added support for Gemm<double> (#6223). Not all combinations of types supported by ONNX are available in the runtime in order to keep code size down. You could try using the nightly build (https://test.pypi.org/project/ort-nightly/) if you'd like to try this out before the next release of the runtime.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/772993538/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/773151976",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6320#issuecomment-773151976",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6320",
        "id": 773151976,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc3MzE1MTk3Ng==",
        "user": {
            "login": "jbissantz",
            "id": 56394934,
            "node_id": "MDQ6VXNlcjU2Mzk0OTM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/56394934?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jbissantz",
            "html_url": "https://github.com/jbissantz",
            "followers_url": "https://api.github.com/users/jbissantz/followers",
            "following_url": "https://api.github.com/users/jbissantz/following{/other_user}",
            "gists_url": "https://api.github.com/users/jbissantz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jbissantz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jbissantz/subscriptions",
            "organizations_url": "https://api.github.com/users/jbissantz/orgs",
            "repos_url": "https://api.github.com/users/jbissantz/repos",
            "events_url": "https://api.github.com/users/jbissantz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jbissantz/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-04T09:10:00Z",
        "updated_at": "2021-02-04T09:10:00Z",
        "author_association": "NONE",
        "body": "Thanks for fixing this issue. The example code above runs now, using the nightly build.\r\nWhile doing some testing I noticed that the onnxruntime_test script doesn't work, I used the python script located in tools instead:\r\n```bash\r\n(ort): $ onnxruntime_test linear.onnx 1000\r\nTraceback (most recent call last):\r\n  File \"/usr/local/anaconda3/envs/ort/bin/onnxruntime_test\", line 5, in <module>\r\n    from onnxruntime.tools.onnxruntime_test import main\r\nImportError: cannot import name 'main'\r\n\r\n(ort): $ python /usr/local/anaconda3/envs/ort/lib/python3.6/site-packages/onnxruntime/tools/onnxruntime_test.py linear.onnx 10000\r\nmodel: torch-jit-export\r\nversion: 9223372036854775807\r\niterations: 10000\r\navg latency: 0.013897672599705401 ms\r\n```\r\nI have an additional question:\r\nBecause I'd like to use the onnxruntime C++-API to use it during runtime of CFD simulations, I was wondering which library files were shipped with pip. I used ```pip show .-f ort-nightly``` to examine all files included in the package and was surprised that I didn't find any *so and *dylib files. So what backend libraries are actually used in the python API and where are they located? Is there a repository to download the nightly binaries to be able to download and link them using cmake?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/773151976/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/774737604",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6320#issuecomment-774737604",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6320",
        "id": 774737604,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc3NDczNzYwNA==",
        "user": {
            "login": "jbissantz",
            "id": 56394934,
            "node_id": "MDQ6VXNlcjU2Mzk0OTM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/56394934?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jbissantz",
            "html_url": "https://github.com/jbissantz",
            "followers_url": "https://api.github.com/users/jbissantz/followers",
            "following_url": "https://api.github.com/users/jbissantz/following{/other_user}",
            "gists_url": "https://api.github.com/users/jbissantz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jbissantz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jbissantz/subscriptions",
            "organizations_url": "https://api.github.com/users/jbissantz/orgs",
            "repos_url": "https://api.github.com/users/jbissantz/repos",
            "events_url": "https://api.github.com/users/jbissantz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jbissantz/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-07T19:09:09Z",
        "updated_at": "2021-02-07T19:10:21Z",
        "author_association": "NONE",
        "body": "I ran some additional more sophisticated tests and encountered additional errors.\r\nDue to some background optimization the Gemm operation is fused with the ReLU activation function and results in the FusedGemm operator which also does not support double precision. Could you let me know how I could disable this optimization or add double precision to the FusedGemm operator. \r\n\r\nSkript to reproduce the error. The only difference to the original test script is the PyTorch model I am using.\r\n```python\r\nimport onnxruntime as ort\r\nimport torch\r\n\r\nclass FlexMLP(torch.nn.Module):\r\n      def __init__(self, n_inp, n_out, n_hidden_neurons=[32, 32], activation_fn=torch.nn.ReLU)\r\n          super().__init__()\r\n\r\n          self.n_inp = n_inp\r\n          self.n_out = n_out\r\n          self.n_neurons = [n_inp] + n_hidden_neurons + [n_out]\r\n          self.hidden_layers = len(n_hidden_neurons)\r\n          self.layers = torch.nn.ModuleList()\r\n          self.activation = activation_fn\r\n\r\n          # construct the network\r\n          for i in range(len(self.n_neurons)-2):\r\n              self.layers.append(torch.nn.Linear(self.n_neurons[i], self.n_neurons[i+1]))\r\n              self.layers.append(self.activation())\r\n          # add the last layer\r\n          self.layers.append(nn.Linear(self.n_neurons[-2], self.n_neurons[-1])\r\n\r\n      def forward(self, x):\r\n          for layer in self.layers:\r\n              x = layer(x)\r\n          return x\r\n\r\n# create simple multi layer perceptron\r\nmodel = FlexMLP(1,1,[32]*3).double()\r\nx = torch.ones(10, 1, dtype=torch.double)\r\n\r\n# export model to onnx\r\ntorch.onnx.export(model,                           # model being run\r\n                   x,                                          # model input (or a tuple for multiple inputs)\r\n                   \"FlexMLP.onnx\",                    # where to save the model (can be a file or file-like object)\r\n                   export_params=True,        # store the trained parameter weights inside the model file\r\n                   opset_version=10,              # the ONNX version to export the model to\r\n                   do_constant_folding=True,  # whether to execute constant folding for optimization\r\n                   input_names = ['input'],    # the model's input names\r\n                   output_names = ['output'], # the model's output names\r\n                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\r\n                                 'output' : {0 : 'batch_size'}})\r\n                                 \r\n# now running the model in ort\r\nimport onnxruntime\r\n\r\nort_session = onnxruntime.InferenceSession(\"FlexMLP.onnx\")\r\n\r\ndef to_numpy(tensor):\r\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\r\n\r\n# compute ONNX Runtime output prediction\r\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\r\nort_outs = ort_session.run(None, ort_inputs)\r\n```\r\n\r\nError code:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 74, in <module>\r\n    ort_session = onnxruntime.InferenceSession(\"linear.onnx\")\r\n  File \"/usr/local/anaconda3/envs/ort/lib/python3.6/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 280, in __init__\r\n    self._create_inference_session(providers, provider_options)\r\n  File \"/usr/local/anaconda3/envs/ort/lib/python3.6/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 312, in _create_inference_session\r\n    sess.initialize_session(providers, provider_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Failed to find kernel for FusedGemm(1) (node fused ). Op with name (fused ) and type (FusedGemm) kernel is not supported in CPUExecutionProvider. Encountered following errors: (Found kernel for Op with name (fused ) and type (FusedGemm) in the supported version range (node_version: 1 kernel start version: 1 kernel_end_version: 2147483647). However the types are incompatible. This op has been implemented only for the following types (tensor(float),), but the node in the model has the following type (tensor(double))\r\n)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/774737604/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]