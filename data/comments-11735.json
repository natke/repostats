[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1146511346",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11735#issuecomment-1146511346",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11735",
        "id": 1146511346,
        "node_id": "IC_kwDOCVq1mM5EVl_y",
        "user": {
            "login": "hanhaowen",
            "id": 70024226,
            "node_id": "MDQ6VXNlcjcwMDI0MjI2",
            "avatar_url": "https://avatars.githubusercontent.com/u/70024226?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hanhaowen",
            "html_url": "https://github.com/hanhaowen",
            "followers_url": "https://api.github.com/users/hanhaowen/followers",
            "following_url": "https://api.github.com/users/hanhaowen/following{/other_user}",
            "gists_url": "https://api.github.com/users/hanhaowen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hanhaowen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hanhaowen/subscriptions",
            "organizations_url": "https://api.github.com/users/hanhaowen/orgs",
            "repos_url": "https://api.github.com/users/hanhaowen/repos",
            "events_url": "https://api.github.com/users/hanhaowen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hanhaowen/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-04T03:09:06Z",
        "updated_at": "2022-06-04T03:09:06Z",
        "author_association": "NONE",
        "body": "what's more, if the batch size of the data I put into the model is too big, how can I do I prevent the program to crash, and then reduce the batch size and put into the model again to get the result?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1146511346/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1147656533",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11735#issuecomment-1147656533",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11735",
        "id": 1147656533,
        "node_id": "IC_kwDOCVq1mM5EZ9lV",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-06T16:44:32Z",
        "updated_at": "2022-06-06T16:56:17Z",
        "author_association": "MEMBER",
        "body": "You can do some tests to how much memory is needed for different batch size. In that way, you know the limit (max batch size) in advance given a GPU.\r\n\r\nExample of measuring GPU memory usage with python:\r\nhttps://github.com/microsoft/onnxruntime/blob/981d45d8d54b0c085a9e424366c99b42f4514c67/onnxruntime/python/tools/transformers/benchmark_helper.py#L420\r\nBasically, it uses py3nvml package to get GPU memory status. I think you can also use NVML C API to get the remaining (free) GPU memory:\r\n\r\n```\r\nnvmlReturn_t DECLDIR nvmlDeviceGetMemoryInfo(nvmlDevice_t device, nvmlMemory_t *memory);\r\ntypedef struct nvmlMemory_st {\r\n    unsigned long long total;\r\n    unsigned long long free;\r\n    unsigned long long used;\r\n} nvmlMemory_t;\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1147656533/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1148265160",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11735#issuecomment-1148265160",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11735",
        "id": 1148265160,
        "node_id": "IC_kwDOCVq1mM5EcSLI",
        "user": {
            "login": "hanhaowen",
            "id": 70024226,
            "node_id": "MDQ6VXNlcjcwMDI0MjI2",
            "avatar_url": "https://avatars.githubusercontent.com/u/70024226?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hanhaowen",
            "html_url": "https://github.com/hanhaowen",
            "followers_url": "https://api.github.com/users/hanhaowen/followers",
            "following_url": "https://api.github.com/users/hanhaowen/following{/other_user}",
            "gists_url": "https://api.github.com/users/hanhaowen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hanhaowen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hanhaowen/subscriptions",
            "organizations_url": "https://api.github.com/users/hanhaowen/orgs",
            "repos_url": "https://api.github.com/users/hanhaowen/repos",
            "events_url": "https://api.github.com/users/hanhaowen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hanhaowen/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-07T06:50:23Z",
        "updated_at": "2022-06-07T06:50:23Z",
        "author_association": "NONE",
        "body": "> You can do some tests to how much memory is needed for different batch size. In that way, you know the limit (max batch size) in advance given a GPU.\r\n> \r\n> Example of measuring GPU memory usage with python:\r\n> \r\n> https://github.com/microsoft/onnxruntime/blob/981d45d8d54b0c085a9e424366c99b42f4514c67/onnxruntime/python/tools/transformers/benchmark_helper.py#L420\r\n> \r\n> \r\n> Basically, it uses py3nvml package to get GPU memory status. I think you can also use NVML C API to get the remaining (free) GPU memory:\r\n> ```\r\n> nvmlReturn_t DECLDIR nvmlDeviceGetMemoryInfo(nvmlDevice_t device, nvmlMemory_t *memory);\r\n> typedef struct nvmlMemory_st {\r\n>     unsigned long long total;\r\n>     unsigned long long free;\r\n>     unsigned long long used;\r\n> } nvmlMemory_t;\r\n> ```\r\n\r\ngot it, thank you.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1148265160/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1152607102",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11735#issuecomment-1152607102",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11735",
        "id": 1152607102,
        "node_id": "IC_kwDOCVq1mM5Es2N-",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-10T18:01:38Z",
        "updated_at": "2022-06-10T18:04:20Z",
        "author_association": "MEMBER",
        "body": "Onnxruntime caches GPU memory internally and re-uses it. To arrive at the true memory allocation dynamics on GPU, you may want to disable Arena Allocator (at the expense of the performance) and see what it shows.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1152607102/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]