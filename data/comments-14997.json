[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1464109287",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1464109287",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1464109287,
        "node_id": "IC_kwDOCVq1mM5XRIjn",
        "user": {
            "login": "jchen351",
            "id": 73297588,
            "node_id": "MDQ6VXNlcjczMjk3NTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/73297588?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jchen351",
            "html_url": "https://github.com/jchen351",
            "followers_url": "https://api.github.com/users/jchen351/followers",
            "following_url": "https://api.github.com/users/jchen351/following{/other_user}",
            "gists_url": "https://api.github.com/users/jchen351/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jchen351/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jchen351/subscriptions",
            "organizations_url": "https://api.github.com/users/jchen351/orgs",
            "repos_url": "https://api.github.com/users/jchen351/repos",
            "events_url": "https://api.github.com/users/jchen351/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jchen351/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-10T17:07:54Z",
        "updated_at": "2023-03-10T17:07:54Z",
        "author_association": "MEMBER",
        "body": "Hi, Pauldog thanks for reaching out. We have received your message and put these requests under consideration! \n\nThank you for your time,\n\nJian Chen (not a A.I.) ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1464109287/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1464230317",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1464230317",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1464230317,
        "node_id": "IC_kwDOCVq1mM5XRmGt",
        "user": {
            "login": "jchen351",
            "id": 73297588,
            "node_id": "MDQ6VXNlcjczMjk3NTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/73297588?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jchen351",
            "html_url": "https://github.com/jchen351",
            "followers_url": "https://api.github.com/users/jchen351/followers",
            "following_url": "https://api.github.com/users/jchen351/following{/other_user}",
            "gists_url": "https://api.github.com/users/jchen351/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jchen351/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jchen351/subscriptions",
            "organizations_url": "https://api.github.com/users/jchen351/orgs",
            "repos_url": "https://api.github.com/users/jchen351/repos",
            "events_url": "https://api.github.com/users/jchen351/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jchen351/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-10T18:38:28Z",
        "updated_at": "2023-03-10T18:38:28Z",
        "author_association": "MEMBER",
        "body": "Also Could you please provide me more information about your scenarios, like: hardware to you wants to run on, and models you are interested in? Again, out currently priority is on fp16 support. And there isn't any hardware we have that supports the 4bit or lower. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1464230317/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1464369969",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1464369969",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1464369969,
        "node_id": "IC_kwDOCVq1mM5XSIMx",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-10T20:18:50Z",
        "updated_at": "2023-03-10T20:25:31Z",
        "author_association": "NONE",
        "body": "Sure here is a very recent example of a practical use case:\r\n\r\n[Llama 4bit](https://github.com/qwopqwop200/GPTQ-for-LLaMa/)\r\n\r\nAs far as I'm aware it doesn't require 4bit hardware it simply stores the weights on the GPU in 4bit, then uses GPU cores at runtime to convert them to int8 or float16 at runtime to do the calculations. \r\n\r\nThe main benefit is the ability to run larger models on the same hardware. \r\n\r\nUse cases would be \r\n\r\n- Running very large language models on consumer hardware\r\n- Running large models on mobile hardware\r\n\r\nHere are some papers\r\n\r\nhttps://arxiv.org/abs/1810.05723\r\nhttps://arxiv.org/abs/2202.05292\r\n\r\nand articles\r\nhttps://karanbirchahal.medium.com/aggressive-quantization-how-to-run-mnist-on-a-4-bit-neural-net-using-pytorch-5703f3faa599\r\n\r\nNow, I don't know whether onnxruntime already can support this or not? Since technically say a 4bit quantized model would presumably appear like an 8bit quantized model as two 4bits are combined into one 8bit.\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1464369969/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467300564",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1467300564",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1467300564,
        "node_id": "IC_kwDOCVq1mM5XdTrU",
        "user": {
            "login": "josephrocca",
            "id": 1167575,
            "node_id": "MDQ6VXNlcjExNjc1NzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1167575?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/josephrocca",
            "html_url": "https://github.com/josephrocca",
            "followers_url": "https://api.github.com/users/josephrocca/followers",
            "following_url": "https://api.github.com/users/josephrocca/following{/other_user}",
            "gists_url": "https://api.github.com/users/josephrocca/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/josephrocca/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/josephrocca/subscriptions",
            "organizations_url": "https://api.github.com/users/josephrocca/orgs",
            "repos_url": "https://api.github.com/users/josephrocca/repos",
            "events_url": "https://api.github.com/users/josephrocca/events{/privacy}",
            "received_events_url": "https://api.github.com/users/josephrocca/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T03:36:02Z",
        "updated_at": "2023-03-14T03:36:02Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hey @jchen351, I'm wondering why this is closed? Shouldn't it stay open if this is being considered?\r\n\r\nThe WebML ecosystem in particular could really do with a 4-bit quantization solution, since model size is such an important factor on the web.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467300564/reactions",
            "total_count": 3,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 3,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467307538",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1467307538",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1467307538,
        "node_id": "IC_kwDOCVq1mM5XdVYS",
        "user": {
            "login": "xenova",
            "id": 26504141,
            "node_id": "MDQ6VXNlcjI2NTA0MTQx",
            "avatar_url": "https://avatars.githubusercontent.com/u/26504141?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xenova",
            "html_url": "https://github.com/xenova",
            "followers_url": "https://api.github.com/users/xenova/followers",
            "following_url": "https://api.github.com/users/xenova/following{/other_user}",
            "gists_url": "https://api.github.com/users/xenova/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xenova/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xenova/subscriptions",
            "organizations_url": "https://api.github.com/users/xenova/orgs",
            "repos_url": "https://api.github.com/users/xenova/repos",
            "events_url": "https://api.github.com/users/xenova/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xenova/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T03:47:18Z",
        "updated_at": "2023-03-14T03:47:18Z",
        "author_association": "NONE",
        "body": "100% agree with @josephrocca. 4-bit quantization would be massive for my [Transformers.js](https://github.com/xenova/transformers.js) library (and other WebML libraries)!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467307538/reactions",
            "total_count": 3,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 3,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468843489",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1468843489",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1468843489,
        "node_id": "IC_kwDOCVq1mM5XjMXh",
        "user": {
            "login": "jchen351",
            "id": 73297588,
            "node_id": "MDQ6VXNlcjczMjk3NTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/73297588?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jchen351",
            "html_url": "https://github.com/jchen351",
            "followers_url": "https://api.github.com/users/jchen351/followers",
            "following_url": "https://api.github.com/users/jchen351/following{/other_user}",
            "gists_url": "https://api.github.com/users/jchen351/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jchen351/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jchen351/subscriptions",
            "organizations_url": "https://api.github.com/users/jchen351/orgs",
            "repos_url": "https://api.github.com/users/jchen351/repos",
            "events_url": "https://api.github.com/users/jchen351/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jchen351/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T21:04:46Z",
        "updated_at": "2023-03-14T21:05:51Z",
        "author_association": "MEMBER",
        "body": "@xenova @josephrocca The only hardware we know that can support 4 bit quantization with performance gain is Nvidia A100, but we cannot get our hands on enough A100,  and the newer H100 has dropped that support.  We don't foresee any performance gain in doing 4 bit quantization on any other popular hardwares. \r\nSo, until them, I will keep this closed :) ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468843489/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468879680",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1468879680",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1468879680,
        "node_id": "IC_kwDOCVq1mM5XjVNA",
        "user": {
            "login": "xenova",
            "id": 26504141,
            "node_id": "MDQ6VXNlcjI2NTA0MTQx",
            "avatar_url": "https://avatars.githubusercontent.com/u/26504141?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xenova",
            "html_url": "https://github.com/xenova",
            "followers_url": "https://api.github.com/users/xenova/followers",
            "following_url": "https://api.github.com/users/xenova/following{/other_user}",
            "gists_url": "https://api.github.com/users/xenova/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xenova/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xenova/subscriptions",
            "organizations_url": "https://api.github.com/users/xenova/orgs",
            "repos_url": "https://api.github.com/users/xenova/repos",
            "events_url": "https://api.github.com/users/xenova/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xenova/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T21:40:11Z",
        "updated_at": "2023-03-14T21:42:06Z",
        "author_association": "NONE",
        "body": "This repo supports 4-bit quantization: https://github.com/ggerganov/llama.cpp\r\n(And, as stated in the [README](https://github.com/ggerganov/llama.cpp#description), it runs on the CPU)\r\n\r\nAlso, considering that WASM uses a 32-bit address space (i.e., max 4GB), the only real way to get large models running on consumer hardware is quantization.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468879680/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1469157476",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1469157476",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1469157476,
        "node_id": "IC_kwDOCVq1mM5XkZBk",
        "user": {
            "login": "josephrocca",
            "id": 1167575,
            "node_id": "MDQ6VXNlcjExNjc1NzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1167575?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/josephrocca",
            "html_url": "https://github.com/josephrocca",
            "followers_url": "https://api.github.com/users/josephrocca/followers",
            "following_url": "https://api.github.com/users/josephrocca/following{/other_user}",
            "gists_url": "https://api.github.com/users/josephrocca/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/josephrocca/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/josephrocca/subscriptions",
            "organizations_url": "https://api.github.com/users/josephrocca/orgs",
            "repos_url": "https://api.github.com/users/josephrocca/repos",
            "events_url": "https://api.github.com/users/josephrocca/events{/privacy}",
            "received_events_url": "https://api.github.com/users/josephrocca/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-15T01:45:52Z",
        "updated_at": "2023-03-15T01:46:32Z",
        "author_association": "CONTRIBUTOR",
        "body": "@jchen351, yes, as xenova pointed out, this is more about running large models on hardware that has a small amount of memory, rather than performance improvements.\r\n\r\nFor example, please see this demo of llama 7B running on a pixel 5 at 1 token/sec using 4 bit quantization: https://twitter.com/ggerganov/status/1635605532726681600\r\n\r\nSo this issue can probably be re-opened considering it is viable to gain this benefit without hardware support? [llama.cpp](https://github.com/ggerganov/llama.cpp) has grown faster than the original stable diffusion repo (which was one of the fastest growing of all time) because it allows people to run big models on small hardware -- there's definitely demand for this! :)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1469157476/reactions",
            "total_count": 8,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 4,
            "confused": 0,
            "heart": 0,
            "rocket": 4,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1519703121",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1519703121",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1519703121,
        "node_id": "IC_kwDOCVq1mM5alNRR",
        "user": {
            "login": "skyne98",
            "id": 4999658,
            "node_id": "MDQ6VXNlcjQ5OTk2NTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4999658?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skyne98",
            "html_url": "https://github.com/skyne98",
            "followers_url": "https://api.github.com/users/skyne98/followers",
            "following_url": "https://api.github.com/users/skyne98/following{/other_user}",
            "gists_url": "https://api.github.com/users/skyne98/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skyne98/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skyne98/subscriptions",
            "organizations_url": "https://api.github.com/users/skyne98/orgs",
            "repos_url": "https://api.github.com/users/skyne98/repos",
            "events_url": "https://api.github.com/users/skyne98/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skyne98/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-24T09:19:42Z",
        "updated_at": "2023-04-24T09:19:42Z",
        "author_association": "NONE",
        "body": "@jchen351, can we have a second look at this? It's not really about performance, but rather allowing running models in places they couldn't before. I insist!\n\nIt just seems like the points that guys made, which are really valid, got seemingly plainly ignored.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1519703121/reactions",
            "total_count": 5,
            "+1": 5,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1573025829",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1573025829",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1573025829,
        "node_id": "IC_kwDOCVq1mM5dwngl",
        "user": {
            "login": "tikikun",
            "id": 22268502,
            "node_id": "MDQ6VXNlcjIyMjY4NTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/22268502?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tikikun",
            "html_url": "https://github.com/tikikun",
            "followers_url": "https://api.github.com/users/tikikun/followers",
            "following_url": "https://api.github.com/users/tikikun/following{/other_user}",
            "gists_url": "https://api.github.com/users/tikikun/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tikikun/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tikikun/subscriptions",
            "organizations_url": "https://api.github.com/users/tikikun/orgs",
            "repos_url": "https://api.github.com/users/tikikun/repos",
            "events_url": "https://api.github.com/users/tikikun/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tikikun/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-02T01:59:59Z",
        "updated_at": "2023-06-02T01:59:59Z",
        "author_association": "NONE",
        "body": "Re-open please, everyone is using 4-5bit quantization now",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1573025829/reactions",
            "total_count": 5,
            "+1": 5,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1624241961",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1624241961",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1624241961,
        "node_id": "IC_kwDOCVq1mM5gz_cp",
        "user": {
            "login": "jywu-msft",
            "id": 43355415,
            "node_id": "MDQ6VXNlcjQzMzU1NDE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/43355415?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jywu-msft",
            "html_url": "https://github.com/jywu-msft",
            "followers_url": "https://api.github.com/users/jywu-msft/followers",
            "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}",
            "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions",
            "organizations_url": "https://api.github.com/users/jywu-msft/orgs",
            "repos_url": "https://api.github.com/users/jywu-msft/repos",
            "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jywu-msft/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-06T20:02:50Z",
        "updated_at": "2023-07-06T20:02:50Z",
        "author_association": "MEMBER",
        "body": "re-opening this. this should not be closed. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1624241961/reactions",
            "total_count": 4,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 3,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1624242927",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1624242927",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1624242927,
        "node_id": "IC_kwDOCVq1mM5gz_rv",
        "user": {
            "login": "jywu-msft",
            "id": 43355415,
            "node_id": "MDQ6VXNlcjQzMzU1NDE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/43355415?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jywu-msft",
            "html_url": "https://github.com/jywu-msft",
            "followers_url": "https://api.github.com/users/jywu-msft/followers",
            "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}",
            "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions",
            "organizations_url": "https://api.github.com/users/jywu-msft/orgs",
            "repos_url": "https://api.github.com/users/jywu-msft/repos",
            "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jywu-msft/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-06T20:03:42Z",
        "updated_at": "2023-07-06T20:03:42Z",
        "author_association": "MEMBER",
        "body": "+@yufenglee FYI",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1624242927/reactions",
            "total_count": 2,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 1,
            "confused": 0,
            "heart": 0,
            "rocket": 1,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1631729280",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1631729280",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1631729280,
        "node_id": "IC_kwDOCVq1mM5hQjaA",
        "user": {
            "login": "ThisisBillhe",
            "id": 56794062,
            "node_id": "MDQ6VXNlcjU2Nzk0MDYy",
            "avatar_url": "https://avatars.githubusercontent.com/u/56794062?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ThisisBillhe",
            "html_url": "https://github.com/ThisisBillhe",
            "followers_url": "https://api.github.com/users/ThisisBillhe/followers",
            "following_url": "https://api.github.com/users/ThisisBillhe/following{/other_user}",
            "gists_url": "https://api.github.com/users/ThisisBillhe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ThisisBillhe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ThisisBillhe/subscriptions",
            "organizations_url": "https://api.github.com/users/ThisisBillhe/orgs",
            "repos_url": "https://api.github.com/users/ThisisBillhe/repos",
            "events_url": "https://api.github.com/users/ThisisBillhe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ThisisBillhe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-12T01:48:44Z",
        "updated_at": "2023-07-12T01:48:44Z",
        "author_association": "NONE",
        "body": "Hi everyone! I have successfully quantized a diffusion model to 2-bit and manually packed them into uint8 format (store 4x 2-bit weight in an uint8 variable) in pytorch. During inference, they are unpacked to float format for calculation. In this way, the model size has been reduced from 1545M to 150M, and the VRAM for loading the model is also greatly reduced (from 2500M to 1000M) in pytorch. However, when I export the model to onnx, only the model size is reduced (to around 190M), the VRAM for loading the model can still reach 3000M.  I guess the uint8 parameters are cast to int32 or float32 during loading the onnx model.\r\n\r\nAny ideas on how to lower the VRAM for loading this ONNX model? I have upload the model at [googledrive](https://drive.google.com/file/d/18L8abpiQ2OK2svDSahWHgsUYa2yAtuxh/view).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1631729280/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1631741015",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1631741015",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1631741015,
        "node_id": "IC_kwDOCVq1mM5hQmRX",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-12T02:06:50Z",
        "updated_at": "2023-07-12T02:06:50Z",
        "author_association": "NONE",
        "body": "> Hi everyone! I have successfully quantized a diffusion model to 2-bit and manually packed them into uint8 format (store 4x 2-bit weight in an uint8 variable) in pytorch. During inference, they are unpacked to float format for calculation. In this way, the model size has been reduced from 1545M to 150M, and the VRAM for loading the model is also greatly reduced (from 2500M to 1000M) in pytorch. However, when I export the model to onnx, only the model size is reduced (to around 190M), the VRAM for loading the model can still reach 3000M. I guess the uint8 parameters are cast to int32 or float32 during loading the onnx model.\r\n> \r\n> Any ideas on how to lower the VRAM for loading this ONNX model? I have upload the model at [googledrive](https://drive.google.com/file/d/18L8abpiQ2OK2svDSahWHgsUYa2yAtuxh/view).\r\n\r\n2-bit diffusion model? Does it actually produce images?\r\n\r\nGuess you could try packing 16 2-bits into an int32. \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1631741015/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1631750166",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14997#issuecomment-1631750166",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14997",
        "id": 1631750166,
        "node_id": "IC_kwDOCVq1mM5hQogW",
        "user": {
            "login": "ThisisBillhe",
            "id": 56794062,
            "node_id": "MDQ6VXNlcjU2Nzk0MDYy",
            "avatar_url": "https://avatars.githubusercontent.com/u/56794062?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ThisisBillhe",
            "html_url": "https://github.com/ThisisBillhe",
            "followers_url": "https://api.github.com/users/ThisisBillhe/followers",
            "following_url": "https://api.github.com/users/ThisisBillhe/following{/other_user}",
            "gists_url": "https://api.github.com/users/ThisisBillhe/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ThisisBillhe/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ThisisBillhe/subscriptions",
            "organizations_url": "https://api.github.com/users/ThisisBillhe/orgs",
            "repos_url": "https://api.github.com/users/ThisisBillhe/repos",
            "events_url": "https://api.github.com/users/ThisisBillhe/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ThisisBillhe/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-12T02:19:50Z",
        "updated_at": "2023-07-12T02:19:50Z",
        "author_association": "NONE",
        "body": "> > Hi everyone! I have successfully quantized a diffusion model to 2-bit and manually packed them into uint8 format (store 4x 2-bit weight in an uint8 variable) in pytorch. During inference, they are unpacked to float format for calculation. In this way, the model size has been reduced from 1545M to 150M, and the VRAM for loading the model is also greatly reduced (from 2500M to 1000M) in pytorch. However, when I export the model to onnx, only the model size is reduced (to around 190M), the VRAM for loading the model can still reach 3000M. I guess the uint8 parameters are cast to int32 or float32 during loading the onnx model.\r\n> > Any ideas on how to lower the VRAM for loading this ONNX model? I have upload the model at [googledrive](https://drive.google.com/file/d/18L8abpiQ2OK2svDSahWHgsUYa2yAtuxh/view).\r\n> \r\n> 2-bit diffusion model? Does it actually produce images?\r\n> \r\n> Guess you could try packing 16 2-bits into an int32.\r\n\r\nThe work is in progress..I guess you make a point, I will have a try.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1631750166/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]