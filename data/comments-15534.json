[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1512156743",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15534#issuecomment-1512156743",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15534",
        "id": 1512156743,
        "node_id": "IC_kwDOCVq1mM5aIa5H",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-17T22:15:19Z",
        "updated_at": "2023-04-17T22:24:22Z",
        "author_association": "MEMBER",
        "body": "FP16 accelerations need either of the following two:\r\n(1) GPUs like P100, V100, T4, A100 etc which has FP16 TFLOPS higher than FP32 TFLOPS, and your model has majority computation on MatMul, Gemm, Conv etc.\r\n(2) Model is I/O bound so that using FP16 inputs/outputs could speed up I/O. This depends on GPU memory bandwidth, input and output size, and compute latency.\r\n\r\nPlease try optimize FP32 model first, then convert the optimized model to FP16. Otherwise, some optimizations might not be applied to FP16 model. \r\n\r\nOne way to do that is a session option to save optimized model like:\r\nhttps://github.com/microsoft/onnxruntime/blob/a30b57da6e1d985a5d6ecf433206c212cc469f8c/onnxruntime/python/tools/transformers/optimizer.py#L107\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1512156743/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1514639384",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15534#issuecomment-1514639384",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15534",
        "id": 1514639384,
        "node_id": "IC_kwDOCVq1mM5aR5AY",
        "user": {
            "login": "lucasjinreal",
            "id": 21303438,
            "node_id": "MDQ6VXNlcjIxMzAzNDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21303438?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucasjinreal",
            "html_url": "https://github.com/lucasjinreal",
            "followers_url": "https://api.github.com/users/lucasjinreal/followers",
            "following_url": "https://api.github.com/users/lucasjinreal/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucasjinreal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucasjinreal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucasjinreal/subscriptions",
            "organizations_url": "https://api.github.com/users/lucasjinreal/orgs",
            "repos_url": "https://api.github.com/users/lucasjinreal/repos",
            "events_url": "https://api.github.com/users/lucasjinreal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucasjinreal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-19T12:22:07Z",
        "updated_at": "2023-04-19T12:22:07Z",
        "author_association": "NONE",
        "body": "@tianleiwu No at all, my rtx2060 didn't get speedup either, even worse, fp16 slower than fp32 my GPU has tensorcores and SM for fp16 calculation.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1514639384/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1517197692",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15534#issuecomment-1517197692",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15534",
        "id": 1517197692,
        "node_id": "IC_kwDOCVq1mM5abpl8",
        "user": {
            "login": "yeliang2258",
            "id": 30516196,
            "node_id": "MDQ6VXNlcjMwNTE2MTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/30516196?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yeliang2258",
            "html_url": "https://github.com/yeliang2258",
            "followers_url": "https://api.github.com/users/yeliang2258/followers",
            "following_url": "https://api.github.com/users/yeliang2258/following{/other_user}",
            "gists_url": "https://api.github.com/users/yeliang2258/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yeliang2258/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yeliang2258/subscriptions",
            "organizations_url": "https://api.github.com/users/yeliang2258/orgs",
            "repos_url": "https://api.github.com/users/yeliang2258/repos",
            "events_url": "https://api.github.com/users/yeliang2258/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yeliang2258/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-21T03:11:40Z",
        "updated_at": "2023-04-21T03:11:40Z",
        "author_association": "NONE",
        "body": "> @tianleiwu No at all, my rtx2060 didn't get speedup either, even worse, fp16 slower than fp32 my GPU has tensorcores and SM for fp16 calculation.\r\n\r\nMy test result is similar to yours,  most of the models did not gain speedup on T4",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1517197692/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]