[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/780817877",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-780817877",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 780817877,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc4MDgxNzg3Nw==",
        "user": {
            "login": "abuvaneswari",
            "id": 15253225,
            "node_id": "MDQ6VXNlcjE1MjUzMjI1",
            "avatar_url": "https://avatars.githubusercontent.com/u/15253225?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/abuvaneswari",
            "html_url": "https://github.com/abuvaneswari",
            "followers_url": "https://api.github.com/users/abuvaneswari/followers",
            "following_url": "https://api.github.com/users/abuvaneswari/following{/other_user}",
            "gists_url": "https://api.github.com/users/abuvaneswari/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/abuvaneswari/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/abuvaneswari/subscriptions",
            "organizations_url": "https://api.github.com/users/abuvaneswari/orgs",
            "repos_url": "https://api.github.com/users/abuvaneswari/repos",
            "events_url": "https://api.github.com/users/abuvaneswari/events{/privacy}",
            "received_events_url": "https://api.github.com/users/abuvaneswari/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-17T20:02:09Z",
        "updated_at": "2021-02-17T20:02:09Z",
        "author_association": "NONE",
        "body": "Further to my experiment above, I used the [resnet50_v1.onnx](https://github.com/microsoft/onnxruntime/tree/rel-1.6.0/onnxruntime/python/tools/quantization/E2E_example_model) model in your repo.\r\n\r\nI used quantize_dynamic() to arrive at the dynamically quantized version of the above. And I used the example code in the same directory to arrive at the statically quantized version. \r\n\r\nThe models execute slower than the FP32 model in the CPU and GPU (similar behavior as my SSD model). Sharing the CPU results here:\r\n\r\nModel | CPU Latency (ms) \r\nOriginal | 10.12\r\nINT8 - Dynamic | 52.28\r\nINT8 â€“ Static | 15.74\r\n\r\nPlease share any benchmarking results that you have on these models. Thank you.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/780817877/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/781109696",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-781109696",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 781109696,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc4MTEwOTY5Ng==",
        "user": {
            "login": "tracysh",
            "id": 42477615,
            "node_id": "MDQ6VXNlcjQyNDc3NjE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/42477615?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tracysh",
            "html_url": "https://github.com/tracysh",
            "followers_url": "https://api.github.com/users/tracysh/followers",
            "following_url": "https://api.github.com/users/tracysh/following{/other_user}",
            "gists_url": "https://api.github.com/users/tracysh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tracysh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tracysh/subscriptions",
            "organizations_url": "https://api.github.com/users/tracysh/orgs",
            "repos_url": "https://api.github.com/users/tracysh/repos",
            "events_url": "https://api.github.com/users/tracysh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tracysh/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-18T07:09:18Z",
        "updated_at": "2021-02-18T07:09:18Z",
        "author_association": "CONTRIBUTOR",
        "body": "@abuvaneswari, many of the performance problems are related to the MaxPool operator. The resnet50_v1.onnx model from ORT 1.6 as well as your SSD model has a number of MaxPool nodes. The latest main branch has faster support for MaxPool, but the model needs to be exported with opset 12. Opset 12 adds a definition for MaxPool that works with quantized uint8 types.\r\n\r\nI was seeing the quantized opset 10 resnet50_v1.onnx take 29ms against a 27ms FP32 model, but after switching to opset 12, the quantized model was getting down to 24ms. I'm seeing an improvement using your model with the latest, but it wasn't quite at FP32 performance yet.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/781109696/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/784320363",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-784320363",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 784320363,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc4NDMyMDM2Mw==",
        "user": {
            "login": "emilianavt",
            "id": 38952746,
            "node_id": "MDQ6VXNlcjM4OTUyNzQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/38952746?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/emilianavt",
            "html_url": "https://github.com/emilianavt",
            "followers_url": "https://api.github.com/users/emilianavt/followers",
            "following_url": "https://api.github.com/users/emilianavt/following{/other_user}",
            "gists_url": "https://api.github.com/users/emilianavt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/emilianavt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/emilianavt/subscriptions",
            "organizations_url": "https://api.github.com/users/emilianavt/orgs",
            "repos_url": "https://api.github.com/users/emilianavt/repos",
            "events_url": "https://api.github.com/users/emilianavt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/emilianavt/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-02-23T16:17:31Z",
        "updated_at": "2021-02-23T16:17:31Z",
        "author_association": "NONE",
        "body": "I also found there to be a significant slow down with the model from issue #5586 as described [here](https://github.com/microsoft/onnxruntime/issues/5319#issuecomment-775887968), both with INT8 and UINT8 as well as dynamic and static quantization. The model has a MobileNet v3 backbone, which doesn't use MaxPool as far as I know. Exporting it with opset 12 made no difference as well.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/784320363/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/988473888",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-988473888",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 988473888,
        "node_id": "IC_kwDOCVq1mM466uog",
        "user": {
            "login": "eggplant95",
            "id": 32993531,
            "node_id": "MDQ6VXNlcjMyOTkzNTMx",
            "avatar_url": "https://avatars.githubusercontent.com/u/32993531?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eggplant95",
            "html_url": "https://github.com/eggplant95",
            "followers_url": "https://api.github.com/users/eggplant95/followers",
            "following_url": "https://api.github.com/users/eggplant95/following{/other_user}",
            "gists_url": "https://api.github.com/users/eggplant95/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eggplant95/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eggplant95/subscriptions",
            "organizations_url": "https://api.github.com/users/eggplant95/orgs",
            "repos_url": "https://api.github.com/users/eggplant95/repos",
            "events_url": "https://api.github.com/users/eggplant95/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eggplant95/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-08T03:51:18Z",
        "updated_at": "2021-12-08T03:51:18Z",
        "author_association": "NONE",
        "body": "@abuvaneswari  Hi, Have you find an explanation to this problem? I am having the same testing result that int8 quantized model is slower than original model. I would be happy to have your reply, thanks.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/988473888/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/991196120",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-991196120",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 991196120,
        "node_id": "IC_kwDOCVq1mM47FHPY",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-10T18:25:01Z",
        "updated_at": "2021-12-10T18:25:01Z",
        "author_association": "MEMBER",
        "body": "@eggplant95, as it is mentioned here https://github.com/microsoft/onnxruntime/issues/5708#issuecomment-722220666, quantization needs hardware support to get better performance.\r\n\r\nRecently, we add new kernels for symmetric quantized weight. It usually get 2~3x speedup on machines with avx512 vnni. Please quantize your model with [weight type int8](https://github.com/microsoft/onnxruntime/blob/3c79f3055f4b9d3a67c7ee814f8ebc9cba1bda12/onnxruntime/python/tools/quantization/quantize.py#L144) to try it out.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/991196120/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100843585",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1100843585",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1100843585,
        "node_id": "IC_kwDOCVq1mM5BnYpB",
        "user": {
            "login": "stale[bot]",
            "id": 26384082,
            "node_id": "MDM6Qm90MjYzODQwODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/1724?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stale%5Bbot%5D",
            "html_url": "https://github.com/apps/stale",
            "followers_url": "https://api.github.com/users/stale%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/stale%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/stale%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stale%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stale%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/stale%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/stale%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/stale%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stale%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "created_at": "2022-04-17T09:53:59Z",
        "updated_at": "2022-04-17T09:53:59Z",
        "author_association": "NONE",
        "body": "This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1100843585/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1115646464",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1115646464",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1115646464,
        "node_id": "IC_kwDOCVq1mM5Cf2oA",
        "user": {
            "login": "MrRace",
            "id": 10300313,
            "node_id": "MDQ6VXNlcjEwMzAwMzEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/10300313?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MrRace",
            "html_url": "https://github.com/MrRace",
            "followers_url": "https://api.github.com/users/MrRace/followers",
            "following_url": "https://api.github.com/users/MrRace/following{/other_user}",
            "gists_url": "https://api.github.com/users/MrRace/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MrRace/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MrRace/subscriptions",
            "organizations_url": "https://api.github.com/users/MrRace/orgs",
            "repos_url": "https://api.github.com/users/MrRace/repos",
            "events_url": "https://api.github.com/users/MrRace/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MrRace/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-03T03:30:42Z",
        "updated_at": "2022-05-03T03:32:03Z",
        "author_association": "NONE",
        "body": "I try quantize_static with resnet50, find it just speed up in CPU. By the way my onnxruntime-gpu=1.11 and the opt=12.\r\nWhen compare the `quantize_static` onnx of resnet50 with the raw  float 32 onnx on GPU V100, the `quantize_static` is slower observably!\r\n```\r\nbenchmarking fp32 model...\r\n3.39ms\r\n3.40ms\r\n3.42ms\r\n3.40ms\r\n3.41ms\r\n3.41ms\r\n3.40ms\r\n3.41ms\r\n3.39ms\r\n3.41ms\r\nAvg: 3.40ms\r\nbenchmarking int8 model...\r\n4.83ms\r\n4.85ms\r\n4.86ms\r\n4.84ms\r\n4.85ms\r\n4.85ms\r\n4.85ms\r\n4.88ms\r\n4.85ms\r\n4.84ms\r\nAvg: 4.85ms\r\n```\r\n\r\nWhen run both models in CPU, the `quantize_static` onnx get better performance:\r\n```\r\nbenchmarking fp32 model...\r\n18.94ms\r\n18.88ms\r\n18.89ms\r\n19.08ms\r\n19.01ms\r\n18.86ms\r\n18.88ms\r\n18.90ms\r\n18.90ms\r\n18.93ms\r\nAvg: 18.93ms\r\nbenchmarking int8 model...\r\n12.83ms\r\n12.79ms\r\n12.81ms\r\n13.09ms\r\n13.03ms\r\n13.41ms\r\n13.00ms\r\n12.73ms\r\n12.67ms\r\n12.72ms\r\nAvg: 12.91ms\r\n```\r\nTherefore static quantization does not support GPU for resnet50? @yufenglee ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1115646464/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1120148761",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1120148761",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1120148761,
        "node_id": "IC_kwDOCVq1mM5CxB0Z",
        "user": {
            "login": "lucasjinreal",
            "id": 21303438,
            "node_id": "MDQ6VXNlcjIxMzAzNDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21303438?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucasjinreal",
            "html_url": "https://github.com/lucasjinreal",
            "followers_url": "https://api.github.com/users/lucasjinreal/followers",
            "following_url": "https://api.github.com/users/lucasjinreal/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucasjinreal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucasjinreal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucasjinreal/subscriptions",
            "organizations_url": "https://api.github.com/users/lucasjinreal/orgs",
            "repos_url": "https://api.github.com/users/lucasjinreal/repos",
            "events_url": "https://api.github.com/users/lucasjinreal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucasjinreal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-07T06:51:03Z",
        "updated_at": "2022-05-07T06:51:03Z",
        "author_association": "NONE",
        "body": "Correction: not for resnet50, but all models QLinearConv much more slower than float32 in ORT, which I don't know why.\r\n\r\nLiterally every model I quantized and using QLinearConv are slower than float32.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1120148761/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1121608901",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1121608901",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1121608901,
        "node_id": "IC_kwDOCVq1mM5C2mTF",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-09T21:38:52Z",
        "updated_at": "2022-05-09T21:38:52Z",
        "author_association": "MEMBER",
        "body": "@MrRace, GPU quantization requires activation to be quantized symmetrically. did you follow the gpu quantization example when quanting for GPU: https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/trt/resnet50? \r\n\r\n@jinfagang , please refer to here for expectation of performance gain of quantization: https://onnxruntime.ai/docs/performance/quantization.html#why-am-i-not-seeing-performance-improvements. \r\nAnd did you see warning like this when quantizing: https://github.com/microsoft/onnxruntime/blob/288892335e7499c4b235aea96fc097e3775435a1/onnxruntime/python/tools/quantization/quantize.py#L40 \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1121608901/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1121794751",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1121794751",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1121794751,
        "node_id": "IC_kwDOCVq1mM5C3Tq_",
        "user": {
            "login": "lucasjinreal",
            "id": 21303438,
            "node_id": "MDQ6VXNlcjIxMzAzNDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21303438?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucasjinreal",
            "html_url": "https://github.com/lucasjinreal",
            "followers_url": "https://api.github.com/users/lucasjinreal/followers",
            "following_url": "https://api.github.com/users/lucasjinreal/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucasjinreal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucasjinreal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucasjinreal/subscriptions",
            "organizations_url": "https://api.github.com/users/lucasjinreal/orgs",
            "repos_url": "https://api.github.com/users/lucasjinreal/repos",
            "events_url": "https://api.github.com/users/lucasjinreal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucasjinreal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-10T02:12:19Z",
        "updated_at": "2022-05-10T02:12:44Z",
        "author_association": "NONE",
        "body": "@yufenglee I am not using QDQ format, but onnxruntime quantized format, all params are int8 already, there is no op that can not quantize, so there doesn't need any int8 -> float32 conversion inside model. Still, the speed is slower.\r\n\r\nHowever, when I switched to ncnn, CPU worked like a charm ,pretty fast.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1121794751/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1125417832",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1125417832",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1125417832,
        "node_id": "IC_kwDOCVq1mM5DFINo",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-12T21:01:35Z",
        "updated_at": "2022-05-12T21:01:35Z",
        "author_association": "MEMBER",
        "body": "Would like to confirm if your activation is int8 or uint8. With ort quant format, it is expected to be slower if you activation is int8 instead of uint8. that's what this warning message about:\r\nhttps://github.com/microsoft/onnxruntime/blob/288892335e7499c4b235aea96fc097e3775435a1/onnxruntime/python/tools/quantization/quantize.py#L40",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1125417832/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1125419093",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1125419093",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1125419093,
        "node_id": "IC_kwDOCVq1mM5DFIhV",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-12T21:03:25Z",
        "updated_at": "2022-05-12T21:03:25Z",
        "author_association": "MEMBER",
        "body": "@jinfagang, it would be helpful if you could share the model or a dummy model to repro the issue. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1125419093/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1125960953",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1125960953",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1125960953,
        "node_id": "IC_kwDOCVq1mM5DHMz5",
        "user": {
            "login": "lucasjinreal",
            "id": 21303438,
            "node_id": "MDQ6VXNlcjIxMzAzNDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21303438?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucasjinreal",
            "html_url": "https://github.com/lucasjinreal",
            "followers_url": "https://api.github.com/users/lucasjinreal/followers",
            "following_url": "https://api.github.com/users/lucasjinreal/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucasjinreal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucasjinreal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucasjinreal/subscriptions",
            "organizations_url": "https://api.github.com/users/lucasjinreal/orgs",
            "repos_url": "https://api.github.com/users/lucasjinreal/repos",
            "events_url": "https://api.github.com/users/lucasjinreal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucasjinreal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-13T11:30:58Z",
        "updated_at": "2022-05-13T11:30:58Z",
        "author_association": "NONE",
        "body": "@yufenglee it's uint8 for activation.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1125960953/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1161424198",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1161424198",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1161424198,
        "node_id": "IC_kwDOCVq1mM5FOe1G",
        "user": {
            "login": "Etty-Cohen",
            "id": 62253636,
            "node_id": "MDQ6VXNlcjYyMjUzNjM2",
            "avatar_url": "https://avatars.githubusercontent.com/u/62253636?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Etty-Cohen",
            "html_url": "https://github.com/Etty-Cohen",
            "followers_url": "https://api.github.com/users/Etty-Cohen/followers",
            "following_url": "https://api.github.com/users/Etty-Cohen/following{/other_user}",
            "gists_url": "https://api.github.com/users/Etty-Cohen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Etty-Cohen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Etty-Cohen/subscriptions",
            "organizations_url": "https://api.github.com/users/Etty-Cohen/orgs",
            "repos_url": "https://api.github.com/users/Etty-Cohen/repos",
            "events_url": "https://api.github.com/users/Etty-Cohen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Etty-Cohen/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-21T08:23:26Z",
        "updated_at": "2022-06-21T08:31:18Z",
        "author_association": "NONE",
        "body": "> Hello,\r\n> \r\n> I used onnxruntime's quantize_dynamic() and qunatize_static() to get the INT8 quantized versions of my original model, which is a flavor of SSD model. Accuracy of the quantized models is acceptable.\r\n> \r\n> But when I run the benchmark tool (i.e) [onnxruntime_test.py](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/onnxruntime_test.py) on these models, I find that the quantized models are very slow both on CPU and GPU:\r\n> \r\n> Model | CPU Latency (ms) | GPU Latency (ms)\r\n> \r\n> Original | 5.42 | 3.41 INT8 - Dynamic | 45.76 | 27.66 INT8 â€“ Static | 17.32 | 9.3\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution Centos 7\r\n> * ONNX Runtime installed from (source or binary): binary\r\n> * ONNX Runtime version: 1.6.0\r\n> * Python version: 3.6\r\n> * CUDA/cuDNN version: 10.2/8\r\n> * GPU model and memory: V100/32GB\r\n> * ONNX version: 1.8.1\r\n> \r\n> **To Reproduce** I attach the 3 ONNX models here. [ssd300_ft_pruned_0.75.8.int8.QD.onnx.zip](https://github.com/microsoft/onnxruntime/files/5998148/ssd300_ft_pruned_0.75.8.int8.QD.onnx.zip) [ssd300_ft_pruned_0.75.8.int8.QS_1.6.0.onnx.zip](https://github.com/microsoft/onnxruntime/files/5998150/ssd300_ft_pruned_0.75.8.int8.QS_1.6.0.onnx.zip) [ssd300_ft_pruned_0.75.8.onnx.zip](https://github.com/microsoft/onnxruntime/files/5998151/ssd300_ft_pruned_0.75.8.onnx.zip)\r\n> \r\n> One can generate the dynamically quantized model ('QD') by using onnxruntime.quantize_dynamic() on the original model as follows:\r\n> \r\n> import onnx from onnxruntime.quantization import quantize_dynamic, QuantType\r\n> \r\n> model_fp32 = \"ssd300_ft_pruned_0.75.8.onnx\" model_quant = \"ssd300_ft_pruned_0.75.8.int8.QD.onnx quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\r\n> \r\n> I will share the static quantization code later if needed.\r\n> \r\n@[abuvaneswari](https://github.com/abuvaneswari)  Can you share it please (static quantization)?\r\nI try like this:\r\n```\r\nquantize_static(input_model_path,\r\n                    output_model_path,\r\n                    dr,\r\n                    quant_format=QuantFormat.QDQ,\r\n                    per_channel=args.per_channel,\r\n                    optimize_model=True,\r\n                    activation_type=QuantType.QInt8,\r\n                    weight_type=QuantType.QInt8,\r\n                    calibrate_method=CalibrationMethod.MinMax)\r\n```\r\nbut get 8 % accuracy decline.\r\n\r\nThen I try that quantization:\r\n`from quantize import quantize, QuantizationMode`\r\n\r\nlike this:\r\n```\r\nquantized_model = quantize(model, quantization_mode=QuantizationMode.IntegerOps,\r\n                           static=True,\r\n                           input_quantization_params={\r\n                                'efficientnetlite0_input': [np.uint8(113), np.float32(0.05)]\r\n\r\n                           })\r\n```\r\n\r\nand got that error:\r\n```\r\nraise ValueError(\"Quantization parameters are not specified for input {}.\".format(input_name))\r\nValueError: Quantization parameters are not specified for input StatefulPartitionedCall/sequential/efficientnetlite0/stem_conv/Conv2D__6:0.\r\n```\r\nI checked the input name like this:\r\n`session = onnxruntime.InferenceSession(model_fp32)\r\n    print(\"input name = \",session.get_inputs()[0].name)`\r\nand get:` input name =  efficientnetlite0_input`\r\n\r\nMaybe you can help me. I hope so.\r\nThanks.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1161424198/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1431238975",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1431238975",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1431238975,
        "node_id": "IC_kwDOCVq1mM5VTvk_",
        "user": {
            "login": "josephrocca",
            "id": 1167575,
            "node_id": "MDQ6VXNlcjExNjc1NzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1167575?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/josephrocca",
            "html_url": "https://github.com/josephrocca",
            "followers_url": "https://api.github.com/users/josephrocca/followers",
            "following_url": "https://api.github.com/users/josephrocca/following{/other_user}",
            "gists_url": "https://api.github.com/users/josephrocca/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/josephrocca/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/josephrocca/subscriptions",
            "organizations_url": "https://api.github.com/users/josephrocca/orgs",
            "repos_url": "https://api.github.com/users/josephrocca/repos",
            "events_url": "https://api.github.com/users/josephrocca/events{/privacy}",
            "received_events_url": "https://api.github.com/users/josephrocca/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-15T11:44:47Z",
        "updated_at": "2023-02-15T11:44:47Z",
        "author_association": "CONTRIBUTOR",
        "body": "Also seeing this with ONNX Runtime Web. You can see examples here:\r\n\r\n* https://josephrocca.github.io/rwkv-v4-web/demo/\r\n\r\nSpecifically you can compare the `rwkv-4-pile-169m.onnx` and `rwkv-4-pile-169m-uint8.onnx` models:\r\n\r\n * https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m.onnx\r\n * https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-uint8.onnx\r\n\r\nThe former runs at ~32 tokens/sec, while the latter runs at *less than half* that.\r\n\r\n(Note that opening DevTools on Chrome after initialising an inference session currently slows down inference speed, even after closing DevTools, so be aware of that when testing differences in inference speeds.)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1431238975/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1515660741",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6732#issuecomment-1515660741",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6732",
        "id": 1515660741,
        "node_id": "IC_kwDOCVq1mM5aVyXF",
        "user": {
            "login": "YaoHan404",
            "id": 24877782,
            "node_id": "MDQ6VXNlcjI0ODc3Nzgy",
            "avatar_url": "https://avatars.githubusercontent.com/u/24877782?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YaoHan404",
            "html_url": "https://github.com/YaoHan404",
            "followers_url": "https://api.github.com/users/YaoHan404/followers",
            "following_url": "https://api.github.com/users/YaoHan404/following{/other_user}",
            "gists_url": "https://api.github.com/users/YaoHan404/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/YaoHan404/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/YaoHan404/subscriptions",
            "organizations_url": "https://api.github.com/users/YaoHan404/orgs",
            "repos_url": "https://api.github.com/users/YaoHan404/repos",
            "events_url": "https://api.github.com/users/YaoHan404/events{/privacy}",
            "received_events_url": "https://api.github.com/users/YaoHan404/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-20T03:27:02Z",
        "updated_at": "2023-04-20T03:27:02Z",
        "author_association": "NONE",
        "body": "> I try quantize_static with resnet50, find it just speed up in CPU. By the way my onnxruntime-gpu=1.11 and the opt=12. When compare the `quantize_static` onnx of resnet50 with the raw float 32 onnx on GPU V100, the `quantize_static` is slower observably!\r\n> \r\n> ```\r\n> benchmarking fp32 model...\r\n> 3.39ms\r\n> 3.40ms\r\n> 3.42ms\r\n> 3.40ms\r\n> 3.41ms\r\n> 3.41ms\r\n> 3.40ms\r\n> 3.41ms\r\n> 3.39ms\r\n> 3.41ms\r\n> Avg: 3.40ms\r\n> benchmarking int8 model...\r\n> 4.83ms\r\n> 4.85ms\r\n> 4.86ms\r\n> 4.84ms\r\n> 4.85ms\r\n> 4.85ms\r\n> 4.85ms\r\n> 4.88ms\r\n> 4.85ms\r\n> 4.84ms\r\n> Avg: 4.85ms\r\n> ```\r\n> \r\n> When run both models in CPU, the `quantize_static` onnx get better performance:\r\n> \r\n> ```\r\n> benchmarking fp32 model...\r\n> 18.94ms\r\n> 18.88ms\r\n> 18.89ms\r\n> 19.08ms\r\n> 19.01ms\r\n> 18.86ms\r\n> 18.88ms\r\n> 18.90ms\r\n> 18.90ms\r\n> 18.93ms\r\n> Avg: 18.93ms\r\n> benchmarking int8 model...\r\n> 12.83ms\r\n> 12.79ms\r\n> 12.81ms\r\n> 13.09ms\r\n> 13.03ms\r\n> 13.41ms\r\n> 13.00ms\r\n> 12.73ms\r\n> 12.67ms\r\n> 12.72ms\r\n> Avg: 12.91ms\r\n> ```\r\n> \r\n> Therefore static quantization does not support GPU for resnet50? @yufenglee\r\n\r\n@MrRace I encountered the same problemï¼šjust speed up in CPU. Have you found a solution yetï¼Ÿ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1515660741/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]