[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633306751",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16674#issuecomment-1633306751",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16674",
        "id": 1633306751,
        "node_id": "IC_kwDOCVq1mM5hWkh_",
        "user": {
            "login": "wschin",
            "id": 3524474,
            "node_id": "MDQ6VXNlcjM1MjQ0NzQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3524474?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wschin",
            "html_url": "https://github.com/wschin",
            "followers_url": "https://api.github.com/users/wschin/followers",
            "following_url": "https://api.github.com/users/wschin/following{/other_user}",
            "gists_url": "https://api.github.com/users/wschin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wschin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wschin/subscriptions",
            "organizations_url": "https://api.github.com/users/wschin/orgs",
            "repos_url": "https://api.github.com/users/wschin/repos",
            "events_url": "https://api.github.com/users/wschin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wschin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-12T22:52:18Z",
        "updated_at": "2023-07-12T22:52:18Z",
        "author_association": "MEMBER",
        "body": "How do you build and run code? The code pasted is not clear. There are some random comments like\r\n```\r\n// Chans fix \r\n// under do not fix\r\n```\r\n\r\nCould elaborate more about the idea behind? Are you trying to compare code w/wo a fix for running `QLinearConv`?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633306751/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633346858",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16674#issuecomment-1633346858",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16674",
        "id": 1633346858,
        "node_id": "IC_kwDOCVq1mM5hWuUq",
        "user": {
            "login": "developerChans",
            "id": 68864422,
            "node_id": "MDQ6VXNlcjY4ODY0NDIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/68864422?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/developerChans",
            "html_url": "https://github.com/developerChans",
            "followers_url": "https://api.github.com/users/developerChans/followers",
            "following_url": "https://api.github.com/users/developerChans/following{/other_user}",
            "gists_url": "https://api.github.com/users/developerChans/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/developerChans/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/developerChans/subscriptions",
            "organizations_url": "https://api.github.com/users/developerChans/orgs",
            "repos_url": "https://api.github.com/users/developerChans/repos",
            "events_url": "https://api.github.com/users/developerChans/events{/privacy}",
            "received_events_url": "https://api.github.com/users/developerChans/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-12T23:55:15Z",
        "updated_at": "2023-07-13T05:06:42Z",
        "author_association": "NONE",
        "body": "Im sorry. I will erase trash code..\r\n\r\nMY Build Command\r\n`./build.sh --config RelWithDebInfo --build_wheel --skip_test --parallel 48\r\n`\r\n\r\nMy provider Code Set\r\n![image](https://github.com/microsoft/onnxruntime/assets/68864422/225523ae-0cac-440d-94a5-0b077ec70ea0)\r\n\r\nAnd then Next is my QLinearConv Code\r\n\r\nheadder\r\n\r\n```\r\n// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#pragma once\r\n\r\n#include <math.h>\r\n#include \"core/framework/op_kernel.h\"\r\n#include \"core/common/common.h\"\r\n#include \"core/providers/npu/helper/npu_interface.h\"\r\n#include \"core/providers/cpu/nn/conv_attributes.h\"\r\n\r\nnamespace onnxruntime {\r\nnamespace npu {\r\ntemplate <typename int8_t>\r\nclass QLinearConv : public OpKernel {\r\n public:\r\n  QLinearConv(const OpKernelInfo& info) : OpKernel(info),\r\n                                                   conv_attrs_(info),\r\n                                                   is_W_signed_(false),\r\n                                                   is_W_packed_(false) {\r\n    channels_last_ = (info.GetAttrOrDefault<int64_t>(\"channels_last\", static_cast<int64_t>(0)) != 0);\r\n    npu_interface = new NPUInterface();\r\n  }\r\n\r\n  Status Compute(OpKernelContext* context) const override;\r\n\r\n  void Conv2d(\r\n    const int8_t* X_buffer, const int64_t* X_shape, int8_t x_zp,\r\n    const int8_t* W_buffer, const int64_t* W_shape, int8_t w_zp, int32_t* Temp_buffer, \r\n    const int64_t* dilations, const int64_t* strides, const int64_t* pads\r\n  ) const;\r\n  \r\n// Cliping Data\r\n  void Quantize(\r\n    int8_t* dst, const int32_t* src, int32_t count,\r\n    float x_scale, float w_scale, float y_scale, int8_t y_z) const {\r\n    for(int i = 0; i < count; i++){\r\n      int32_t rounded_val = static_cast<int32_t>(std::nearbyintf(static_cast<double>(src[i]) * x_scale * w_scale / y_scale)) + y_z; \r\n      if(rounded_val <= -128) {\r\n        dst[i] = -128;\r\n      } else if(rounded_val >= 127) {\r\n        dst[i] = 127;\r\n      } else {\r\n        dst[i] = (int8_t)rounded_val;\r\n      }\r\n    }\r\n  };\r\n\r\n// Cliping Data overloading\r\n  void Quantize(\r\n    int8_t* dst, const int32_t* src, int32_t count,\r\n    float x_scale, const float* w_scale, float y_scale, int8_t y_z, int32_t feature_size) const {\r\n    for(int i = 0; i < count; i++){\r\n      int32_t channel =  i / feature_size;\r\n      int32_t rounded_val = static_cast<int32_t>(std::nearbyintf(static_cast<double>(src[i]) * x_scale * w_scale[channel] / y_scale)) + y_z; \r\n      if(rounded_val <= -128) {\r\n        dst[i] = -128;\r\n      } else if(rounded_val >= 127) {\r\n        dst[i] = 127;\r\n      } else {\r\n        dst[i] = (int8_t)rounded_val;\r\n      }\r\n    }      \r\n  };\r\n\r\n \r\n NPUInterface* npu_interface;\r\n private:\r\n  ConvAttributes conv_attrs_;\r\n  TensorShape W_shape_;\r\n  BufferUniquePtr reordered_W_buffer_;\r\n  bool is_W_signed_;\r\n  bool is_W_packed_;\r\n  bool channels_last_;\r\n};\r\n\r\n}  // namespace pim\r\n}  // namespace onnxruntime\r\n\r\n```\r\n\r\nAnd My QlinearConv implement\r\n\r\n```\r\n\r\n#include \"core/providers/npu/math/qlinearconv.h\"\r\n\r\nnamespace onnxruntime {\r\nnamespace npu {\r\n\r\ntemplate <typename int8_t>\r\nvoid QLinearConv<int8_t>::Conv2d(\r\n  const int8_t* X_buffer, const int64_t* X_shape, int8_t x_zp,\r\n  const int8_t* W_buffer, const int64_t* W_shape, int8_t w_zp,\r\n  int32_t* Temp_buffer, \r\n  const int64_t* dilations, const int64_t* strides, const int64_t* pads) const {\r\n    int32_t height_dilations = 0;\r\n\r\n    for(int32_t W_height = 0; W_height < W_shape[2]; W_height++) {\r\n        int32_t Y_unit = 0;\r\n\r\n        for(int32_t X_height = -pads[0]; X_height < X_shape[2] + pads[2] - dilations[0] * (W_shape[2] - 1); X_height += strides[0]) {\r\n            for(int32_t X_width = -pads[1]; X_width < X_shape[3] + pads[3] - dilations[1] * (W_shape[3] - 1); X_width += strides[1]) {\r\n                for(int32_t W_width = 0; W_width < W_shape[3]; W_width++) {\r\n                    if(X_height + height_dilations < 0 || X_width + W_width * dilations[1] < 0 || X_height + height_dilations > X_shape[2] - 1 || X_width + W_width * dilations[1] > X_shape[3] - 1) {\r\n                        Temp_buffer[Y_unit] += 0;\r\n                    } else {\r\n                        \r\n                        int32_t temp_X = X_buffer[(X_height + height_dilations) * X_shape[3] + X_width + W_width * dilations[1]] - x_zp;\r\n                        Temp_buffer[Y_unit] += temp_X * ((int32_t)W_buffer[W_height * W_shape[3] + W_width] - (int32_t)w_zp);\r\n                    }\r\n\r\n                }\r\n                Y_unit++;\r\n                \r\n            }\r\n        }\r\n        height_dilations += static_cast<int32_t>(dilations[0]);\r\n\r\n    }\r\n}\r\n\r\n/*\r\n\r\n#define REGISTER_KERNEL_TYPED(int8_t)\r\nONNX_OPERATOR_VERSIONED_TYPED_KERNEL_EX(                                              \\\r\n    QLinearConv,\r\n    kOnnxDomain,\r\n    1,\r\n    11,\r\n    int8_t,\r\n    kNpuExecutionProvider,\r\n    KernelDefBuilder()\r\n        .TypeConstraint(\"T1\", DataTypeImpl::GetTensorType<int8_t>())\r\n        .TypeConstraint(\"T2\", {DataTypeImpl::GetTensorType<int8_t>(), DataTypeImpl::GetTensorType<int8_t>()})\r\n        .TypeConstraint(\"T3\", DataTypeImpl::GetTensorType<int8_t>())\r\n        .TypeConstraint(\"T4\", DataTypeImpl::GetTensorType<int32_t>()),\r\n    QLinearConv<int8_t>);\r\nREGISTER_KERNEL_TYPED(int8_t)\r\n\r\n*/\r\n\r\n#define REGISTER_KERNEL_TYPED(int8_t)\r\nONNX_OPERATOR_TYPED_KERNEL_EX(                                              \\\r\n    QLinearConv,\r\n    kOnnxDomain,\r\n    10,\r\n    int8_t,\r\n    kNpuExecutionProvider,\r\n    KernelDefBuilder()\r\n        .TypeConstraint(\"T1\", DataTypeImpl::GetTensorType<int8_t>())\r\n        .TypeConstraint(\"T2\", DataTypeImpl::GetTensorType<int8_t>())\r\n        .TypeConstraint(\"T3\", DataTypeImpl::GetTensorType<int8_t>())\r\n        .TypeConstraint(\"T4\", DataTypeImpl::GetTensorType<int32_t>()),\r\n    QLinearConv<int8_t>);\r\nREGISTER_KERNEL_TYPED(int8_t)\r\n\r\ntemplate <typename int8_t>\r\nStatus QLinearConv<int8_t>::Compute(OpKernelContext* ctx) const {\r\n  int npu_code = npu_interface->GetNpuMode();\r\n  fprintf(stderr, \"npu_code: %d\\n\", npu_code);\r\n  \r\n\r\n  if(npu_code == 1) {\r\n    // Todo: Xilinx VMK180: Add ACC Code \r\n    ORT_NOT_IMPLEMENTED(\"NOT IMPLEMENTED NPU MODE\");\r\n  } else {\r\n    const auto* X = ctx->Input<Tensor>(0);\r\n    const auto* W = ctx->Input<Tensor>(3);\r\n    const auto& X_shape = X->Shape();\r\n    const auto& W_shape = is_W_packed_ ? W_shape_ : W->Shape();\r\n    const bool is_W_signed = (W != nullptr) ? W->IsDataType<int8_t>() : is_W_signed_;\r\n    \r\n    const auto* X_zero_point = ctx->Input<Tensor>(2);\r\n    const auto* W_zero_point = ctx->Input<Tensor>(5);\r\n    const auto* Y_zero_point = ctx->Input<Tensor>(7);\r\n    ORT_ENFORCE(IsScalarOr1ElementVector(X_zero_point), \"QLinearConv : input zero point must be a scalar or 1D tensor of size 1\");\r\n    ORT_ENFORCE(IsScalarOr1ElementVector(Y_zero_point), \"QLinearConv : result zero point must be a scalar or 1D tensor of size 1\");\r\n\r\n    auto X_zero_point_value = *(X_zero_point->template Data<int8_t>());\r\n    auto Y_zero_point_value = *(Y_zero_point->template Data<int8_t>());\r\n\r\n    int8_t W_zero_point_value;\r\n    const auto& W_zero_point_shape = W_zero_point->Shape();\r\n    if (W_zero_point_shape.NumDimensions() == 0 || (W_zero_point_shape.NumDimensions() == 1 && (W_zero_point_shape[0] == 1 || W_zero_point_shape[0] == W_shape[0]))) {\r\n      const int64_t W_zero_point_size = W_zero_point_shape.Size();\r\n      const auto* W_zero_point_data = static_cast<const int8_t*>(W_zero_point->DataRaw());\r\n      W_zero_point_value = W_zero_point_data[0];\r\n      for (int64_t i = 1; i < W_zero_point_size; i++) {\r\n        if (W_zero_point_data[i] != W_zero_point_value) {\r\n          return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, \"QLinearConv : filter zero point must be constant\");\r\n        }\r\n      }\r\n    } else {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, \"QLinearConv : filter zero point shape invalid\");\r\n    }\r\n\r\n    const auto* X_scale = ctx->Input<Tensor>(1);\r\n    const auto* W_scale = ctx->Input<Tensor>(4);\r\n    const auto* Y_scale = ctx->Input<Tensor>(6);\r\n\r\n    ORT_ENFORCE(IsScalarOr1ElementVector(X_scale), \"QLinearConv : input scale must be a scalar or 1D tensor of size 1\");\r\n    ORT_ENFORCE(IsScalarOr1ElementVector(Y_scale), \"QLinearConv : result scale must be a scalar or 1D tensor of size 1\");\r\n\r\n    auto X_scale_value = *(X_scale->template Data<float>());\r\n    auto Y_scale_value = *(Y_scale->template Data<float>());\r\n    const auto& W_scale_shape = W_scale->Shape();\r\n\r\n    const auto* W_scale_data = static_cast<const float*>(W_scale->DataRaw());\r\n\r\n\r\n    const auto* B = ctx->Input<Tensor>(8);\r\n\r\n    ORT_RETURN_IF_ERROR(conv_attrs_.ValidateInputShape(X_shape, W_shape, channels_last_));\r\n    \r\n    // Kernel Shape\r\n    std::vector<int64_t> kernel_shape;\r\n    ORT_RETURN_IF_ERROR(conv_attrs_.ComputeKernelShape(W_shape, kernel_shape));\r\n    const size_t kernel_rank = kernel_shape.size();\r\n    \r\n    // Pads\r\n    std::vector<int64_t> pads(conv_attrs_.pads);\r\n    if (pads.empty()) {\r\n      pads.resize(kernel_rank * 2, 0);\r\n    }\r\n    \r\n    // Dilations\r\n    std::vector<int64_t> dilations(conv_attrs_.dilations);\r\n    if (dilations.empty()) {\r\n      dilations.resize(kernel_rank, 1);\r\n    }\r\n\r\n    //Strides\r\n    std::vector<int64_t> strides(conv_attrs_.strides);\r\n    if (strides.empty()) {\r\n      strides.resize(kernel_rank, 1);\r\n    }\r\n\r\n    //Group\r\n    const int64_t group = conv_attrs_.group;\r\n    \r\n    const int64_t C = X_shape[channels_last_ ? 1 + kernel_rank : 1];\r\n    const size_t spatial_dim_start = channels_last_ ? 1 : 2;\r\n    const size_t spatial_dim_end = spatial_dim_start + kernel_rank;\r\n\r\n    std::vector<int64_t> Y_dims({X_shape[0]});\r\n    if (!channels_last_) {\r\n      Y_dims.push_back(W_shape[0]);\r\n    }\r\n    TensorShape input_shape = X_shape.Slice(spatial_dim_start, spatial_dim_end);\r\n    ORT_RETURN_IF_ERROR(conv_attrs_.InferOutputShape(input_shape, kernel_shape, strides, dilations, pads, Y_dims));\r\n    if (channels_last_) {\r\n      Y_dims.push_back(W_shape[0]);\r\n    }\r\n    Tensor* Y = ctx->Output(0, TensorShape(Y_dims));\r\n    const auto& Y_shape = Y->Shape();\r\n    int64_t Y_total_unit = Y->Shape().Size();\r\n\r\n    // Bail out early if one of the dimensions is zero.\r\n    if (Y->Shape().Size() == 0) {\r\n      return Status::OK();\r\n    }    \r\n\r\n    int32_t* Y_temp = (int32_t*)calloc(sizeof(int32_t), Y_total_unit);\r\n\r\n    const auto* Xdata = X->template Data<int8_t>();\r\n    const auto* Wdata = W->template Data<int8_t>();\r\n    const auto* Bdata = B != nullptr ? B->template Data<int32_t>() : nullptr;\r\n\r\n    int64_t X_unit = X_shape[2] * X_shape[3];\r\n    int64_t W_unit = W_shape[2] * W_shape[3];\r\n    int64_t Y_unit = Y_shape[2] * Y_shape[3];\r\n\r\n    for(size_t batch = 0; batch < static_cast<size_t>(X_shape[0]); batch++) {\r\n      for(size_t feature = 0; feature < static_cast<size_t>(W_shape[0]); feature++) {\r\n        int64_t g = static_cast<int64_t>(feature) * group / W_shape[0];\r\n\r\n        for(size_t channel = 0; channel < static_cast<size_t>(W_shape[1]); channel++) {\r\n          const auto* X_flatten = Xdata + X_unit * (W_shape[1] * g + channel);\r\n          const auto* W_flatten = Wdata + W_unit * (W_shape[1] * feature + channel);\r\n          Conv2d(Xdata, X_shape.GetDims().data(), X_zero_point_value,\r\n                Wdata, W_shape.GetDims().data(), W_zero_point_value,\r\n                Y_temp, dilations.data(), strides.data(), pads.data());\r\n        }\r\n\r\n        if(B != nullptr) {\r\n          int32_t bias = Bdata[feature];\r\n          for(size_t i = 0; i < static_cast<size_t>(Y_total_unit); i++) {\r\n            Y_temp[i] += bias;\r\n          }\r\n        }\r\n\r\n        Y_temp += Y_total_unit;\r\n\r\n      }\r\n      Xdata += X_unit * group * W_shape[1];\r\n    }\r\n\r\n    Y_temp = Y_temp - Y_total_unit;\r\n\r\n    auto* Ydata = Y->template MutableData<int8_t>();\r\n\r\n    const int64_t W_scale_size = W_scale_shape.Size();\r\n    if(W_scale_size == 1) {\r\n      Quantize(Ydata, Y_temp, Y_total_unit, X_scale_value, W_scale_data[0], Y_scale_value, Y_zero_point_value);\r\n    } else {\r\n      Quantize(Ydata, Y_temp, Y_total_unit, X_scale_value, static_cast<const float*>(W_scale_data), Y_scale_value, Y_zero_point_value, Y_unit);\r\n    }\r\n\r\n    free(Y_temp);\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\n} // namespace npu\r\n} // namespace onnxruntime\r\n\r\n```\r\n\r\nPython Run Code\r\nAlso I use python3-venv environment\r\n\r\n```\r\nimport onnxruntime \r\nimport numpy as np\r\nimport os\r\nimport sys\r\n\r\nprint(onnxruntime.__version__)\r\n\r\nmodel_name = sys.argv[1]\r\n\r\nexport_model_path = './{}/model.onnx'.format(model_name)\r\nsess_options = onnxruntime.SessionOptions()\r\n#ep_list = ['CPUExecutionProvider']\r\n# ep_list = [ 'PIMExecutionProvider', 'CPUExecutionProvider']\r\nep_list = ['NPUExecutionProvider', 'CPUExecutionProvider']\r\nsession = onnxruntime.InferenceSession(export_model_path, sess_options, providers = ep_list)\r\n\r\n\r\ninput_numpy = None\r\nif model_name == \"mq\" :\r\n  # mobilenet quantized model\r\n  input_numpy = np.random.rand(1,3,224,224).astype(np.float32)\r\n  print(\"mobilenet quant model\")\r\nelse :\r\n  # mnist model\r\n  input_numpy = np.random.rand(1,1,28,28).astype(np.float32)\r\n  print(\"mnist model\")\r\n\r\n\r\nort_outputs = session.run(None, {session.get_inputs()[0].name : input_numpy})\r\nprint(ort_outputs)\r\n```\r\n\r\nAlso I have one more question\r\n- What is GetCapability's roles ?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633346858/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633359827",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16674#issuecomment-1633359827",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16674",
        "id": 1633359827,
        "node_id": "IC_kwDOCVq1mM5hWxfT",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-13T00:15:42Z",
        "updated_at": "2023-07-13T00:15:42Z",
        "author_association": "MEMBER",
        "body": "GetCapability is how the EP requests nodes it wants to run. If you cannot run a node you should not request it. I would suspect you're requesting the QLinearMatMul node in the values returned by GetCapability, which fails as your EP does not have a kernel for that. \r\n\r\n>   if (!graph.is_partitioned) {\r\n\r\nNot sure where this code is coming from as GraphViewer in the ORT source does not have an `is_partitioned` member (and if it did it would be accessed via a getter). Possibly it's causing the issue given the code in the `if` branch seems to only request QLinearConv (correctly), and what is returned by the `else` branch is unclear and suspicious. \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633359827/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633539632",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16674#issuecomment-1633539632",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16674",
        "id": 1633539632,
        "node_id": "IC_kwDOCVq1mM5hXdYw",
        "user": {
            "login": "developerChans",
            "id": 68864422,
            "node_id": "MDQ6VXNlcjY4ODY0NDIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/68864422?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/developerChans",
            "html_url": "https://github.com/developerChans",
            "followers_url": "https://api.github.com/users/developerChans/followers",
            "following_url": "https://api.github.com/users/developerChans/following{/other_user}",
            "gists_url": "https://api.github.com/users/developerChans/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/developerChans/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/developerChans/subscriptions",
            "organizations_url": "https://api.github.com/users/developerChans/orgs",
            "repos_url": "https://api.github.com/users/developerChans/repos",
            "events_url": "https://api.github.com/users/developerChans/events{/privacy}",
            "received_events_url": "https://api.github.com/users/developerChans/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-13T04:48:08Z",
        "updated_at": "2023-07-13T04:48:08Z",
        "author_association": "NONE",
        "body": "\r\nI look about graph is_partitioned... \r\nBut, CPUExectuionProvider\" spits out the result that \"QLinearMatMul\" is not implemented.\r\nAlso, in the case of operators of other \"CPUExecutionProvider\", it was confirmed that they were added to Session well.\r\n\r\nNext image is my Log\r\n![image](https://github.com/microsoft/onnxruntime/assets/68864422/44cc7793-6917-48ad-88ee-c20fe37edb13)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633539632/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633577800",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16674#issuecomment-1633577800",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16674",
        "id": 1633577800,
        "node_id": "IC_kwDOCVq1mM5hXmtI",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-13T05:37:35Z",
        "updated_at": "2023-07-13T05:37:48Z",
        "author_association": "MEMBER",
        "body": "What data types does the QLinearMatMul node have? The current ORT implementation doesn't support everything. \r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/b7fd5af48bcad76466b67dcd95dfc78a0bb469b2/onnxruntime/core/providers/cpu/quantization/quantize_linear_matmul.cc#L16-L41\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633577800/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633612839",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16674#issuecomment-1633612839",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16674",
        "id": 1633612839,
        "node_id": "IC_kwDOCVq1mM5hXvQn",
        "user": {
            "login": "developerChans",
            "id": 68864422,
            "node_id": "MDQ6VXNlcjY4ODY0NDIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/68864422?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/developerChans",
            "html_url": "https://github.com/developerChans",
            "followers_url": "https://api.github.com/users/developerChans/followers",
            "following_url": "https://api.github.com/users/developerChans/following{/other_user}",
            "gists_url": "https://api.github.com/users/developerChans/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/developerChans/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/developerChans/subscriptions",
            "organizations_url": "https://api.github.com/users/developerChans/orgs",
            "repos_url": "https://api.github.com/users/developerChans/repos",
            "events_url": "https://api.github.com/users/developerChans/events{/privacy}",
            "received_events_url": "https://api.github.com/users/developerChans/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-13T06:13:36Z",
        "updated_at": "2023-07-13T06:20:54Z",
        "author_association": "NONE",
        "body": "The currently used models are int8_t, int8_t, and int8_t.\r\n\r\n![image](https://github.com/microsoft/onnxruntime/assets/68864422/0f0ac3d7-2933-4666-bc27-a3e9f11720db)\r\n\r\nOMG... In ORT 1.6.0 version not support int8 int8 int8 data type...\r\n![image](https://github.com/microsoft/onnxruntime/assets/68864422/a030ace3-7bc4-4d8c-a76a-6d980c487309)\r\n\r\nThanks for your help @skottmckay ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1633612839/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]