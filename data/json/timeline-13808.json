[
    {
        "id": 7932354295,
        "node_id": "LE_lADOCVq1mM5Xs8eezwAAAAHYzh73",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7932354295",
        "actor": {
            "login": "github-actions[bot]",
            "id": 41898282,
            "node_id": "MDM6Qm90NDE4OTgyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/in/15368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/github-actions%5Bbot%5D",
            "html_url": "https://github.com/apps/github-actions",
            "followers_url": "https://api.github.com/users/github-actions%5Bbot%5D/followers",
            "following_url": "https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}",
            "gists_url": "https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/github-actions%5Bbot%5D/subscriptions",
            "organizations_url": "https://api.github.com/users/github-actions%5Bbot%5D/orgs",
            "repos_url": "https://api.github.com/users/github-actions%5Bbot%5D/repos",
            "events_url": "https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}",
            "received_events_url": "https://api.github.com/users/github-actions%5Bbot%5D/received_events",
            "type": "Bot",
            "site_admin": false
        },
        "event": "labeled",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-01T14:18:23Z",
        "label": {
            "name": "model:transformer",
            "color": "4EF6CD"
        },
        "performed_via_github_app": null
    },
    {
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T14:21:34Z",
        "updated_at": "2022-12-01T14:21:34Z",
        "source": {
            "type": "issue",
            "issue": {
                "url": "https://api.github.com/repos/huggingface/optimum/issues/524",
                "repository_url": "https://api.github.com/repos/huggingface/optimum",
                "labels_url": "https://api.github.com/repos/huggingface/optimum/issues/524/labels{/name}",
                "comments_url": "https://api.github.com/repos/huggingface/optimum/issues/524/comments",
                "events_url": "https://api.github.com/repos/huggingface/optimum/issues/524/events",
                "html_url": "https://github.com/huggingface/optimum/issues/524",
                "id": 1467745390,
                "node_id": "I_kwDOFx0ogc5XfARu",
                "number": 524,
                "title": "Possibility to speed up inference of onnx models with transformers.pipeline",
                "user": {
                    "login": "young-chao",
                    "id": 34190033,
                    "node_id": "MDQ6VXNlcjM0MTkwMDMz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/34190033?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/young-chao",
                    "html_url": "https://github.com/young-chao",
                    "followers_url": "https://api.github.com/users/young-chao/followers",
                    "following_url": "https://api.github.com/users/young-chao/following{/other_user}",
                    "gists_url": "https://api.github.com/users/young-chao/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/young-chao/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/young-chao/subscriptions",
                    "organizations_url": "https://api.github.com/users/young-chao/orgs",
                    "repos_url": "https://api.github.com/users/young-chao/repos",
                    "events_url": "https://api.github.com/users/young-chao/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/young-chao/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 10,
                "created_at": "2022-11-29T09:39:16Z",
                "updated_at": "2022-12-08T12:26:15Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "repository": {
                    "id": 387786881,
                    "node_id": "MDEwOlJlcG9zaXRvcnkzODc3ODY4ODE=",
                    "name": "optimum",
                    "full_name": "huggingface/optimum",
                    "private": false,
                    "owner": {
                        "login": "huggingface",
                        "id": 25720743,
                        "node_id": "MDEyOk9yZ2FuaXphdGlvbjI1NzIwNzQz",
                        "avatar_url": "https://avatars.githubusercontent.com/u/25720743?v=4",
                        "gravatar_id": "",
                        "url": "https://api.github.com/users/huggingface",
                        "html_url": "https://github.com/huggingface",
                        "followers_url": "https://api.github.com/users/huggingface/followers",
                        "following_url": "https://api.github.com/users/huggingface/following{/other_user}",
                        "gists_url": "https://api.github.com/users/huggingface/gists{/gist_id}",
                        "starred_url": "https://api.github.com/users/huggingface/starred{/owner}{/repo}",
                        "subscriptions_url": "https://api.github.com/users/huggingface/subscriptions",
                        "organizations_url": "https://api.github.com/users/huggingface/orgs",
                        "repos_url": "https://api.github.com/users/huggingface/repos",
                        "events_url": "https://api.github.com/users/huggingface/events{/privacy}",
                        "received_events_url": "https://api.github.com/users/huggingface/received_events",
                        "type": "Organization",
                        "site_admin": false
                    },
                    "html_url": "https://github.com/huggingface/optimum",
                    "description": "ðŸŽï¸ Accelerate training and inference of ðŸ¤— Transformers with easy to use hardware optimization tools",
                    "fork": false,
                    "url": "https://api.github.com/repos/huggingface/optimum",
                    "forks_url": "https://api.github.com/repos/huggingface/optimum/forks",
                    "keys_url": "https://api.github.com/repos/huggingface/optimum/keys{/key_id}",
                    "collaborators_url": "https://api.github.com/repos/huggingface/optimum/collaborators{/collaborator}",
                    "teams_url": "https://api.github.com/repos/huggingface/optimum/teams",
                    "hooks_url": "https://api.github.com/repos/huggingface/optimum/hooks",
                    "issue_events_url": "https://api.github.com/repos/huggingface/optimum/issues/events{/number}",
                    "events_url": "https://api.github.com/repos/huggingface/optimum/events",
                    "assignees_url": "https://api.github.com/repos/huggingface/optimum/assignees{/user}",
                    "branches_url": "https://api.github.com/repos/huggingface/optimum/branches{/branch}",
                    "tags_url": "https://api.github.com/repos/huggingface/optimum/tags",
                    "blobs_url": "https://api.github.com/repos/huggingface/optimum/git/blobs{/sha}",
                    "git_tags_url": "https://api.github.com/repos/huggingface/optimum/git/tags{/sha}",
                    "git_refs_url": "https://api.github.com/repos/huggingface/optimum/git/refs{/sha}",
                    "trees_url": "https://api.github.com/repos/huggingface/optimum/git/trees{/sha}",
                    "statuses_url": "https://api.github.com/repos/huggingface/optimum/statuses/{sha}",
                    "languages_url": "https://api.github.com/repos/huggingface/optimum/languages",
                    "stargazers_url": "https://api.github.com/repos/huggingface/optimum/stargazers",
                    "contributors_url": "https://api.github.com/repos/huggingface/optimum/contributors",
                    "subscribers_url": "https://api.github.com/repos/huggingface/optimum/subscribers",
                    "subscription_url": "https://api.github.com/repos/huggingface/optimum/subscription",
                    "commits_url": "https://api.github.com/repos/huggingface/optimum/commits{/sha}",
                    "git_commits_url": "https://api.github.com/repos/huggingface/optimum/git/commits{/sha}",
                    "comments_url": "https://api.github.com/repos/huggingface/optimum/comments{/number}",
                    "issue_comment_url": "https://api.github.com/repos/huggingface/optimum/issues/comments{/number}",
                    "contents_url": "https://api.github.com/repos/huggingface/optimum/contents/{+path}",
                    "compare_url": "https://api.github.com/repos/huggingface/optimum/compare/{base}...{head}",
                    "merges_url": "https://api.github.com/repos/huggingface/optimum/merges",
                    "archive_url": "https://api.github.com/repos/huggingface/optimum/{archive_format}{/ref}",
                    "downloads_url": "https://api.github.com/repos/huggingface/optimum/downloads",
                    "issues_url": "https://api.github.com/repos/huggingface/optimum/issues{/number}",
                    "pulls_url": "https://api.github.com/repos/huggingface/optimum/pulls{/number}",
                    "milestones_url": "https://api.github.com/repos/huggingface/optimum/milestones{/number}",
                    "notifications_url": "https://api.github.com/repos/huggingface/optimum/notifications{?since,all,participating}",
                    "labels_url": "https://api.github.com/repos/huggingface/optimum/labels{/name}",
                    "releases_url": "https://api.github.com/repos/huggingface/optimum/releases{/id}",
                    "deployments_url": "https://api.github.com/repos/huggingface/optimum/deployments",
                    "created_at": "2021-07-20T12:36:40Z",
                    "updated_at": "2022-12-20T17:43:08Z",
                    "pushed_at": "2022-12-20T17:37:52Z",
                    "git_url": "git://github.com/huggingface/optimum.git",
                    "ssh_url": "git@github.com:huggingface/optimum.git",
                    "clone_url": "https://github.com/huggingface/optimum.git",
                    "svn_url": "https://github.com/huggingface/optimum",
                    "homepage": "https://huggingface.co/docs/optimum/",
                    "size": 2376,
                    "stargazers_count": 831,
                    "watchers_count": 831,
                    "language": "Python",
                    "has_issues": true,
                    "has_projects": true,
                    "has_downloads": true,
                    "has_wiki": true,
                    "has_pages": false,
                    "has_discussions": false,
                    "forks_count": 105,
                    "mirror_url": null,
                    "archived": false,
                    "disabled": false,
                    "open_issues_count": 93,
                    "license": {
                        "key": "apache-2.0",
                        "name": "Apache License 2.0",
                        "spdx_id": "Apache-2.0",
                        "url": "https://api.github.com/licenses/apache-2.0",
                        "node_id": "MDc6TGljZW5zZTI="
                    },
                    "allow_forking": true,
                    "is_template": false,
                    "web_commit_signoff_required": false,
                    "topics": [
                        "graphcore",
                        "habana",
                        "inference",
                        "intel",
                        "onnx",
                        "onnxruntime",
                        "optimization",
                        "pytorch",
                        "quantization",
                        "training",
                        "transformers"
                    ],
                    "visibility": "public",
                    "forks": 105,
                    "open_issues": 93,
                    "watchers": 831,
                    "default_branch": "main",
                    "permissions": {
                        "admin": false,
                        "maintain": false,
                        "push": false,
                        "triage": false,
                        "pull": true
                    }
                },
                "body": "### Problem Description\r\nBased on the model named \"Helsinki-NLP/opus-mt-es-en\", I investigated the time-consuming composition of using the onnx model and the pytorch model for inference, and found that the main time difference lies in a small step in beam search:\r\n[scores = scores.masked_fill(banned_mask, -float(\"inf\"))](https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/generation_logits_process.py)\r\nWhen I use the pytorch model for inference, this line of code only consumes 0.10ms per execution, while using the onnx model consumes close to 10msã€‚\r\nI consider that each execution needs to load the corresponding pytorch environment, which consumes some initialization time. If some measures can be taken to reduce the time here, the efficiency of using the onnx model for inference will be significantly improved.\r\n\r\nThe time-consuming of the following codes is also significantly different in the two reasoning methodsï¼š\r\n```\r\n# https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/generation_logits_process.py\r\nstatic_bad_words_mask = torch.zeros(scores.shape[1])\r\nstatic_bad_words_mask[self.bad_words_id_length_1] = 1\r\nreturn static_bad_words_mask.unsqueeze(0).to(scores.device).bool()\r\n```\r\n\r\n### Model loading and inference\r\n1. pytorch modelï¼š\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./bin_model)\r\nresult = model.generate(**model_inputs)\r\n2. onnx modelï¼š\r\nmodel = ORTModelForSeq2SeqLM.from_pretrained(\"./onnx_model\", from_transformers=False)\r\nonnx_translation = pipeline(\"translation_es_to_en\", model=model, tokenizer=tokenizer)\r\nresult = onnx_translation(inputs)\r\n\r\n### Machine configuration\r\n36 cores CPUï¼Œno GPU",
                "reactions": {
                    "url": "https://api.github.com/repos/huggingface/optimum/issues/524/reactions",
                    "total_count": 1,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 1,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/huggingface/optimum/issues/524/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "event": "cross-referenced"
    },
    {
        "id": 7932451159,
        "node_id": "RTE_lADOCVq1mM5Xs8eezwAAAAHYz5lX",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7932451159",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "renamed",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-01T14:28:41Z",
        "rename": {
            "from": "[CPUExecutionProvider] PyTorch operations following InferenceSession.run() are 50x slower",
            "to": "[CPUExecutionProvider] PyTorch operations following InferenceSession.run() are 50x slower compared to using dummy inputs"
        },
        "performed_via_github_app": null
    },
    {
        "id": 7932552359,
        "node_id": "RTE_lADOCVq1mM5Xs8eezwAAAAHY0SSn",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7932552359",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "renamed",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-01T14:39:54Z",
        "rename": {
            "from": "[CPUExecutionProvider] PyTorch operations following InferenceSession.run() are 50x slower compared to using dummy inputs",
            "to": "[CPUExecutionProvider] PyTorch/Numpy operations following InferenceSession.run() are 50x slower compared to using dummy inputs"
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334195038",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334195038",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334195038,
        "node_id": "IC_kwDOCVq1mM5PhjNe",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T18:44:42Z",
        "updated_at": "2022-12-01T23:02:42Z",
        "author_association": "MEMBER",
        "body": "Updated:\r\n\r\nActually, I can reproduce the issue. Add profiling:\r\n```\r\nimport copy\r\nimport time\r\n\r\nimport onnxruntime as ort\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\ndef test(use_ort_in_loop):\r\n\tbatch_size = 4\r\n\tsequence_length = 32\r\n\r\n\ttokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\r\n\r\n\ttotal_infsession = []\r\n\ttotal_no_infsession = []\r\n\tn_loop = 100\r\n\r\n\tinp = {\r\n\t    \"input_ids\": torch.randint(tokenizer.vocab_size - 1, (batch_size, 5), dtype=torch.int64),\r\n\t    \"encoder_attention_mask\": torch.ones(batch_size, sequence_length, dtype=torch.int64),\r\n\t    \"encoder_hidden_states\": torch.rand((batch_size, sequence_length, 512), dtype=torch.float32)\r\n\t}\r\n\r\n\tort_inp = {}\r\n\tfor key, value in inp.items():\r\n\t    ort_inp[key] = inp[key].detach().cpu().numpy()\r\n\r\n\tsession = ort.InferenceSession(\"decoder_model.onnx\", providers=[\"CPUExecutionProvider\"])\r\n\r\n\tM = torch.zeros(32128, 2)\r\n\r\n\tdef get_next_token_logits(ort_inp):\r\n\t    res = session.run(None, ort_inp)\r\n\t    out = torch.from_numpy(res[0])\r\n\t    next_token_logits = out[:, -1, :]\r\n\r\n\t    # forcing contiguous does not help :-(\r\n\t    # next_token_logits = next_token_logits.contiguous()\r\n\r\n\t    return next_token_logits\r\n\r\n\tnext_token_logits = get_next_token_logits(ort_inp)\r\n\ttime.sleep(2)\r\n\twith torch.profiler.profile(\r\n\t    activities=[\r\n\t        torch.profiler.ProfilerActivity.CPU,\r\n\t        torch.profiler.ProfilerActivity.CUDA,\r\n\t    ]\r\n\t) as p:\r\n\t\tfor i in range(n_loop):\r\n\t\t\tif use_ort_in_loop:\r\n\t\t\t\tnext_token_logits = get_next_token_logits(ort_inp)\r\n\t\t\tstart = time.time()\r\n\t\t\ttorch.matmul(next_token_logits, M)\r\n\t\t\ttotal_no_infsession += [time.time() - start]\r\n\tprint(f\"use_ort_in_loop={use_ort_in_loop}, operation took {sum(total_no_infsession) / n_loop * 1000} ms over {n_loop} runs\")\r\n\tprint(p.key_averages().table( sort_by=\"self_cpu_time_total\", row_limit=-1))\r\n\r\nwith torch.no_grad():\r\n    test(True)\r\n    test(False)\r\n```\r\n\r\nThe output:\r\n```\r\nuse_ort_in_loop=True, operation took 32.16004133224487 ms over 100 runs\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                   aten::mm        93.75%        3.214s        93.75%        3.214s      32.140ms           100\r\n      cudaDeviceSynchronize         6.07%     208.037ms         6.07%     208.037ms     208.037ms             1\r\n    cudaGetDeviceProperties         0.06%       2.213ms         0.06%       2.213ms       2.213ms             1\r\n                aten::slice         0.05%       1.772ms         0.06%       2.084ms      10.420us           200\r\n               aten::select         0.02%     723.000us         0.02%     823.000us       8.230us           100\r\n               aten::matmul         0.02%     515.000us        93.76%        3.215s      32.145ms           100\r\n           aten::lift_fresh         0.01%     441.000us         0.01%     441.000us       4.410us           100\r\n           aten::as_strided         0.01%     412.000us         0.01%     412.000us       1.373us           300\r\n         cudaGetDeviceCount         0.01%     189.000us         0.01%     189.000us     189.000us             1\r\n         aten::resolve_conj         0.00%       2.000us         0.00%       2.000us       0.007us           300\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 3.428s\r\n\r\nSTAGE:2022-12-01 22:37:36 2951695:2951695 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 22:37:36 2951695:2951695 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=False, operation took 0.2479982376098633 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        94.07%      24.398ms        94.07%      24.399ms     243.990us           100\r\n             aten::matmul         5.88%       1.525ms        94.46%      24.498ms     244.980us           100\r\n    cudaDeviceSynchronize         0.05%      12.000us         0.05%      12.000us      12.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 25.936ms\r\n\r\n```\r\n\r\nRight now, I am not sure why there are extra code called (like aten::slice).\r\n\r\nOne possible cause is ORT session has thread pool, and torch uses threads for matmul. In some way, ORT thread that slows down torch thread.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334195038/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334203162",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334203162",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334203162,
        "node_id": "IC_kwDOCVq1mM5PhlMa",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T18:53:17Z",
        "updated_at": "2022-12-01T19:21:04Z",
        "author_association": "CONTRIBUTOR",
        "body": "I'll try again tomorrow, I had the issue with contiguous as well. Edit: interestingly on my home laptop I can't reproduce the x50 slowdown, only x2 (both contiguous and non contiguous).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334203162/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334579176",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334579176",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334579176,
        "node_id": "IC_kwDOCVq1mM5PjA_o",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T23:32:15Z",
        "updated_at": "2022-12-03T19:11:23Z",
        "author_association": "MEMBER",
        "body": "I did an experiments which shows that reducing ORT threads help: 31ms => 0.3ms. It confirms that the cause is thread conflictions between ORT and Torch.\r\n\r\n```\r\nimport time\r\n\r\nimport onnxruntime as ort\r\nimport psutil\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\ndef test(\r\n    use_ort_in_loop,\r\n    num_thread_ort,\r\n    num_thread_torch,\r\n    contiguous=False,\r\n    n_loop=100,\r\n    disable_ort_spinning=False,\r\n    profiling=False,\r\n):\r\n    batch_size = 4\r\n    sequence_length = 32\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\r\n\r\n    inp = {\r\n        \"input_ids\": torch.randint(tokenizer.vocab_size - 1, (batch_size, 5), dtype=torch.int64),\r\n        \"encoder_attention_mask\": torch.ones(batch_size, sequence_length, dtype=torch.int64),\r\n        \"encoder_hidden_states\": torch.rand((batch_size, sequence_length, 512), dtype=torch.float32),\r\n    }\r\n\r\n    ort_inp = {}\r\n    for key in inp:\r\n        ort_inp[key] = inp[key].detach().cpu().numpy()\r\n\r\n    sess_options = ort.SessionOptions()\r\n    if disable_ort_spinning:\r\n        sess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\r\n\r\n    session = ort.InferenceSession(\"decoder_model.onnx\", sess_options=sess_options, providers=[\"CPUExecutionProvider\"])\r\n\r\n    M = torch.zeros(32128, 2)\r\n\r\n    def get_next_token_logits(ort_inp):\r\n        res = session.run(None, ort_inp)\r\n        out = torch.from_numpy(res[0])\r\n        next_token_logits = out[:, -1, :]\r\n        return next_token_logits.contiguous() if contiguous else next_token_logits\r\n\r\n    next_token_logits = get_next_token_logits(ort_inp)\r\n    time.sleep(15)\r\n\r\n    torch.set_num_threads(num_thread_torch)\r\n\r\n    def run_perf_test(next_token_logits):\r\n        latency = []\r\n        for _ in range(n_loop):\r\n            if use_ort_in_loop:\r\n                next_token_logits = get_next_token_logits(ort_inp)\r\n            start = time.time()\r\n            torch.matmul(next_token_logits, M)\r\n            latency += [time.time() - start]\r\n        return latency\r\n\r\n    if profiling:\r\n        with torch.profiler.profile(\r\n            activities=[\r\n                torch.profiler.ProfilerActivity.CPU,\r\n                torch.profiler.ProfilerActivity.CUDA,\r\n            ]\r\n        ) as p:\r\n            latency = run_perf_test(next_token_logits)\r\n        print(p.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))\r\n    else:\r\n        latency = run_perf_test(next_token_logits)\r\n\r\n    print(\r\n        f\"use_ort_in_loop={use_ort_in_loop}, disable_ort_spinning={disable_ort_spinning} ort_threads={num_thread_ort}, torch_threads={num_thread_torch}, operation took {sum(latency) / n_loop * 1000:.1f} ms over {n_loop} runs\"\r\n    )\r\n\r\n\r\nwith torch.no_grad():\r\n    cpu_count = psutil.cpu_count(logical=False)\r\n    for disable_spinning in [False, True]:\r\n        test(True, int(cpu_count), int(cpu_count), disable_ort_spinning=disable_spinning)\r\n        test(True, int(cpu_count / 2), int(cpu_count / 2), disable_ort_spinning=disable_spinning)\r\n        test(False, int(cpu_count), int(cpu_count), disable_ort_spinning=disable_spinning)\r\n        test(False, int(cpu_count / 2), int(cpu_count / 2), disable_ort_spinning=disable_spinning)\r\n```\r\n\r\nThe output:\r\n```\r\nuse_ort_in_loop=True, ort_threads=4, torch_threads=4, operation took 21.24567747116089 ms over 100 runs\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                   aten::mm        90.60%        2.123s        90.60%        2.123s      21.227ms           100\r\n      cudaDeviceSynchronize         9.13%     214.017ms         9.13%     214.017ms     214.017ms             1\r\n    cudaGetDeviceProperties         0.09%       2.170ms         0.09%       2.170ms       2.170ms             1\r\n                aten::slice         0.07%       1.638ms         0.08%       1.962ms       9.810us           200\r\n               aten::select         0.03%     737.000us         0.04%     837.000us       8.370us           100\r\n               aten::matmul         0.03%     621.000us        90.62%        2.123s      21.233ms           100\r\n           aten::lift_fresh         0.02%     504.000us         0.02%     504.000us       5.040us           100\r\n           aten::as_strided         0.02%     424.000us         0.02%     424.000us       1.413us           300\r\n         cudaGetDeviceCount         0.01%     197.000us         0.01%     197.000us     197.000us             1\r\n         aten::resolve_conj         0.00%       6.000us         0.00%       6.000us       0.020us           300\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 2.343s\r\n\r\nSTAGE:2022-12-01 23:38:03 2962600:2962600 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 23:38:05 2962600:2962600 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=True, ort_threads=2, torch_threads=2, operation took 0.32085180282592773 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        89.22%      30.525ms        89.22%      30.526ms     305.260us           100\r\n              aten::slice         4.70%       1.609ms         5.64%       1.928ms       9.640us           200\r\n             aten::select         2.00%     685.000us         2.31%     790.000us       7.900us           100\r\n             aten::matmul         1.48%     507.000us        90.71%      31.033ms     310.330us           100\r\n         aten::lift_fresh         1.31%     447.000us         1.31%     447.000us       4.470us           100\r\n         aten::as_strided         1.24%     424.000us         1.24%     424.000us       1.413us           300\r\n    cudaDeviceSynchronize         0.04%      15.000us         0.04%      15.000us      15.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 34.213ms\r\n\r\nSTAGE:2022-12-01 23:38:21 2962600:2962600 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 23:38:21 2962600:2962600 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=False, ort_threads=4, torch_threads=4, operation took 0.8952760696411133 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        99.33%      89.091ms        99.33%      89.092ms     890.920us           100\r\n             aten::matmul         0.65%     587.000us        99.46%      89.203ms     892.030us           100\r\n    cudaDeviceSynchronize         0.01%      12.000us         0.01%      12.000us      12.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 89.691ms\r\n\r\nSTAGE:2022-12-01 23:38:38 2962600:2962600 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 23:38:38 2962600:2962600 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=False, ort_threads=2, torch_threads=2, operation took 0.2782249450683594 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        96.15%      27.365ms        96.15%      27.366ms     273.660us           100\r\n             aten::matmul         3.81%       1.084ms        96.57%      27.485ms     274.850us           100\r\n    cudaDeviceSynchronize         0.04%      12.000us         0.04%      12.000us      12.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 28.462ms\r\n```\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334579176/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "id": 7936482095,
        "node_id": "LE_lADOCVq1mM5Xs8eezwAAAAHZDRsv",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7936482095",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "labeled",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-01T23:49:12Z",
        "label": {
            "name": "core runtime",
            "color": "006B75"
        },
        "performed_via_github_app": null
    },
    {
        "id": 7936483543,
        "node_id": "UNLE_lADOCVq1mM5Xs8eezwAAAAHZDSDX",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7936483543",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "unlabeled",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-01T23:49:25Z",
        "label": {
            "name": "model:transformer",
            "color": "4EF6CD"
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334895190",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334895190",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334895190,
        "node_id": "IC_kwDOCVq1mM5PkOJW",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-02T08:19:20Z",
        "updated_at": "2022-12-02T08:19:20Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks a lot for the followup and better profiling! Do you see it as being a bug/issue in ONNX Runtime?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334895190/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1335867352",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1335867352",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1335867352,
        "node_id": "IC_kwDOCVq1mM5Pn7fY",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-02T21:37:55Z",
        "updated_at": "2022-12-02T21:37:55Z",
        "author_association": "MEMBER",
        "body": "I do not think it is a bug. It is expected that when total threads of a process are larger than number of cpu cores, the performance will be impacted due to thread context switch.\r\n\r\nTo achieve better performance, I suggest experiments on thread setting of ORT and PyTorch & Numpy.\r\nhttps://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html\r\nhttps://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy\r\n\r\nORT python API has two settings (inter_op_num_threads and intra_op_num_threads):\r\nhttps://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.SessionOptions.inter_op_num_threads\r\n\r\n@pranavsharma, any suggestions on this topic?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1335867352/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "id": 7943973853,
        "node_id": "MEE_lADOCVq1mM5Xs8eezwAAAAHZf2vd",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7943973853",
        "actor": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "mentioned",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-02T21:37:55Z",
        "performed_via_github_app": null
    },
    {
        "id": 7943973862,
        "node_id": "SE_lADOCVq1mM5Xs8eezwAAAAHZf2vm",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7943973862",
        "actor": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "subscribed",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-02T21:37:55Z",
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336007732",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336007732",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336007732,
        "node_id": "IC_kwDOCVq1mM5Podw0",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T01:51:55Z",
        "updated_at": "2022-12-03T05:12:08Z",
        "author_association": "MEMBER",
        "body": "Couple of options:\r\n\r\n1. Is it possible to delete the session object once you're done with inferencing by calling ```del session```? That'll cause the session and hence the ORT threadpool to be destructed.\r\n2. Another option is to pin pytorch and ORT threads to different cores, but we don't support that in ORT today (it's work in progress).\r\n3. You can also try by turning off spinning using [this config](https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/session/onnxruntime_session_options_config_keys.h#L88).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336007732/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336109177",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336109177",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336109177,
        "node_id": "IC_kwDOCVq1mM5Po2h5",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T08:01:38Z",
        "updated_at": "2022-12-03T16:24:53Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks for your answers and suggestion! Will try it out. Will check if PyTorch + Numpy have the same issue or not.\r\n\r\nDoes this mean that torchdynamo + ort as execution backend will be very bad (by default) on CPU in case the whole graph is not traced (edit: spoiler: IMO yes ðŸ˜…)? Or does dynamo works differently than spawning InferenceSessions?\r\n\r\n```\r\nimport torch._dynamo as dynamo\r\n\r\ndynamo_model = dynamo.optimize(\"onnxrt\")(model)\r\n```\r\n\r\nRelevant torchdynamo backend: https://github.com/pytorch/pytorch/blob/1ee189ce8ebf392b4a9c026f040b14f7145ca5e6/torch/_dynamo/optimizations/backends.py#L135-L176 (looks weird, there's IOBinding even on CPU @ezyang)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336109177/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T08:23:29Z",
        "updated_at": "2022-12-03T08:23:29Z",
        "source": {
            "type": "issue",
            "issue": {
                "url": "https://api.github.com/repos/huggingface/optimum/issues/526",
                "repository_url": "https://api.github.com/repos/huggingface/optimum",
                "labels_url": "https://api.github.com/repos/huggingface/optimum/issues/526/labels{/name}",
                "comments_url": "https://api.github.com/repos/huggingface/optimum/issues/526/comments",
                "events_url": "https://api.github.com/repos/huggingface/optimum/issues/526/events",
                "html_url": "https://github.com/huggingface/optimum/issues/526",
                "id": 1468153562,
                "node_id": "I_kwDOFx0ogc5Xgj7a",
                "number": 526,
                "title": "Make ORTModel PyTorch free",
                "user": {
                    "login": "fxmarty",
                    "id": 9808326,
                    "node_id": "MDQ6VXNlcjk4MDgzMjY=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/fxmarty",
                    "html_url": "https://github.com/fxmarty",
                    "followers_url": "https://api.github.com/users/fxmarty/followers",
                    "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
                    "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
                    "organizations_url": "https://api.github.com/users/fxmarty/orgs",
                    "repos_url": "https://api.github.com/users/fxmarty/repos",
                    "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/fxmarty/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 1,
                "created_at": "2022-11-29T14:24:04Z",
                "updated_at": "2022-12-06T17:45:03Z",
                "closed_at": null,
                "author_association": "CONTRIBUTOR",
                "active_lock_reason": null,
                "repository": {
                    "id": 387786881,
                    "node_id": "MDEwOlJlcG9zaXRvcnkzODc3ODY4ODE=",
                    "name": "optimum",
                    "full_name": "huggingface/optimum",
                    "private": false,
                    "owner": {
                        "login": "huggingface",
                        "id": 25720743,
                        "node_id": "MDEyOk9yZ2FuaXphdGlvbjI1NzIwNzQz",
                        "avatar_url": "https://avatars.githubusercontent.com/u/25720743?v=4",
                        "gravatar_id": "",
                        "url": "https://api.github.com/users/huggingface",
                        "html_url": "https://github.com/huggingface",
                        "followers_url": "https://api.github.com/users/huggingface/followers",
                        "following_url": "https://api.github.com/users/huggingface/following{/other_user}",
                        "gists_url": "https://api.github.com/users/huggingface/gists{/gist_id}",
                        "starred_url": "https://api.github.com/users/huggingface/starred{/owner}{/repo}",
                        "subscriptions_url": "https://api.github.com/users/huggingface/subscriptions",
                        "organizations_url": "https://api.github.com/users/huggingface/orgs",
                        "repos_url": "https://api.github.com/users/huggingface/repos",
                        "events_url": "https://api.github.com/users/huggingface/events{/privacy}",
                        "received_events_url": "https://api.github.com/users/huggingface/received_events",
                        "type": "Organization",
                        "site_admin": false
                    },
                    "html_url": "https://github.com/huggingface/optimum",
                    "description": "ðŸŽï¸ Accelerate training and inference of ðŸ¤— Transformers with easy to use hardware optimization tools",
                    "fork": false,
                    "url": "https://api.github.com/repos/huggingface/optimum",
                    "forks_url": "https://api.github.com/repos/huggingface/optimum/forks",
                    "keys_url": "https://api.github.com/repos/huggingface/optimum/keys{/key_id}",
                    "collaborators_url": "https://api.github.com/repos/huggingface/optimum/collaborators{/collaborator}",
                    "teams_url": "https://api.github.com/repos/huggingface/optimum/teams",
                    "hooks_url": "https://api.github.com/repos/huggingface/optimum/hooks",
                    "issue_events_url": "https://api.github.com/repos/huggingface/optimum/issues/events{/number}",
                    "events_url": "https://api.github.com/repos/huggingface/optimum/events",
                    "assignees_url": "https://api.github.com/repos/huggingface/optimum/assignees{/user}",
                    "branches_url": "https://api.github.com/repos/huggingface/optimum/branches{/branch}",
                    "tags_url": "https://api.github.com/repos/huggingface/optimum/tags",
                    "blobs_url": "https://api.github.com/repos/huggingface/optimum/git/blobs{/sha}",
                    "git_tags_url": "https://api.github.com/repos/huggingface/optimum/git/tags{/sha}",
                    "git_refs_url": "https://api.github.com/repos/huggingface/optimum/git/refs{/sha}",
                    "trees_url": "https://api.github.com/repos/huggingface/optimum/git/trees{/sha}",
                    "statuses_url": "https://api.github.com/repos/huggingface/optimum/statuses/{sha}",
                    "languages_url": "https://api.github.com/repos/huggingface/optimum/languages",
                    "stargazers_url": "https://api.github.com/repos/huggingface/optimum/stargazers",
                    "contributors_url": "https://api.github.com/repos/huggingface/optimum/contributors",
                    "subscribers_url": "https://api.github.com/repos/huggingface/optimum/subscribers",
                    "subscription_url": "https://api.github.com/repos/huggingface/optimum/subscription",
                    "commits_url": "https://api.github.com/repos/huggingface/optimum/commits{/sha}",
                    "git_commits_url": "https://api.github.com/repos/huggingface/optimum/git/commits{/sha}",
                    "comments_url": "https://api.github.com/repos/huggingface/optimum/comments{/number}",
                    "issue_comment_url": "https://api.github.com/repos/huggingface/optimum/issues/comments{/number}",
                    "contents_url": "https://api.github.com/repos/huggingface/optimum/contents/{+path}",
                    "compare_url": "https://api.github.com/repos/huggingface/optimum/compare/{base}...{head}",
                    "merges_url": "https://api.github.com/repos/huggingface/optimum/merges",
                    "archive_url": "https://api.github.com/repos/huggingface/optimum/{archive_format}{/ref}",
                    "downloads_url": "https://api.github.com/repos/huggingface/optimum/downloads",
                    "issues_url": "https://api.github.com/repos/huggingface/optimum/issues{/number}",
                    "pulls_url": "https://api.github.com/repos/huggingface/optimum/pulls{/number}",
                    "milestones_url": "https://api.github.com/repos/huggingface/optimum/milestones{/number}",
                    "notifications_url": "https://api.github.com/repos/huggingface/optimum/notifications{?since,all,participating}",
                    "labels_url": "https://api.github.com/repos/huggingface/optimum/labels{/name}",
                    "releases_url": "https://api.github.com/repos/huggingface/optimum/releases{/id}",
                    "deployments_url": "https://api.github.com/repos/huggingface/optimum/deployments",
                    "created_at": "2021-07-20T12:36:40Z",
                    "updated_at": "2022-12-20T17:43:08Z",
                    "pushed_at": "2022-12-20T17:37:52Z",
                    "git_url": "git://github.com/huggingface/optimum.git",
                    "ssh_url": "git@github.com:huggingface/optimum.git",
                    "clone_url": "https://github.com/huggingface/optimum.git",
                    "svn_url": "https://github.com/huggingface/optimum",
                    "homepage": "https://huggingface.co/docs/optimum/",
                    "size": 2376,
                    "stargazers_count": 831,
                    "watchers_count": 831,
                    "language": "Python",
                    "has_issues": true,
                    "has_projects": true,
                    "has_downloads": true,
                    "has_wiki": true,
                    "has_pages": false,
                    "has_discussions": false,
                    "forks_count": 105,
                    "mirror_url": null,
                    "archived": false,
                    "disabled": false,
                    "open_issues_count": 93,
                    "license": {
                        "key": "apache-2.0",
                        "name": "Apache License 2.0",
                        "spdx_id": "Apache-2.0",
                        "url": "https://api.github.com/licenses/apache-2.0",
                        "node_id": "MDc6TGljZW5zZTI="
                    },
                    "allow_forking": true,
                    "is_template": false,
                    "web_commit_signoff_required": false,
                    "topics": [
                        "graphcore",
                        "habana",
                        "inference",
                        "intel",
                        "onnx",
                        "onnxruntime",
                        "optimization",
                        "pytorch",
                        "quantization",
                        "training",
                        "transformers"
                    ],
                    "visibility": "public",
                    "forks": 105,
                    "open_issues": 93,
                    "watchers": 831,
                    "default_branch": "main",
                    "permissions": {
                        "admin": false,
                        "maintain": false,
                        "push": false,
                        "triage": false,
                        "pull": true
                    }
                },
                "body": "### Feature request\r\n\r\nCurrently, ORTModel has a hard dependency on `torch` and `transformers`. Could we make it such that this dependency is soft, and that Optimum + ORTModel can be used without PyTorch? Would this be useful and elegant?\r\n\r\n### Motivation\r\n\r\nOne of the reason ONNX Runtime is nice is that it is much lighter than PyTorch. By forcing the dependency on PyTorch, one of the advantages of ONNX Runtime is lost.\r\n\r\nOne difficulty is that encoder-decoder models use `generate()` from transformers.\r\n\r\n### Your contribution\r\n\r\nNone ATM, just an idea I have in mind, not sure it makes sense to commit to this.",
                "reactions": {
                    "url": "https://api.github.com/repos/huggingface/optimum/issues/526/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/huggingface/optimum/issues/526/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "event": "cross-referenced"
    },
    {
        "id": 7946600579,
        "node_id": "MEE_lADOCVq1mM5Xs8eezwAAAAHZp4CD",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7946600579",
        "actor": {
            "login": "ezyang",
            "id": 13564,
            "node_id": "MDQ6VXNlcjEzNTY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/13564?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ezyang",
            "html_url": "https://github.com/ezyang",
            "followers_url": "https://api.github.com/users/ezyang/followers",
            "following_url": "https://api.github.com/users/ezyang/following{/other_user}",
            "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions",
            "organizations_url": "https://api.github.com/users/ezyang/orgs",
            "repos_url": "https://api.github.com/users/ezyang/repos",
            "events_url": "https://api.github.com/users/ezyang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ezyang/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "mentioned",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-03T16:21:49Z",
        "performed_via_github_app": null
    },
    {
        "id": 7946600582,
        "node_id": "SE_lADOCVq1mM5Xs8eezwAAAAHZp4CG",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7946600582",
        "actor": {
            "login": "ezyang",
            "id": 13564,
            "node_id": "MDQ6VXNlcjEzNTY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/13564?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ezyang",
            "html_url": "https://github.com/ezyang",
            "followers_url": "https://api.github.com/users/ezyang/followers",
            "following_url": "https://api.github.com/users/ezyang/following{/other_user}",
            "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions",
            "organizations_url": "https://api.github.com/users/ezyang/orgs",
            "repos_url": "https://api.github.com/users/ezyang/repos",
            "events_url": "https://api.github.com/users/ezyang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ezyang/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "subscribed",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-03T16:21:49Z",
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219034",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336219034",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336219034,
        "node_id": "IC_kwDOCVq1mM5PpRWa",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T18:58:46Z",
        "updated_at": "2022-12-03T19:12:17Z",
        "author_association": "MEMBER",
        "body": "@fxmarty,  add the following setting seems help:\r\n```\r\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\r\n```\r\n\r\nUpdated the previous script inline. Here is the result:\r\n```\r\nuse_ort_in_loop=True, disable_ort_spinning=False ort_threads=4, torch_threads=4, operation took 23.15556049346924 ms over 100 runs\r\nuse_ort_in_loop=True, disable_ort_spinning=False ort_threads=2, torch_threads=2, operation took 4.832360744476318 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=False ort_threads=4, torch_threads=4, operation took 0.7350826263427734 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=False ort_threads=2, torch_threads=2, operation took 0.24795770645141602 ms over 100 runs\r\nuse_ort_in_loop=True, disable_ort_spinning=True ort_threads=4, torch_threads=4, operation took 0.8697772026062012 ms over 100 runs\r\nuse_ort_in_loop=True, disable_ort_spinning=True ort_threads=2, torch_threads=2, operation took 0.33701181411743164 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=True ort_threads=4, torch_threads=4, operation took 0.7239651679992676 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=True ort_threads=2, torch_threads=2, operation took 0.24130821228027344 ms over 100 runs\r\n\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219034/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "id": 7946779874,
        "node_id": "MEE_lADOCVq1mM5Xs8eezwAAAAHZqjzi",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7946779874",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "mentioned",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-03T18:58:46Z",
        "performed_via_github_app": null
    },
    {
        "id": 7946779877,
        "node_id": "SE_lADOCVq1mM5Xs8eezwAAAAHZqjzl",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7946779877",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "subscribed",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-03T18:58:46Z",
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219493",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336219493",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336219493,
        "node_id": "IC_kwDOCVq1mM5PpRdl",
        "user": {
            "login": "ezyang",
            "id": 13564,
            "node_id": "MDQ6VXNlcjEzNTY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/13564?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ezyang",
            "html_url": "https://github.com/ezyang",
            "followers_url": "https://api.github.com/users/ezyang/followers",
            "following_url": "https://api.github.com/users/ezyang/following{/other_user}",
            "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions",
            "organizations_url": "https://api.github.com/users/ezyang/orgs",
            "repos_url": "https://api.github.com/users/ezyang/repos",
            "events_url": "https://api.github.com/users/ezyang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ezyang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T19:01:10Z",
        "updated_at": "2022-12-03T19:01:10Z",
        "author_association": "NONE",
        "body": "We haven't really heavily tested the ORT backend so there may be flagrant performance problems for no good reason. One thing is that you should imagine what the performance of your program would be if you replaced your model with several calls into ORT backend. That should give a sense for what the perf is",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219493/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "ezyang",
            "id": 13564,
            "node_id": "MDQ6VXNlcjEzNTY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/13564?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ezyang",
            "html_url": "https://github.com/ezyang",
            "followers_url": "https://api.github.com/users/ezyang/followers",
            "following_url": "https://api.github.com/users/ezyang/following{/other_user}",
            "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions",
            "organizations_url": "https://api.github.com/users/ezyang/orgs",
            "repos_url": "https://api.github.com/users/ezyang/repos",
            "events_url": "https://api.github.com/users/ezyang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ezyang/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1337177148",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1337177148",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1337177148,
        "node_id": "IC_kwDOCVq1mM5Ps7Q8",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-05T11:28:59Z",
        "updated_at": "2022-12-05T11:33:38Z",
        "author_association": "CONTRIBUTOR",
        "body": "Here's an updated script:\r\n\r\n<details>\r\n  <summary>Script</summary>\r\n\r\n```python\r\nimport time\r\n\r\nimport onnxruntime as ort\r\nimport psutil\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\ndef test(\r\n    use_ort_in_loop,\r\n    use_pt_in_loop,\r\n    num_thread_ort,\r\n    num_thread_torch,\r\n    contiguous=True,\r\n    n_loop=200,\r\n    disable_ort_spinning=False,\r\n    profiling=True,\r\n):\r\n    batch_size = 4\r\n    sequence_length = 32\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\r\n\r\n    inp = {\r\n        \"input_ids\": torch.randint(tokenizer.vocab_size - 1, (batch_size, 5), dtype=torch.int64),\r\n        \"encoder_attention_mask\": torch.ones(batch_size, sequence_length, dtype=torch.int64),\r\n        \"encoder_hidden_states\": torch.rand((batch_size, sequence_length, 512), dtype=torch.float32),\r\n    }\r\n\r\n    ort_inp = {}\r\n    for key in inp:\r\n        ort_inp[key] = inp[key].detach().cpu().numpy()\r\n\r\n    sess_options = ort.SessionOptions()\r\n    sess_options.intra_op_num_threads = num_thread_ort\r\n    if disable_ort_spinning:\r\n        sess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\r\n\r\n    session = ort.InferenceSession(\"decoder_model.onnx\", sess_options=sess_options, providers=[\"CPUExecutionProvider\"])\r\n\r\n    M = torch.zeros(32128, 100)\r\n\r\n    def get_next_token_logits(ort_inp):\r\n        res = session.run(None, ort_inp)\r\n        next_token_logits = torch.from_numpy(res[0])\r\n        return next_token_logits.contiguous() if contiguous else next_token_logits\r\n\r\n    next_token_logits = get_next_token_logits(ort_inp)\r\n    time.sleep(10)\r\n\r\n    torch.set_num_threads(num_thread_torch)\r\n\r\n    def run_perf_test(next_token_logits):\r\n        latency = [0]\r\n        ort_latency = [0]\r\n        for _ in range(n_loop):\r\n            if use_ort_in_loop:\r\n                start = time.time()\r\n                next_token_logits = get_next_token_logits(ort_inp)\r\n                ort_latency += [time.time() - start]\r\n            if use_pt_in_loop:\r\n                start = time.time()\r\n                torch.matmul(next_token_logits, M)\r\n                latency += [time.time() - start]\r\n        return latency, ort_latency\r\n\r\n    if profiling:\r\n        with torch.profiler.profile(\r\n            activities=[\r\n                torch.profiler.ProfilerActivity.CPU,\r\n                torch.profiler.ProfilerActivity.CUDA,\r\n            ]\r\n        ) as p:\r\n            latency, ort_latency = run_perf_test(next_token_logits)\r\n        print(p.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))\r\n    else:\r\n        latency, ort_latency = run_perf_test(next_token_logits)\r\n\r\n    print(\r\n        f\"use_ort={use_ort_in_loop}, use_pt={use_pt_in_loop}, disable_ort_spinning={disable_ort_spinning} ort_threads={num_thread_ort}, torch_threads={num_thread_torch}, PT operation took {sum(latency) * 1e3:.1f} ms, ORT operation took {sum(ort_latency) * 1e3:.1f} ms over {n_loop} runs\"\r\n    )\r\n    print(\"------------\")\r\n\r\n\r\nwith torch.no_grad():\r\n    cpu_count = psutil.cpu_count(logical=False)\r\n\r\n    for disable_spinning in [False, True]:\r\n        test(True, True, int(cpu_count), int(cpu_count), disable_ort_spinning=disable_spinning)\r\n        test(True, True, int(cpu_count) // 2, int(cpu_count) // 2, disable_ort_spinning=disable_spinning)\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n  <summary>Results on AWS EC2 c6i instance (with a piece of Platinum 8375C CPU, 4 physical cores)</summary>\r\n\r\n```\r\nSTAGE:2022-12-05 11:21:41 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:21:43 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        97.13%     242.705ms        97.13%     242.709ms       1.214ms           200  \r\n            aten::matmul         1.04%       2.611ms        99.72%     249.184ms       1.246ms           200  \r\n           aten::reshape         0.64%       1.597ms         1.08%       2.700ms      13.500us           200  \r\n    aten::_reshape_alias         0.49%       1.228ms         0.49%       1.228ms       6.140us           200  \r\n      aten::_unsafe_view         0.42%       1.039ms         0.42%       1.039ms       5.195us           200  \r\n        aten::lift_fresh         0.28%     701.000us         0.28%     701.000us       3.505us           200  \r\n      aten::resolve_conj         0.00%       4.000us         0.00%       4.000us       0.007us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 249.885ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=4, torch_threads=4, PT operation took 251.7 ms, ORT operation took 2005.3 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 11:21:53 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:21:57 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        98.37%     459.505ms        98.37%     459.507ms       2.298ms           200  \r\n            aten::matmul         0.60%       2.815ms        99.83%     466.335ms       2.332ms           200  \r\n           aten::reshape         0.35%       1.627ms         0.61%       2.843ms      14.215us           200  \r\n    aten::_reshape_alias         0.29%       1.333ms         0.29%       1.333ms       6.665us           200  \r\n      aten::_unsafe_view         0.23%       1.053ms         0.23%       1.053ms       5.265us           200  \r\n        aten::lift_fresh         0.17%     776.000us         0.17%     776.000us       3.880us           200  \r\n      aten::resolve_conj         0.00%       2.000us         0.00%       2.000us       0.003us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 467.111ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=2, torch_threads=2, PT operation took 469.0 ms, ORT operation took 3139.4 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 11:22:08 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:22:11 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        96.50%     199.427ms        96.50%     199.431ms     997.155us           200  \r\n            aten::matmul         1.22%       2.513ms        99.59%     205.809ms       1.029ms           200  \r\n           aten::reshape         0.77%       1.592ms         1.41%       2.905ms      14.525us           200  \r\n    aten::_reshape_alias         0.66%       1.369ms         0.66%       1.369ms       6.845us           200  \r\n      aten::_unsafe_view         0.44%     904.000us         0.44%     904.000us       4.520us           200  \r\n        aten::lift_fresh         0.41%     846.000us         0.41%     846.000us       4.230us           200  \r\n      aten::resolve_conj         0.00%       4.000us         0.00%       4.000us       0.007us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 206.655ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=4, torch_threads=4, PT operation took 208.3 ms, ORT operation took 2891.3 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 11:22:21 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:22:25 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        97.57%     282.228ms        97.57%     282.231ms       1.411ms           200  \r\n            aten::matmul         0.88%       2.538ms        99.74%     288.486ms       1.442ms           200  \r\n           aten::reshape         0.54%       1.575ms         0.94%       2.722ms      13.610us           200  \r\n    aten::_reshape_alias         0.42%       1.227ms         0.42%       1.227ms       6.135us           200  \r\n      aten::_unsafe_view         0.32%     915.000us         0.32%     915.000us       4.575us           200  \r\n        aten::lift_fresh         0.26%     765.000us         0.26%     765.000us       3.825us           200  \r\n      aten::resolve_conj         0.00%       3.000us         0.00%       3.000us       0.005us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 289.251ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=2, torch_threads=2, PT operation took 291.0 ms, ORT operation took 3259.3 ms over 200 runs\r\n------------\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n  <summary>Results on my laptop (with i7-1280P, that has both E-cores and P-cores)</summary>\r\n\r\n```\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                   aten::mm        97.80%        6.796s        97.80%        6.796s      33.979ms           200  \r\n      cudaDeviceSynchronize         1.74%     121.127ms         1.74%     121.127ms     121.127ms             1  \r\n               aten::matmul         0.36%      25.196ms        98.24%        6.826s      34.132ms           200  \r\n              aten::reshape         0.03%       2.187ms         0.06%       3.918ms      19.590us           200  \r\n       aten::_reshape_alias         0.03%       1.744ms         0.03%       1.744ms       8.720us           200  \r\n         aten::_unsafe_view         0.02%       1.420ms         0.02%       1.420ms       7.100us           200  \r\n           aten::lift_fresh         0.01%       1.006ms         0.01%       1.006ms       5.030us           200  \r\n         cudaGetDeviceCount         0.00%     163.000us         0.00%     163.000us     163.000us             1  \r\n    cudaGetDeviceProperties         0.00%      95.000us         0.00%      95.000us      95.000us             1  \r\n         aten::resolve_conj         0.00%      36.000us         0.00%      36.000us       0.060us           600  \r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 6.949s\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=14, torch_threads=14, PT operation took 6830.3 ms, ORT operation took 2642.2 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 12:19:48 22172:22172 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 12:19:51 22172:22172 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                 aten::mm        95.21%     193.453ms        95.22%     193.474ms     967.370us           200  \r\n             aten::matmul         1.85%       3.758ms        99.47%     202.094ms       1.010ms           200  \r\n            aten::reshape         0.99%       2.003ms         1.81%       3.674ms      18.370us           200  \r\n     aten::_reshape_alias         0.84%       1.698ms         0.84%       1.698ms       8.490us           200  \r\n       aten::_unsafe_view         0.57%       1.161ms         0.57%       1.161ms       5.805us           200  \r\n         aten::lift_fresh         0.53%       1.072ms         0.53%       1.072ms       5.360us           200  \r\n       aten::resolve_conj         0.01%      21.000us         0.01%      21.000us       0.035us           600  \r\n    cudaDeviceSynchronize         0.01%      14.000us         0.01%      14.000us      14.000us             1  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 203.180ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=7, torch_threads=7, PT operation took 205.2 ms, ORT operation took 2778.5 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 12:20:03 22172:22172 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 12:20:05 22172:22172 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                 aten::mm        98.37%     244.519ms        98.37%     244.522ms       1.223ms           200  \r\n             aten::matmul         0.64%       1.580ms        99.85%     248.196ms       1.241ms           200  \r\n            aten::reshape         0.41%       1.018ms         0.68%       1.680ms       8.400us           200  \r\n     aten::_reshape_alias         0.28%     696.000us         0.28%     696.000us       3.480us           200  \r\n       aten::_unsafe_view         0.15%     380.000us         0.15%     380.000us       1.900us           200  \r\n         aten::lift_fresh         0.15%     361.000us         0.15%     361.000us       1.805us           200  \r\n    cudaDeviceSynchronize         0.00%      11.000us         0.00%      11.000us      11.000us             1  \r\n       aten::resolve_conj         0.00%       3.000us         0.00%       3.000us       0.005us           600  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 248.568ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=14, torch_threads=14, PT operation took 249.8 ms, ORT operation took 2117.6 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 12:20:17 22172:22172 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 12:20:20 22172:22172 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                 aten::mm        98.12%     175.821ms        98.12%     175.822ms     879.110us           200  \r\n             aten::matmul         0.83%       1.492ms        99.87%     178.952ms     894.760us           200  \r\n            aten::reshape         0.44%     787.000us         0.70%       1.263ms       6.315us           200  \r\n     aten::_reshape_alias         0.30%     529.000us         0.30%     529.000us       2.645us           200  \r\n       aten::_unsafe_view         0.18%     322.000us         0.18%     322.000us       1.610us           200  \r\n         aten::lift_fresh         0.13%     229.000us         0.13%     229.000us       1.145us           200  \r\n    cudaDeviceSynchronize         0.01%      11.000us         0.01%      11.000us      11.000us             1  \r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.002us           600  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 179.192ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=7, torch_threads=7, PT operation took 180.4 ms, ORT operation took 2496.5 ms over 200 runs\r\n------------\r\n```\r\n</details>\r\n\r\nTL;DR, for this specific model and specific pytorch operation:\r\n* On the c6i instance, I have no issue of 50x slowdown when mixing ORT + PyTorch to start with\r\n* Disabling ORT thread spinning hurts ORT latency on the c6i instance, but not on my laptop\r\n* On the c6i instance, passing from 4 threads to 2 threads hurt both ORT and PT perfs \r\n* On my laptop, passing from 4 threads to 2 threads does not hurt ORT perfs",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1337177148/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1339040093",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1339040093",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1339040093,
        "node_id": "IC_kwDOCVq1mM5P0CFd",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-06T09:39:54Z",
        "updated_at": "2022-12-08T19:06:09Z",
        "author_association": "CONTRIBUTOR",
        "body": "@tianleiwu On which hardware/CPU did you reproduce?\r\n\r\nNote: the issue can be reproduced on Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1339040093/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null,
        "event": "commented",
        "actor": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        }
    },
    {
        "id": 7964478453,
        "node_id": "MEE_lADOCVq1mM5Xs8eezwAAAAHauEv1",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7964478453",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "mentioned",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-06T09:39:55Z",
        "performed_via_github_app": null
    },
    {
        "id": 7964478461,
        "node_id": "SE_lADOCVq1mM5Xs8eezwAAAAHauEv9",
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/events/7964478461",
        "actor": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "event": "subscribed",
        "commit_id": null,
        "commit_url": null,
        "created_at": "2022-12-06T09:39:55Z",
        "performed_via_github_app": null
    },
    {
        "actor": {
            "login": "mht-sharma",
            "id": 21088122,
            "node_id": "MDQ6VXNlcjIxMDg4MTIy",
            "avatar_url": "https://avatars.githubusercontent.com/u/21088122?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mht-sharma",
            "html_url": "https://github.com/mht-sharma",
            "followers_url": "https://api.github.com/users/mht-sharma/followers",
            "following_url": "https://api.github.com/users/mht-sharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/mht-sharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mht-sharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mht-sharma/subscriptions",
            "organizations_url": "https://api.github.com/users/mht-sharma/orgs",
            "repos_url": "https://api.github.com/users/mht-sharma/repos",
            "events_url": "https://api.github.com/users/mht-sharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mht-sharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-19T08:46:05Z",
        "updated_at": "2022-12-19T08:46:05Z",
        "source": {
            "type": "issue",
            "issue": {
                "url": "https://api.github.com/repos/huggingface/transformers/issues/20644",
                "repository_url": "https://api.github.com/repos/huggingface/transformers",
                "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/20644/labels{/name}",
                "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/20644/comments",
                "events_url": "https://api.github.com/repos/huggingface/transformers/issues/20644/events",
                "html_url": "https://github.com/huggingface/transformers/issues/20644",
                "id": 1481450965,
                "node_id": "I_kwDOCUB6oc5YTSXV",
                "number": 20644,
                "title": "ONNX encoder decoder exchange invoke issue",
                "user": {
                    "login": "umanniyaz",
                    "id": 33204214,
                    "node_id": "MDQ6VXNlcjMzMjA0MjE0",
                    "avatar_url": "https://avatars.githubusercontent.com/u/33204214?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/umanniyaz",
                    "html_url": "https://github.com/umanniyaz",
                    "followers_url": "https://api.github.com/users/umanniyaz/followers",
                    "following_url": "https://api.github.com/users/umanniyaz/following{/other_user}",
                    "gists_url": "https://api.github.com/users/umanniyaz/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/umanniyaz/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/umanniyaz/subscriptions",
                    "organizations_url": "https://api.github.com/users/umanniyaz/orgs",
                    "repos_url": "https://api.github.com/users/umanniyaz/repos",
                    "events_url": "https://api.github.com/users/umanniyaz/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/umanniyaz/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": {
                    "login": "mht-sharma",
                    "id": 21088122,
                    "node_id": "MDQ6VXNlcjIxMDg4MTIy",
                    "avatar_url": "https://avatars.githubusercontent.com/u/21088122?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/mht-sharma",
                    "html_url": "https://github.com/mht-sharma",
                    "followers_url": "https://api.github.com/users/mht-sharma/followers",
                    "following_url": "https://api.github.com/users/mht-sharma/following{/other_user}",
                    "gists_url": "https://api.github.com/users/mht-sharma/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/mht-sharma/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/mht-sharma/subscriptions",
                    "organizations_url": "https://api.github.com/users/mht-sharma/orgs",
                    "repos_url": "https://api.github.com/users/mht-sharma/repos",
                    "events_url": "https://api.github.com/users/mht-sharma/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/mht-sharma/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "assignees": [
                    {
                        "login": "mht-sharma",
                        "id": 21088122,
                        "node_id": "MDQ6VXNlcjIxMDg4MTIy",
                        "avatar_url": "https://avatars.githubusercontent.com/u/21088122?v=4",
                        "gravatar_id": "",
                        "url": "https://api.github.com/users/mht-sharma",
                        "html_url": "https://github.com/mht-sharma",
                        "followers_url": "https://api.github.com/users/mht-sharma/followers",
                        "following_url": "https://api.github.com/users/mht-sharma/following{/other_user}",
                        "gists_url": "https://api.github.com/users/mht-sharma/gists{/gist_id}",
                        "starred_url": "https://api.github.com/users/mht-sharma/starred{/owner}{/repo}",
                        "subscriptions_url": "https://api.github.com/users/mht-sharma/subscriptions",
                        "organizations_url": "https://api.github.com/users/mht-sharma/orgs",
                        "repos_url": "https://api.github.com/users/mht-sharma/repos",
                        "events_url": "https://api.github.com/users/mht-sharma/events{/privacy}",
                        "received_events_url": "https://api.github.com/users/mht-sharma/received_events",
                        "type": "User",
                        "site_admin": false
                    }
                ],
                "milestone": null,
                "comments": 8,
                "created_at": "2022-12-07T09:32:29Z",
                "updated_at": "2022-12-19T09:13:22Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "repository": {
                    "id": 155220641,
                    "node_id": "MDEwOlJlcG9zaXRvcnkxNTUyMjA2NDE=",
                    "name": "transformers",
                    "full_name": "huggingface/transformers",
                    "private": false,
                    "owner": {
                        "login": "huggingface",
                        "id": 25720743,
                        "node_id": "MDEyOk9yZ2FuaXphdGlvbjI1NzIwNzQz",
                        "avatar_url": "https://avatars.githubusercontent.com/u/25720743?v=4",
                        "gravatar_id": "",
                        "url": "https://api.github.com/users/huggingface",
                        "html_url": "https://github.com/huggingface",
                        "followers_url": "https://api.github.com/users/huggingface/followers",
                        "following_url": "https://api.github.com/users/huggingface/following{/other_user}",
                        "gists_url": "https://api.github.com/users/huggingface/gists{/gist_id}",
                        "starred_url": "https://api.github.com/users/huggingface/starred{/owner}{/repo}",
                        "subscriptions_url": "https://api.github.com/users/huggingface/subscriptions",
                        "organizations_url": "https://api.github.com/users/huggingface/orgs",
                        "repos_url": "https://api.github.com/users/huggingface/repos",
                        "events_url": "https://api.github.com/users/huggingface/events{/privacy}",
                        "received_events_url": "https://api.github.com/users/huggingface/received_events",
                        "type": "Organization",
                        "site_admin": false
                    },
                    "html_url": "https://github.com/huggingface/transformers",
                    "description": "ðŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
                    "fork": false,
                    "url": "https://api.github.com/repos/huggingface/transformers",
                    "forks_url": "https://api.github.com/repos/huggingface/transformers/forks",
                    "keys_url": "https://api.github.com/repos/huggingface/transformers/keys{/key_id}",
                    "collaborators_url": "https://api.github.com/repos/huggingface/transformers/collaborators{/collaborator}",
                    "teams_url": "https://api.github.com/repos/huggingface/transformers/teams",
                    "hooks_url": "https://api.github.com/repos/huggingface/transformers/hooks",
                    "issue_events_url": "https://api.github.com/repos/huggingface/transformers/issues/events{/number}",
                    "events_url": "https://api.github.com/repos/huggingface/transformers/events",
                    "assignees_url": "https://api.github.com/repos/huggingface/transformers/assignees{/user}",
                    "branches_url": "https://api.github.com/repos/huggingface/transformers/branches{/branch}",
                    "tags_url": "https://api.github.com/repos/huggingface/transformers/tags",
                    "blobs_url": "https://api.github.com/repos/huggingface/transformers/git/blobs{/sha}",
                    "git_tags_url": "https://api.github.com/repos/huggingface/transformers/git/tags{/sha}",
                    "git_refs_url": "https://api.github.com/repos/huggingface/transformers/git/refs{/sha}",
                    "trees_url": "https://api.github.com/repos/huggingface/transformers/git/trees{/sha}",
                    "statuses_url": "https://api.github.com/repos/huggingface/transformers/statuses/{sha}",
                    "languages_url": "https://api.github.com/repos/huggingface/transformers/languages",
                    "stargazers_url": "https://api.github.com/repos/huggingface/transformers/stargazers",
                    "contributors_url": "https://api.github.com/repos/huggingface/transformers/contributors",
                    "subscribers_url": "https://api.github.com/repos/huggingface/transformers/subscribers",
                    "subscription_url": "https://api.github.com/repos/huggingface/transformers/subscription",
                    "commits_url": "https://api.github.com/repos/huggingface/transformers/commits{/sha}",
                    "git_commits_url": "https://api.github.com/repos/huggingface/transformers/git/commits{/sha}",
                    "comments_url": "https://api.github.com/repos/huggingface/transformers/comments{/number}",
                    "issue_comment_url": "https://api.github.com/repos/huggingface/transformers/issues/comments{/number}",
                    "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/{+path}",
                    "compare_url": "https://api.github.com/repos/huggingface/transformers/compare/{base}...{head}",
                    "merges_url": "https://api.github.com/repos/huggingface/transformers/merges",
                    "archive_url": "https://api.github.com/repos/huggingface/transformers/{archive_format}{/ref}",
                    "downloads_url": "https://api.github.com/repos/huggingface/transformers/downloads",
                    "issues_url": "https://api.github.com/repos/huggingface/transformers/issues{/number}",
                    "pulls_url": "https://api.github.com/repos/huggingface/transformers/pulls{/number}",
                    "milestones_url": "https://api.github.com/repos/huggingface/transformers/milestones{/number}",
                    "notifications_url": "https://api.github.com/repos/huggingface/transformers/notifications{?since,all,participating}",
                    "labels_url": "https://api.github.com/repos/huggingface/transformers/labels{/name}",
                    "releases_url": "https://api.github.com/repos/huggingface/transformers/releases{/id}",
                    "deployments_url": "https://api.github.com/repos/huggingface/transformers/deployments",
                    "created_at": "2018-10-29T13:56:00Z",
                    "updated_at": "2022-12-20T18:14:08Z",
                    "pushed_at": "2022-12-20T18:29:43Z",
                    "git_url": "git://github.com/huggingface/transformers.git",
                    "ssh_url": "git@github.com:huggingface/transformers.git",
                    "clone_url": "https://github.com/huggingface/transformers.git",
                    "svn_url": "https://github.com/huggingface/transformers",
                    "homepage": "https://huggingface.co/transformers",
                    "size": 116570,
                    "stargazers_count": 76530,
                    "watchers_count": 76530,
                    "language": "Python",
                    "has_issues": true,
                    "has_projects": true,
                    "has_downloads": true,
                    "has_wiki": true,
                    "has_pages": false,
                    "has_discussions": false,
                    "forks_count": 17285,
                    "mirror_url": null,
                    "archived": false,
                    "disabled": false,
                    "open_issues_count": 578,
                    "license": {
                        "key": "apache-2.0",
                        "name": "Apache License 2.0",
                        "spdx_id": "Apache-2.0",
                        "url": "https://api.github.com/licenses/apache-2.0",
                        "node_id": "MDc6TGljZW5zZTI="
                    },
                    "allow_forking": true,
                    "is_template": false,
                    "web_commit_signoff_required": false,
                    "topics": [
                        "bert",
                        "deep-learning",
                        "flax",
                        "hacktoberfest",
                        "jax",
                        "language-model",
                        "language-models",
                        "machine-learning",
                        "model-hub",
                        "natural-language-processing",
                        "nlp",
                        "nlp-library",
                        "pretrained-models",
                        "python",
                        "pytorch",
                        "pytorch-transformers",
                        "seq2seq",
                        "speech-recognition",
                        "tensorflow",
                        "transformer"
                    ],
                    "visibility": "public",
                    "forks": 17285,
                    "open_issues": 578,
                    "watchers": 76530,
                    "default_branch": "main",
                    "permissions": {
                        "admin": false,
                        "maintain": false,
                        "push": false,
                        "triage": false,
                        "pull": true
                    }
                },
                "body": "### System Info\r\n\r\nTR-OCR  Model\r\nEncoder - BeiT  -encoder.onnx\r\nDecoder - Roberta large- decoder.onnx\r\n\r\nSystem config:\r\nIntel i9 11 gen\r\nNvidia Quadro RTX 4000 Max Q design - 16GB\r\n\r\nDependencies version: \r\nonnx == 1.12.0\r\nonnx-runtime == 1.13.1\r\ntorch == 1.13.0\r\ntransformers == 4.24.0\r\ntorchvision ==0.14.0\r\n\r\nIssue:\r\nUnable to start ONNX inference sessions with Tr-OCR ONNX conversions-> encoder.onnx & decoder.onnx with **ORTModelForVision2Seq** , model.generate() -> raising this error:\r\n\r\n```\r\nmodel.generate(pixel_values.to('cpu'))\r\n``` \r\n\r\nTraceback (most recent call last):  \r\n```\r\nFile \"<string>\", line 1, in <module>   File \"C:\\Users\\110769\\Anaconda3\\envs\\ocr2\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context     return func(*args, **kwargs)   File \"C:\\Users\\110769\\Anaconda3\\envs\\ocr2\\lib\\site-packages\\transformers\\generation_utils.py\", line 1339, in generate model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation( File \"C:\\Users\\110769\\Anaconda3\\envs\\ocr2\\lib\\site-packages\\transformers\\generation_utils.py\", line 583, in _prepare_encoder_decoder_kwargs_for_generation model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs) File \"C:\\Users\\110769\\Anaconda3\\envs\\ocr2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1188, in _call_impl if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks File \"C:\\Users\\110769\\Anaconda3\\envs\\ocr2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1265, in __getattr__ raise AttributeError(\"'{}' object has no attribute '{}'\".format( AttributeError: 'ORTEncoder' object has no attribute '_backward_hooks'\r\n```\r\n\r\nWhole Code Snippet :\r\n```\r\nclass ORTEncoder(nn.Module):\r\n    \"\"\"\r\n    Encoder model for ONNX Runtime inference.\r\n    Arguments:\r\n        session (`onnxruntime.InferenceSession`):\r\n            The ONNX Runtime inference session associated to the encoder.\r\n    \"\"\"\r\n    def __init__(\r\n        self, session: onnxrt.InferenceSession, device: torch.device, main_input_name: str = \"input_ids\"\r\n    ):\r\n        self.session = session\r\n        self._device = device\r\n        self.main_input_name = main_input_name\r\n        self.input_names = {input_key.name: idx for idx, input_key in enumerate(self.session.get_inputs())}\r\n        self.output_names = {output_key.name: idx for idx, output_key in enumerate(self.session.get_outputs())}\r\nclass ORTDecoder(nn.Module):\r\n    \"\"\"\r\n    Encoder model for ONNX Runtime inference.\r\n    Arguments:\r\n        session (`onnxruntime.InferenceSession`):\r\n            The ONNX Runtime inference session associated to the encoder.\r\n    \"\"\"\r\n    def __init__(\r\n        self, session: onnxrt.InferenceSession, device: torch.device, main_input_name: str = \"input_ids\"\r\n    ):\r\n        self.session = session\r\n        self._device = device\r\n        self.main_input_name = main_input_name\r\n        self.input_names = {input_key.name: idx for idx, input_key in enumerate(self.session.get_inputs())}\r\n        self.output_names = {output_key.name: idx for idx, output_key in enumerate(self.session.get_outputs())}\r\nclass ORTModelForVision2Seq(VisionEncoderDecoderModel):\r\n    def __init__(self, *args, **kwargs):\r\n        config = AutoConfig.from_pretrained('microsoft/trocr-base-printed')\r\n        super().__init__(config)\r\n        self._device = \"cpu\"\r\n        self.encoder = ORTEncoder(onnxrt.InferenceSession(encoder_path,providers=[\"CPUExecutionProvider\"]),device='cpu')\r\n        self.decoder = ORTDecoder(onnxrt.InferenceSession(decoder_path,providers=[\"CPUExecutionProvider\"]),device='cpu')\r\n    def forward(\r\n        self,\r\n        pixel_values: Optional[torch.FloatTensor] = None,\r\n        decoder_input_ids: Optional[torch.LongTensor] = None,\r\n        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\r\n        **kwargs,\r\n    ) -> Seq2SeqLMOutput:\r\n        # Encode if needed : first prediction pass\r\n        if encoder_outputs is None:\r\n            encoder_outputs = self.encoder(pixel_values=pixel_values)\r\n        # Decode\r\n        decoder_attention_mask = decoder_input_ids.new_ones(decoder_input_ids.shape)\r\n        decoder_outputs = self.decoder(\r\n            input_ids=decoder_input_ids,\r\n            attention_mask=decoder_attention_mask,\r\n            encoder_hidden_states=encoder_outputs.last_hidden_state,\r\n        )\r\n        return Seq2SeqLMOutput(\r\n            logits=decoder_outputs.logits,\r\n        )\r\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, encoder_outputs=None, **kwargs):\r\n        return {\r\n            \"decoder_input_ids\": input_ids,\r\n            \"decoder_atttention_mask\": input_ids,\r\n            \"encoder_outputs\": encoder_outputs,\r\n        }\r\nmodel = ORTModelForVision2Seq()\r\nstart = time.time()\r\nimg = Image.open(r'PATH').convert(\"RGB\")\r\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\r\npixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\r\nmodel.config.decoder_start_token_id = 2\r\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\r\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\r\nmodel.config.vocab_size = model.config.decoder.vocab_size\r\ngenerated_ids = model.generate(pixel_values.to(device))\r\nend = time.time()\r\n```\r\n\r\nHow can I run encoder/decoder with wrapped ORT instead invoking two concurrent sessions for Encoder and decoder in loop\r\n\r\n@mht-sharma  @NielsRogge \r\n\r\nencoder_path -> takes encoder ONNX\r\ndecoder_path -> take decdoer ONNX\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n\r\n1. Run above code snippet model.generate() invokes error\r\n\r\n\r\n### Expected behavior\r\n\r\nONNX inference session should be invoked using ORTVisionseq2seq ,when given encoder and decoder. Also Model.generate() not a valid function for generating IDS",
                "reactions": {
                    "url": "https://api.github.com/repos/huggingface/transformers/issues/20644/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/huggingface/transformers/issues/20644/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "event": "cross-referenced"
    }
]