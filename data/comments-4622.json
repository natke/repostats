[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664161763",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4622#issuecomment-664161763",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622",
        "id": 664161763,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY2NDE2MTc2Mw==",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-27T07:06:31Z",
        "updated_at": "2020-07-27T07:06:31Z",
        "author_association": "MEMBER",
        "body": "Not sure what you mean by 'set four cpu cores'. Are those equivalent to physical cores? If there are 4 physical cores and no hyperthreading I believe OpenMP would use 4 threads by default. Also configurable via the OMP_NUM_THREADS environment variable.\r\n\r\nORT doesn't break the input data into multiple batches based on the number of threads and run the model concurrently with those batches (assuming that's what you mean by 'model parallelism'). Instead some operators may utilize multiple threads (intra-op threads) to perform work. For example MatMul or GEMM can be split across multiple threads if there's enough work to do, and if OpenMP is enabled that most likely is done via `#pragma omp paralle for` ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664161763/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664218547",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4622#issuecomment-664218547",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622",
        "id": 664218547,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY2NDIxODU0Nw==",
        "user": {
            "login": "Peppa-cs",
            "id": 60772185,
            "node_id": "MDQ6VXNlcjYwNzcyMTg1",
            "avatar_url": "https://avatars.githubusercontent.com/u/60772185?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Peppa-cs",
            "html_url": "https://github.com/Peppa-cs",
            "followers_url": "https://api.github.com/users/Peppa-cs/followers",
            "following_url": "https://api.github.com/users/Peppa-cs/following{/other_user}",
            "gists_url": "https://api.github.com/users/Peppa-cs/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Peppa-cs/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Peppa-cs/subscriptions",
            "organizations_url": "https://api.github.com/users/Peppa-cs/orgs",
            "repos_url": "https://api.github.com/users/Peppa-cs/repos",
            "events_url": "https://api.github.com/users/Peppa-cs/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Peppa-cs/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-27T09:06:41Z",
        "updated_at": "2020-07-27T09:06:41Z",
        "author_association": "NONE",
        "body": "> Not sure what you mean by 'set four cpu cores'. Are those equivalent to physical cores? If there are 4 physical cores and no hyperthreading I believe OpenMP would use 4 threads by default. Also configurable via the OMP_NUM_THREADS environment variable.\r\n> \r\n> ORT doesn't break the input data into multiple batches based on the number of threads and run the model concurrently with those batches (assuming that's what you mean by 'model parallelism'). Instead some operators may utilize multiple threads (intra-op threads) to perform work. For example MatMul or GEMM can be split across multiple threads if there's enough work to do, and if OpenMP is enabled that most likely is done via `#pragma omp paralle for`\r\n\r\nThanks for fast reply!  Get it!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664218547/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664728980",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4622#issuecomment-664728980",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622",
        "id": 664728980,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY2NDcyODk4MA==",
        "user": {
            "login": "snnn",
            "id": 856316,
            "node_id": "MDQ6VXNlcjg1NjMxNg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/856316?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/snnn",
            "html_url": "https://github.com/snnn",
            "followers_url": "https://api.github.com/users/snnn/followers",
            "following_url": "https://api.github.com/users/snnn/following{/other_user}",
            "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/snnn/subscriptions",
            "organizations_url": "https://api.github.com/users/snnn/orgs",
            "repos_url": "https://api.github.com/users/snnn/repos",
            "events_url": "https://api.github.com/users/snnn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/snnn/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-28T01:51:12Z",
        "updated_at": "2020-07-28T01:51:12Z",
        "author_association": "MEMBER",
        "body": "ORT supports both of them.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664728980/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664796586",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4622#issuecomment-664796586",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622",
        "id": 664796586,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY2NDc5NjU4Ng==",
        "user": {
            "login": "Peppa-cs",
            "id": 60772185,
            "node_id": "MDQ6VXNlcjYwNzcyMTg1",
            "avatar_url": "https://avatars.githubusercontent.com/u/60772185?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Peppa-cs",
            "html_url": "https://github.com/Peppa-cs",
            "followers_url": "https://api.github.com/users/Peppa-cs/followers",
            "following_url": "https://api.github.com/users/Peppa-cs/following{/other_user}",
            "gists_url": "https://api.github.com/users/Peppa-cs/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Peppa-cs/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Peppa-cs/subscriptions",
            "organizations_url": "https://api.github.com/users/Peppa-cs/orgs",
            "repos_url": "https://api.github.com/users/Peppa-cs/repos",
            "events_url": "https://api.github.com/users/Peppa-cs/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Peppa-cs/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-28T06:04:28Z",
        "updated_at": "2020-07-28T06:04:28Z",
        "author_association": "NONE",
        "body": "> ORT supports both of them.\r\n\r\nThanks for your reply! Can you explain it in more detail? Did 'them' mean data parallelism or model parallelism?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/664796586/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/665150603",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/4622#issuecomment-665150603",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622",
        "id": 665150603,
        "node_id": "MDEyOklzc3VlQ29tbWVudDY2NTE1MDYwMw==",
        "user": {
            "login": "snnn",
            "id": 856316,
            "node_id": "MDQ6VXNlcjg1NjMxNg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/856316?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/snnn",
            "html_url": "https://github.com/snnn",
            "followers_url": "https://api.github.com/users/snnn/followers",
            "following_url": "https://api.github.com/users/snnn/following{/other_user}",
            "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/snnn/subscriptions",
            "organizations_url": "https://api.github.com/users/snnn/orgs",
            "repos_url": "https://api.github.com/users/snnn/repos",
            "events_url": "https://api.github.com/users/snnn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/snnn/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-07-28T16:45:48Z",
        "updated_at": "2020-07-28T16:47:48Z",
        "author_association": "MEMBER",
        "body": "Right. \r\n\r\nData parallelism means onnxruntime can run multiple inference on the same session(model) in parallel. And for each inference, if your batch size>1, in some nodes onnxruntime can run these samples in parallel. \r\nmodel parallelism onnxruntime can run multiple parts of the model in parallel by using the parallel executor, which is not enabled by default. Please reference onnxruntime's document for how to enable it. \r\n\r\nHowever, these things only applies to the default CPU provider and CUDA provider. Even the CUDA provider supports model parallelism, but it doesn't support multiple GPU card yet, so probably it's not what you want. \r\n ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/665150603/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]