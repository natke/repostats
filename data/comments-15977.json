[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1563266602",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15977#issuecomment-1563266602",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15977",
        "id": 1563266602,
        "node_id": "IC_kwDOCVq1mM5dLY4q",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-25T17:31:33Z",
        "updated_at": "2023-05-25T17:31:33Z",
        "author_association": "MEMBER",
        "body": "The mismatch is from fp16_tensor * fp32_constant. In the onnx graph, it uses fp16_tensor * Cast(fp32_constant, to=float16). In numpy, the computation is in fp32 then output fp16.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1563266602/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604407214",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15977#issuecomment-1604407214",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15977",
        "id": 1604407214,
        "node_id": "IC_kwDOCVq1mM5foU-u",
        "user": {
            "login": "justinchuby",
            "id": 11205048,
            "node_id": "MDQ6VXNlcjExMjA1MDQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11205048?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/justinchuby",
            "html_url": "https://github.com/justinchuby",
            "followers_url": "https://api.github.com/users/justinchuby/followers",
            "following_url": "https://api.github.com/users/justinchuby/following{/other_user}",
            "gists_url": "https://api.github.com/users/justinchuby/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/justinchuby/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/justinchuby/subscriptions",
            "organizations_url": "https://api.github.com/users/justinchuby/orgs",
            "repos_url": "https://api.github.com/users/justinchuby/repos",
            "events_url": "https://api.github.com/users/justinchuby/events{/privacy}",
            "received_events_url": "https://api.github.com/users/justinchuby/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-23T15:00:39Z",
        "updated_at": "2023-06-23T15:00:39Z",
        "author_association": "MEMBER",
        "body": "The same happens when we provide the constant as float16 @tianleiwu:\r\n\r\n```python\r\nimport torch\r\nimport onnxruntime as ort\r\nimport numpy as np\r\n\r\ninput = {\r\n    \"input_0\": np.array(\r\n        [\r\n            [7.99, -3.93, -8.35, -5.188, -8.1],\r\n            [3.7, 6.18, -2.293, -2.523, -1.925],\r\n            [2.68, -8.15, 7.46, -1.995, 2.936],\r\n            [-1.459, -5.188, -5.08, 8.73, 2.7],\r\n            [-6.82, -7.55, 4.22, -0.3604, 2.936],\r\n        ],\r\n        dtype=np.float16,\r\n    ),\r\n    \"input_1\": np.array(\r\n        [\r\n            [-0.04395, -4.246, -2.338, 0.923, 4.938],\r\n            [-8.3, -7.84, -2.004, -1.099, -7.797],\r\n            [-7.39, 3.516, 2.89, -2.11, 4.457],\r\n            [7.48, -0.3604, -8.41, -4.21, 6.793],\r\n            [-8.55, 3.945, -7.207, -7.902, 6.555],\r\n        ],\r\n        dtype=np.float16,\r\n    ),\r\n}\r\n\r\n# use onnxscript model\r\nmodel_path = \"add.onnx\"\r\nsess = ort.InferenceSession(model_path)\r\noutput1 = sess.run(None, input)\r\n\r\nnp.set_printoptions(formatter={\"float\": \"{: 0.6f}\".format})\r\nprint(output1)\r\nprint(\"------------\")\r\n# use torch.ops.aten.add\r\n# output = self + other * alpha\r\noutput_torch = torch.ops.aten.add(\r\n    torch.tensor(input[\"input_0\"]), torch.tensor(input[\"input_1\"]), alpha=-3.125\r\n)\r\noutput2 = output_torch.numpy()\r\nprint(output2)\r\nprint(\"------------\")\r\n# use numpy\r\noutput3 = input[\"input_0\"] + input[\"input_1\"] * np.array(-3.125, dtype=np.float16)\r\nprint(output3)\r\nprint(\"------------\")\r\nprint(np.allclose(output1, output2, rtol=1e-03, atol=1e-05))\r\nprint(np.allclose(output1, output3, rtol=1e-03, atol=1e-05))\r\nprint(np.allclose(output2, output3, rtol=1e-03, atol=1e-05))\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604407214/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604562734",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15977#issuecomment-1604562734",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15977",
        "id": 1604562734,
        "node_id": "IC_kwDOCVq1mM5fo68u",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-23T17:03:34Z",
        "updated_at": "2023-06-23T17:06:48Z",
        "author_association": "MEMBER",
        "body": "@justinchuby, you can see that `sess = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider'])` works fine.\r\n```\r\n[array([[ 8.125000,  9.335938, -1.046875, -8.070312, -23.531250],\r\n       [ 29.625000,  30.687500,  3.968750,  0.910156,  22.437500],\r\n       [ 25.781250, -19.125000, -1.570312,  4.597656, -10.992188],\r\n       [-24.828125, -4.062500,  21.187500,  21.875000, -18.531250],\r\n       [ 19.875000, -19.875000,  26.734375,  24.328125, -17.546875]],\r\n      dtype=float16)]\r\n------------\r\n[[ 8.125000  9.335938 -1.046875 -8.070312 -23.531250]\r\n [ 29.625000  30.687500  3.968750  0.910156  22.437500]\r\n [ 25.781250 -19.125000 -1.570312  4.597656 -10.992188]\r\n [-24.828125 -4.062500  21.187500  21.875000 -18.531250]\r\n [ 19.875000 -19.875000  26.734375  24.328125 -17.546875]]\r\n------------\r\n[[ 8.125000  9.335938 -1.046875 -8.070312 -23.531250]\r\n [ 29.625000  30.687500  3.968750  0.910156  22.437500]\r\n [ 25.781250 -19.125000 -1.570312  4.597656 -10.992188]\r\n [-24.828125 -4.062500  21.187500  21.875000 -18.531250]\r\n [ 19.875000 -19.875000  26.734375  24.328125 -17.546875]]\r\n------------\r\nTrue\r\nTrue\r\nTrue\r\n```\r\n\r\nIf you use CPU EP:\r\n```\r\n[array([[ 8.125000,  9.335938, -1.045898, -8.070312, -23.531250],\r\n       [ 29.625000,  30.671875,  3.968750,  0.909668,  22.437500],\r\n       [ 25.781250, -19.140625, -1.572266,  4.597656, -10.992188],\r\n       [-24.828125, -4.062500,  21.187500,  21.890625, -18.531250],\r\n       [ 19.890625, -19.875000,  26.734375,  24.328125, -17.546875]],\r\n      dtype=float16)]\r\n------------\r\n[[ 8.125000  9.335938 -1.046875 -8.070312 -23.531250]\r\n [ 29.625000  30.687500  3.968750  0.910156  22.437500]\r\n [ 25.781250 -19.125000 -1.570312  4.597656 -10.992188]\r\n [-24.828125 -4.062500  21.187500  21.875000 -18.531250]\r\n [ 19.875000 -19.875000  26.734375  24.328125 -17.546875]]\r\n------------\r\n[[ 8.125000  9.335938 -1.046875 -8.070312 -23.531250]\r\n [ 29.625000  30.687500  3.968750  0.910156  22.437500]\r\n [ 25.781250 -19.125000 -1.570312  4.597656 -10.992188]\r\n [-24.828125 -4.062500  21.187500  21.875000 -18.531250]\r\n [ 19.875000 -19.875000  26.734375  24.328125 -17.546875]]\r\n------------\r\nFalse\r\nFalse\r\nTrue\r\n```\r\nThe reason is that, CPU does not have FP16 version of Mul and Add, so it adds Cast to compute Mul and Add in FP32. It is obvious that onnx script has not considered this case since the alpha will use FP32 value in CPU EP, but alpha is FP16 in CUDA EP.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604562734/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604575425",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15977#issuecomment-1604575425",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15977",
        "id": 1604575425,
        "node_id": "IC_kwDOCVq1mM5fo-DB",
        "user": {
            "login": "justinchuby",
            "id": 11205048,
            "node_id": "MDQ6VXNlcjExMjA1MDQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11205048?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/justinchuby",
            "html_url": "https://github.com/justinchuby",
            "followers_url": "https://api.github.com/users/justinchuby/followers",
            "following_url": "https://api.github.com/users/justinchuby/following{/other_user}",
            "gists_url": "https://api.github.com/users/justinchuby/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/justinchuby/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/justinchuby/subscriptions",
            "organizations_url": "https://api.github.com/users/justinchuby/orgs",
            "repos_url": "https://api.github.com/users/justinchuby/repos",
            "events_url": "https://api.github.com/users/justinchuby/events{/privacy}",
            "received_events_url": "https://api.github.com/users/justinchuby/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-23T17:12:57Z",
        "updated_at": "2023-06-23T17:12:57Z",
        "author_association": "MEMBER",
        "body": "I see. Thank you!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604575425/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604604477",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15977#issuecomment-1604604477",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15977",
        "id": 1604604477,
        "node_id": "IC_kwDOCVq1mM5fpFI9",
        "user": {
            "login": "BowenBao",
            "id": 9376104,
            "node_id": "MDQ6VXNlcjkzNzYxMDQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9376104?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BowenBao",
            "html_url": "https://github.com/BowenBao",
            "followers_url": "https://api.github.com/users/BowenBao/followers",
            "following_url": "https://api.github.com/users/BowenBao/following{/other_user}",
            "gists_url": "https://api.github.com/users/BowenBao/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BowenBao/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BowenBao/subscriptions",
            "organizations_url": "https://api.github.com/users/BowenBao/orgs",
            "repos_url": "https://api.github.com/users/BowenBao/repos",
            "events_url": "https://api.github.com/users/BowenBao/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BowenBao/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-23T17:32:53Z",
        "updated_at": "2023-06-23T17:38:56Z",
        "author_association": "MEMBER",
        "body": "Posting a comparison script including gpu results\r\n```python\r\nimport torch\r\nimport onnxruntime as ort\r\nimport numpy as np\r\nimport onnxscript\r\nimport onnx\r\n\r\ninput = {\r\n    \"input_0\": np.array(\r\n        [\r\n            [7.99, -3.93, -8.35, -5.188, -8.1],\r\n            [3.7, 6.18, -2.293, -2.523, -1.925],\r\n            [2.68, -8.15, 7.46, -1.995, 2.936],\r\n            [-1.459, -5.188, -5.08, 8.73, 2.7],\r\n            [-6.82, -7.55, 4.22, -0.3604, 2.936],\r\n        ],\r\n        dtype=np.float16,\r\n    ),\r\n    \"input_1\": np.array(\r\n        [\r\n            [-0.04395, -4.246, -2.338, 0.923, 4.938],\r\n            [-8.3, -7.84, -2.004, -1.099, -7.797],\r\n            [-7.39, 3.516, 2.89, -2.11, 4.457],\r\n            [7.48, -0.3604, -8.41, -4.21, 6.793],\r\n            [-8.55, 3.945, -7.207, -7.902, 6.555],\r\n        ],\r\n        dtype=np.float16,\r\n    ),\r\n}\r\n\r\n\r\n# use onnxscript model\r\nmodel_path = \"add.onnx\"\r\nprint(onnxscript.proto2text(onnx.load(model_path)))\r\nsess = ort.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\"])\r\noutput_ort_cuda = sess.run(None, input)\r\nsess = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\r\noutput_ort_cpu = sess.run(None, input)\r\n\r\nnp.set_printoptions(formatter={\"float\": \"{: 0.6f}\".format})\r\nprint(\"ort_cpu\")\r\nprint(output_ort_cpu)\r\nprint(\"ort_cuda\")\r\nprint(output_ort_cuda)\r\nprint(\"------------\")\r\n# use torch.ops.aten.add\r\n# output = self + other * alpha\r\noutput_torch = torch.ops.aten.add(\r\n    torch.tensor(input[\"input_0\"]), torch.tensor(input[\"input_1\"]), alpha=-3.125\r\n)\r\noutput_pt_cpu = output_torch.cpu().numpy()\r\nprint(\"pt_cpu\")\r\nprint(output_pt_cpu)\r\noutput_torch = torch.ops.aten.add(\r\n    torch.tensor(input[\"input_0\"]).to(torch.device(\"cuda\")), torch.tensor(input[\"input_1\"]).to(torch.device(\"cuda\")), alpha=-3.125\r\n)\r\noutput_pt_cuda = output_torch.cpu().numpy()\r\nprint(\"pt_cuda\")\r\nprint(output_pt_cuda)\r\nprint(\"------------\")\r\n# use numpy\r\noutput_np = input[\"input_0\"] + input[\"input_1\"] * np.array(-3.125, dtype=np.float16)\r\nnp.add(1, 2)\r\nprint(\"np\")\r\nprint(output_np)\r\nprint(\"------------\")\r\ndef comp(a, b, a_name, b_name):\r\n    print(f\"{a_name} vs {b_name}, {np.allclose(a, b, rtol=1e-03, atol=1e-05)}\")\r\n\r\ncomp(output_ort_cpu, output_ort_cuda, \"ort_cpu\", \"ort_cuda\")\r\ncomp(output_pt_cpu, output_pt_cuda, \"pt_cpu\", \"pt_cuda\")\r\ncomp(output_ort_cpu, output_pt_cuda, \"ort_cpu\", \"pt_cuda\")\r\ncomp(output_ort_cuda, output_pt_cpu, \"ort_cuda\", \"pt_cpu\")\r\n\r\ncomp(output_ort_cpu, output_np, \"ort_cpu\", \"np\")\r\ncomp(output_ort_cuda, output_np, \"ort_cuda\", \"np\")\r\ncomp(output_pt_cpu, output_np, \"pt_cpu\", \"np\")\r\ncomp(output_pt_cuda, output_np, \"pt_cuda\", \"np\")\r\n```\r\nResults\r\n```\r\n...\r\n------------\r\nort_cpu vs ort_cuda, False\r\npt_cpu vs pt_cuda, False\r\nort_cpu vs pt_cuda, True\r\nort_cuda vs pt_cpu, True\r\nort_cpu vs np, False\r\nort_cuda vs np, True\r\npt_cpu vs np, True\r\npt_cuda vs np, False\r\n```\r\n\r\nIt is interesting that `ort_cuda == pt_cpu == np`, `ort_cpu == pt_cuda` and `ort_cpu != np`\r\n@tianleiwu do you have insight? We are fairly sure that both `pt_cpu` and `np` computes fp16 arithmetic in fp32 with cast back. Are you suggesting that ORT CPU somehow is able to compute fp16 directly, but on the other hand applies the upcasting logic for CUDA EP?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604604477/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604672381",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15977#issuecomment-1604672381",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15977",
        "id": 1604672381,
        "node_id": "IC_kwDOCVq1mM5fpVt9",
        "user": {
            "login": "BowenBao",
            "id": 9376104,
            "node_id": "MDQ6VXNlcjkzNzYxMDQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9376104?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BowenBao",
            "html_url": "https://github.com/BowenBao",
            "followers_url": "https://api.github.com/users/BowenBao/followers",
            "following_url": "https://api.github.com/users/BowenBao/following{/other_user}",
            "gists_url": "https://api.github.com/users/BowenBao/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BowenBao/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BowenBao/subscriptions",
            "organizations_url": "https://api.github.com/users/BowenBao/orgs",
            "repos_url": "https://api.github.com/users/BowenBao/repos",
            "events_url": "https://api.github.com/users/BowenBao/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BowenBao/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-23T18:19:38Z",
        "updated_at": "2023-06-23T18:19:38Z",
        "author_association": "MEMBER",
        "body": "```python\r\nimport torch\r\n\r\ndef func(a, b, alpha):\r\n    return a + b * alpha\r\n\r\ndef native_add(a, b, alpha):\r\n    return torch.ops.aten.add(a, b, alpha=alpha)\r\n\r\ndef call(a, b, alpha, device, fn):\r\n    a = a.to(device)\r\n    b = b.to(device)\r\n\r\n    return fn(a, b, alpha)\r\n\r\na = torch.tensor([-8.35], dtype=torch.float16)\r\nb = torch.tensor([-2.338], dtype=torch.float16)\r\nalpha = -3.125\r\n\r\nprint(call(a, b, alpha, torch.device(\"cpu\"), func))\r\nprint(call(a, b, alpha, torch.device(\"cpu\"), native_add))\r\nprint(call(a, b, alpha, torch.device(\"cuda\"), func))\r\nprint(call(a, b, alpha, torch.device(\"cuda\"), native_add))\r\n```\r\nResult\r\n```\r\ntensor([-1.0469], dtype=torch.float16)\r\ntensor([-1.0469], dtype=torch.float16)\r\ntensor([-1.0469], device='cuda:0', dtype=torch.float16)\r\ntensor([-1.0459], device='cuda:0', dtype=torch.float16)\r\n```\r\nOn a side note, @justinchuby for this case torch is consistent between cpu & gpu w.r.t op math type promotion. It is how alpha is being handled internally inside kernel that differs.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1604672381/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]