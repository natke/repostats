[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1412531172",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1412531172",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1412531172,
        "node_id": "IC_kwDOCVq1mM5UMYPk",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-01T18:31:24Z",
        "updated_at": "2023-02-01T18:31:24Z",
        "author_association": "MEMBER",
        "body": "CUDA allocations are expensive, so ORT caches them in its own Arena. It attempts to re-use it, but the overall footprint is high.\r\n\r\nYou can try disabling it with `sessionOptions.DisableCpuMemoryArena()` and see how much your performance depends on it.\r\nCPU is a misnomer as the arena was never meant to be for CPU memory.\r\n\r\nTo deal with the memory growth, there is a `shrink` feature that may attempt periodically to shrink the arena's memory. It is not perfect.\r\n\r\nYou can read about it [here](https://onnxruntime.ai/docs/get-started/with-c.html#features).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1412531172/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1412648926",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1412648926",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1412648926,
        "node_id": "IC_kwDOCVq1mM5UM0_e",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-01T20:02:53Z",
        "updated_at": "2023-02-01T20:03:23Z",
        "author_association": "NONE",
        "body": "Same with DirectML. Can use 5-6GB of GPU VRAM plus lots of RAM for about 2GB of onnx files. The DirectML people are saying they are working on reducing the memory size. So cross fingers.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1412648926/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1413457011",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1413457011",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1413457011,
        "node_id": "IC_kwDOCVq1mM5UP6Rz",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-02T09:59:18Z",
        "updated_at": "2023-02-02T10:00:04Z",
        "author_association": "CONTRIBUTOR",
        "body": "@yuslepukhin @pauldog Thank you for your advice!\r\n\r\nI tried using the CUDA provider options `{\"arena_extend_strategy\": \"kSameAsRequested\", \"cudnn_conv_algo_search\": \"HEURISTIC\"}` with no success.\r\n\r\nThe\r\n\r\n```\r\nrun_options = onnxrt.RunOptions()\r\nrun_options.add_run_config_entry(\"memory.enable_memory_arena_shrinkage\", \"cpu:0;gpu:0\")\r\nsession.run([], {input_name: x}, run_options)\r\n````\r\n\r\ndid not help either.\r\n\r\nThis is a good reference: https://github.com/microsoft/onnxruntime/issues/14038#issuecomment-1368306161\r\n\r\nI still don't really understand what's the difference with PyTorch, where the huge memory allocation comes from, and why Pytorch is more memory efficient out-of-the-box. Digging into provider options to have something (maybe) work is not very user friendly. I think I'll give up for now.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1413457011/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1431828534",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1431828534",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1431828534,
        "node_id": "IC_kwDOCVq1mM5VV_g2",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-15T18:32:57Z",
        "updated_at": "2023-02-15T18:32:57Z",
        "author_association": "MEMBER",
        "body": "Looking at the title of this issue, I am not sure you are looking for a correct thing. Are you comparing ONNX file size with the memory requirements during runtime?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1431828534/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1431863162",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1431863162",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1431863162,
        "node_id": "IC_kwDOCVq1mM5VWH96",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-15T19:00:49Z",
        "updated_at": "2023-02-15T19:01:38Z",
        "author_association": "CONTRIBUTOR",
        "body": "Yes, roughly - I take it as a proxy as I don't expect to be caching activations with skip connections or anything. Or better, I am comparing PyTorch's memory usage vs ONNX Runtime on CUDAExecutionProvider.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1431863162/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1477536796",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1477536796",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1477536796,
        "node_id": "IC_kwDOCVq1mM5YEWwc",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-21T09:47:06Z",
        "updated_at": "2023-03-21T09:51:15Z",
        "author_association": "CONTRIBUTOR",
        "body": "I still haven't solved the issue. GPT-J loads fine on 12 GB memory with PyTorch, 20 GB is not enough with ORT in `ort.InferenceSession`.\r\n\r\nEdit: even 40 GB is not enough. It does not make for a good user experience if one needs to tweak undocumented parameters, read github issues and such to have something work.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1477536796/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1477592968",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1477592968",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1477592968,
        "node_id": "IC_kwDOCVq1mM5YEkeI",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-21T10:26:53Z",
        "updated_at": "2023-03-21T10:26:53Z",
        "author_association": "CONTRIBUTOR",
        "body": "For reference, the model is https://huggingface.co/fxmarty/gpt-j-6B-onnx/tree/main . cc @tianleiwu @yufenglee\r\n\r\n```python\r\nfrom optimum.onnxruntime import ORTModelForCausalLM\r\nfrom transformers import AutoTokenizer\r\nimport time\r\n\r\nmodel_id = \"gptj_onnx\"\r\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\r\n\r\nprint(\"loading model\")\r\nstart = time.time()\r\nmodel = ORTModelForCausalLM.from_pretrained(model_id, provider=\"CUDAExecutionProvider\")\r\nprint(f\"Loading took: {time.time() - start:.2f} s\")\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1477592968/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1478234486",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1478234486",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1478234486,
        "node_id": "IC_kwDOCVq1mM5YHBF2",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-21T16:48:36Z",
        "updated_at": "2023-03-21T16:48:36Z",
        "author_association": "MEMBER",
        "body": "@fxmarty have you tried disabling the arena altogether?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1478234486/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1478647283",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1478647283",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1478647283,
        "node_id": "IC_kwDOCVq1mM5YIl3z",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-21T22:02:18Z",
        "updated_at": "2023-03-21T22:04:04Z",
        "author_association": "MEMBER",
        "body": "@fxmarty, for GPT-j, I saw many Cast nodes from fp16 to fp32:\r\n<img width=\"253\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30328909/226748984-24dd02c2-e2d8-4efa-a50b-e6eb78f8beaf.png\">\r\nThat will need more memory to run in fp32.\r\nGeneral guideline is to run graph optimizations first (with CUDA EP), then convert graph to fp16. \r\n\r\nFor GPT-j, ORT do not have GPT-j specified optimizations so probably only partial optimization.\r\n\r\nAlso, I saw If operator is to used to combine two subgraphs. It is recommended to export two onnx models (one for initial run without past state, another with past state) separately. Those two ONNX models can be used in BeamSearch operator as subgraphs, which will call them in sequential order. In this way, you need not use If operator.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1478647283/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1479309446",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1479309446",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1479309446,
        "node_id": "IC_kwDOCVq1mM5YLHiG",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-22T10:38:14Z",
        "updated_at": "2023-03-22T10:47:06Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thank you for your suggestions @pranavsharma @tianleiwu!\r\n\r\nThe model was exported with `torch.onnx.export` already with fp16. I did not use ORT fp16 conversion due to the issues I hit with incomplete symbolic shape inference we discussed in an other issue.\r\n\r\nExporting in fp16 without running ORT optimizations, we don't have these `InsertedCast`. So it appears the `InsertedCast` nodes were inserted by `optimize_model`, and it seems that `optimize_model` should not be used when exporting already in fp16 from pytorch. Is it correct?\r\n\r\nAlso, the thing is that it is not about running the model. It is just about loading it into an InferenceSession:\r\n\r\n```python\r\nimport onnxruntime as ort\r\n\r\nsession = ort.InferenceSession(\"decoder_model_merged.onnx\", providers=[\"CUDAExecutionProvider\"])\r\n```\r\n\r\nThis one allocates 66419 MiB and will OOM if fewer GPU memory than that is available.\r\n\r\nRunning\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM\r\nimport torch\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(\"cuda\")\r\n```\r\n\r\nallocates 12157MiB.\r\n\r\nI would like to avoid using BeamSearch or GreedySearch operators, as they are I think only supported by CPUExecutionProvider and CUDAExecutionProvider. But you are right that we could support them as an option in the ONNX export - especially if those work with onnxruntime-web.\r\n\r\nI will try disabling the arena.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1479309446/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480893122",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1480893122",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1480893122,
        "node_id": "IC_kwDOCVq1mM5YRKLC",
        "user": {
            "login": "cqray1990",
            "id": 32585434,
            "node_id": "MDQ6VXNlcjMyNTg1NDM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/32585434?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cqray1990",
            "html_url": "https://github.com/cqray1990",
            "followers_url": "https://api.github.com/users/cqray1990/followers",
            "following_url": "https://api.github.com/users/cqray1990/following{/other_user}",
            "gists_url": "https://api.github.com/users/cqray1990/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cqray1990/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cqray1990/subscriptions",
            "organizations_url": "https://api.github.com/users/cqray1990/orgs",
            "repos_url": "https://api.github.com/users/cqray1990/repos",
            "events_url": "https://api.github.com/users/cqray1990/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cqray1990/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-23T09:52:26Z",
        "updated_at": "2023-03-23T09:52:26Z",
        "author_association": "NONE",
        "body": "@pranavsharma\r\nhow to disabling the arena?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1480893122/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1489132798",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1489132798",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1489132798,
        "node_id": "IC_kwDOCVq1mM5Ywlz-",
        "user": {
            "login": "igormis",
            "id": 6599037,
            "node_id": "MDQ6VXNlcjY1OTkwMzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6599037?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/igormis",
            "html_url": "https://github.com/igormis",
            "followers_url": "https://api.github.com/users/igormis/followers",
            "following_url": "https://api.github.com/users/igormis/following{/other_user}",
            "gists_url": "https://api.github.com/users/igormis/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/igormis/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/igormis/subscriptions",
            "organizations_url": "https://api.github.com/users/igormis/orgs",
            "repos_url": "https://api.github.com/users/igormis/repos",
            "events_url": "https://api.github.com/users/igormis/events{/privacy}",
            "received_events_url": "https://api.github.com/users/igormis/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-29T18:53:45Z",
        "updated_at": "2023-03-29T18:53:45Z",
        "author_association": "NONE",
        "body": "I have also some issues when doing inference using onnx optimized xlm-roberta model. I have following settings:\r\n```\r\ncuda_provider_options = {\"arena_extend_strategy\": \"kSameAsRequested\", \"do_copy_in_default_stream\": False, \"cudnn_conv_use_max_workspace\": \"1\"}\r\ncpu_provider_options = {\"arena_extend_strategy\": \"kSameAsRequested\", \"do_copy_in_default_stream\": False}\r\nexecution_providers = [(\"CUDAExecutionProvider\", cuda_provider_options), (\"CPUExecutionProvider\", cpu_provider_options)]\r\nsess = rt.InferenceSession(\"roberta_onnx_model/__MODEL_PROTO.onnx\", providers=execution_providers)\r\n```\r\nFor the run options I have:\r\n```\r\nrun_options.add_run_config_entry(\"kOrtRunOptionsConfigEnableMemoryArenaShrinkage\", \"cpu:0,gpu:0\")\r\nrun_options.add_run_config_entry(\"kOrtSessionOptionsUseDeviceAllocatorForInitializers\", \"1\")\r\n```\r\nand here is the inference part:\r\n`logits = sess.run([label_name], {input_name: X_test_sample}, run_options)[0]`\r\nThe GPU RAM starts with 3.5GB and after a while it increases to 7 GB and more. Any suggestions on this",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1489132798/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1539577190",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1539577190",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1539577190,
        "node_id": "IC_kwDOCVq1mM5bxBVm",
        "user": {
            "login": "lukas-folle-snkeos",
            "id": 126877803,
            "node_id": "U_kgDOB5AAaw",
            "avatar_url": "https://avatars.githubusercontent.com/u/126877803?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lukas-folle-snkeos",
            "html_url": "https://github.com/lukas-folle-snkeos",
            "followers_url": "https://api.github.com/users/lukas-folle-snkeos/followers",
            "following_url": "https://api.github.com/users/lukas-folle-snkeos/following{/other_user}",
            "gists_url": "https://api.github.com/users/lukas-folle-snkeos/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lukas-folle-snkeos/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lukas-folle-snkeos/subscriptions",
            "organizations_url": "https://api.github.com/users/lukas-folle-snkeos/orgs",
            "repos_url": "https://api.github.com/users/lukas-folle-snkeos/repos",
            "events_url": "https://api.github.com/users/lukas-folle-snkeos/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lukas-folle-snkeos/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-09T07:05:09Z",
        "updated_at": "2023-05-09T07:05:09Z",
        "author_association": "NONE",
        "body": "Is there a way to associate the memory usage per layer and to identify problematic ones?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1539577190/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1540511207",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14526#issuecomment-1540511207",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14526",
        "id": 1540511207,
        "node_id": "IC_kwDOCVq1mM5b0lXn",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-09T16:37:59Z",
        "updated_at": "2023-05-09T16:39:28Z",
        "author_association": "MEMBER",
        "body": "@lukas-folle-snkeos, You can build ORT from source and enable node input and outputs dumping, and apply some debug code (or build the branch):\r\nhttps://github.com/microsoft/onnxruntime/commit/6c0a0aacfdc3b63c99921e7ead69bbeb46dbb3f4\r\nThen you can see the memory allocation.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1540511207/reactions",
            "total_count": 2,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 2,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]