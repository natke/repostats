[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/791595861",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6851#issuecomment-791595861",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6851",
        "id": 791595861,
        "node_id": "MDEyOklzc3VlQ29tbWVudDc5MTU5NTg2MQ==",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-03-05T18:18:27Z",
        "updated_at": "2021-03-05T18:22:12Z",
        "author_association": "MEMBER",
        "body": "> Which optimization options really matter when you write \"make sure to use the exact same options\"? You mention execution providers and optimization level - but how about things like e.g. BertOptimizationOptions, which doesn't have their counterparts in SessionOptions?\r\n> What is the expected outcome of having opt_level=0 during offline optimization and ORT_ENABLE_ALL during inference session, with quantization between them (i.e. as in the code snippets above)? How about having opt_level=99 and ORT_ENABLE_ALL instead?\r\n> If I don't get any errors or warnings from quantize_dynamic, is it OK to assume that the current combination of graph optimization and quantization settings works OK?\r\n\r\nAs long as the model is optimized for CPU and your inference is also in CPU, you can use the default opt level in offline and online tool. Different optimization level might output different graph, which will get same output but different performance for optimized (but not quantized) model. For quantized model, since quantize and de-quantize (there is precision loss in quantization) nodes might place differently given different graph, the result might be different for different optimization level.\r\n\r\n> If I don't get any errors or warnings at the serving stage (i.e. from InferenceSession), is it OK to assume that everything should work fine (i.e. my metrics should be the same on every machine)?\r\n\r\nFor dynamic quantization, if you used batch size 1 in evaluating, the metric shall be same. If you used batch_size > 1, it depends on whether quantization is on whole input or per sample in a batch.\r\n\r\nA potential variance is whether your machine has AVX2 or AVX-512. If you find their output are different, please fill an issue.\r\n\r\n>Finally, the docstring for InferenceSession mentions ORT and ONNX formats - what is the difference between them?\r\n\r\nThat are different file format in storage. ONNX is for portable and it can be used by others (like TensorRT), and ORT can only be used in OnnxRuntime.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/791595861/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/800112868",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6851#issuecomment-800112868",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6851",
        "id": 800112868,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgwMDExMjg2OA==",
        "user": {
            "login": "xor-xor",
            "id": 2082842,
            "node_id": "MDQ6VXNlcjIwODI4NDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2082842?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xor-xor",
            "html_url": "https://github.com/xor-xor",
            "followers_url": "https://api.github.com/users/xor-xor/followers",
            "following_url": "https://api.github.com/users/xor-xor/following{/other_user}",
            "gists_url": "https://api.github.com/users/xor-xor/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xor-xor/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xor-xor/subscriptions",
            "organizations_url": "https://api.github.com/users/xor-xor/orgs",
            "repos_url": "https://api.github.com/users/xor-xor/repos",
            "events_url": "https://api.github.com/users/xor-xor/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xor-xor/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-03-16T09:50:04Z",
        "updated_at": "2021-03-16T09:54:08Z",
        "author_association": "NONE",
        "body": "After some experiments, it seems that indeed, AVX-512 (VNNI?) is the source of the aforementioned variance in my metrics while using `dynamic_quantization` (and yes, I'm evaluating my models with batch size == 1). I'll try to gather some more data on this and eventually open a new issue, as suggested.\r\n\r\nJust one more question here regarding ONNX/ORT formats: I understand that the output of `onnxruntime_tools`\r\n(e.g. `optimizer.optimize_model(...).save_model_to_file(...)`) and `dynamic_quantization` is always ORT, not ONNX - am I correct?\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/800112868/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1030909075",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/6851#issuecomment-1030909075",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/6851",
        "id": 1030909075,
        "node_id": "IC_kwDOCVq1mM49cmyT",
        "user": {
            "login": "xor-xor",
            "id": 2082842,
            "node_id": "MDQ6VXNlcjIwODI4NDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2082842?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xor-xor",
            "html_url": "https://github.com/xor-xor",
            "followers_url": "https://api.github.com/users/xor-xor/followers",
            "following_url": "https://api.github.com/users/xor-xor/following{/other_user}",
            "gists_url": "https://api.github.com/users/xor-xor/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xor-xor/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xor-xor/subscriptions",
            "organizations_url": "https://api.github.com/users/xor-xor/orgs",
            "repos_url": "https://api.github.com/users/xor-xor/repos",
            "events_url": "https://api.github.com/users/xor-xor/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xor-xor/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-06T20:34:56Z",
        "updated_at": "2022-02-06T20:48:06Z",
        "author_association": "NONE",
        "body": "As for my last question, in case someone would land here:\r\n\r\n> I understand that the output of `onnxruntime_tools` (e.g. `optimizer.optimize_model(...).save_model_to_file(...))` and `dynamic_quantization` is always ORT, not ONNX - am I correct?\r\n\r\nNo, this will be still ONNX (Protocol Buffers), whereas ORT (FlatBuffers) needs to be chosen explicitly, as it serves different purposes (applications in more constrained environments) and - as previously mentioned - can be loaded only by ONNX Runtime.\r\n\r\nBTW, there's a [whole new section devoted to ORT format](https://onnxruntime.ai/docs/reference/ort-model-format.html) in the docs now.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1030909075/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]