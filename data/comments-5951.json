[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/734156233",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5951#issuecomment-734156233",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5951",
        "id": 734156233,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczNDE1NjIzMw==",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-26T08:38:50Z",
        "updated_at": "2020-11-26T08:41:31Z",
        "author_association": "MEMBER",
        "body": "> I am further asking the following questions (in a new issue as suggested) with relevance and continuation to the replies in the [comment](https://github.com/microsoft/onnxruntime/issues/232#issuecomment-734060575) by @pranavsharma\r\n> \r\n> * CPU was provided under Execution Providers in the design document. May I understand the reason?\r\nORT ships with a default EP that can run all ONNX ops on the CPU. This is what we call the CPU EP; it is implemented by ORT and is the fallback for all ops. Other EPs like openvino also run on the CPU but may not implement all ONNX ops since they're third party and hence outside ORT's control. May be our CPU EP should've been called \"Default ORT CPU\" EP.\r\n\r\n> * How is standalone execution provider different from execution provider along with ONNX Runtime?\r\nAlready answered this before.\r\n\r\n> * Why is the need of hardware independent optimizations (after converting ONNX model to in-memory graph rep) ?\r\nTo improve performance. You can read more here https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md.\r\n\r\n> * How are the sub-graphs after partitioning scheduled on to different EPs? Do we need to write a script defining the priorities for scheduling?\r\nNo script is needed. The order in which you register EPs with ORT session determines the preference order for partitioning. EPs that are registered earlier are considered first for assigning nodes/sub-graphs to them. Please read the design doc linked earlier.\r\n\r\n> * As few execution providers are not having the implementation of few onnx operators, would it be possible to implement those operators in execution providers? (to avoid the fallback onto cpu and hit bad with performance)\r\nYes. However, many EPs are third party to ORT (for e.g. openvino and tensorrt) and not open source. Hence they're outside ORT's control.\r\n\r\n> \r\n> Trying to connect all the dots of understanding. Any support with patience is a great help to me. Thank you.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/734156233/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/734411345",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5951#issuecomment-734411345",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5951",
        "id": 734411345,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczNDQxMTM0NQ==",
        "user": {
            "login": "jay-karan",
            "id": 65300616,
            "node_id": "MDQ6VXNlcjY1MzAwNjE2",
            "avatar_url": "https://avatars.githubusercontent.com/u/65300616?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jay-karan",
            "html_url": "https://github.com/jay-karan",
            "followers_url": "https://api.github.com/users/jay-karan/followers",
            "following_url": "https://api.github.com/users/jay-karan/following{/other_user}",
            "gists_url": "https://api.github.com/users/jay-karan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jay-karan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jay-karan/subscriptions",
            "organizations_url": "https://api.github.com/users/jay-karan/orgs",
            "repos_url": "https://api.github.com/users/jay-karan/repos",
            "events_url": "https://api.github.com/users/jay-karan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jay-karan/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-26T17:13:38Z",
        "updated_at": "2020-11-26T17:13:38Z",
        "author_association": "NONE",
        "body": "- Why haven't few execution providers could not implement few onnx operators? (what could be the possible reasons?)\r\n\r\n- ONNX Operators are [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md) and the kernels for these operators are unavailable. So, how are these operators being implemented by ORT for default EP? And also, how are these implemented by other EPs into their packages? \r\n\r\n- \r\nIf I am using TensorRT without ORT. It is fair to assume high-level flow as below: \r\n**Input Graph->TensortRT IR Representation -> Optimizations -> Inference.**                            (1)\r\n\r\nSay now, if I am using TensorRT as EP in ORT.\r\nMay I ask to complete/update the below flow: \r\n**ONNX Graph -> In-Memory -> Optimizations by ORT -> Partitioning -> Scheduling subgraphs -> Subgraph on TensorRT-> ....... -> ....... ->.........**                                     (2)\r\n\r\nDoes the TensorRT IR (similar to in (1) ) will be formed on the subgraph generated from the partitioning by ORT in (2)?\r\n\r\n[The above is asked to validate few conclusions on standalone EPs. Hope it doesn't confuse you]\r\n\r\n- Will onnxruntime-gpu package enable CUDAExecutionP?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/734411345/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/734680770",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5951#issuecomment-734680770",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5951",
        "id": 734680770,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczNDY4MDc3MA==",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-27T07:04:38Z",
        "updated_at": "2020-11-27T07:05:31Z",
        "author_association": "MEMBER",
        "body": "> * Why haven't few execution providers could not implement few onnx operators? (what could be the possible reasons?)\r\n\r\nCan't say for sure. One possible reason is that ONNX has many opset versions with variations in the operator specs in new versions. It's not always possible for them to keep up.\r\n\r\n> * ONNX Operators are [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md) and the kernels for these operators are unavailable. So, how are these operators being implemented by ORT for default EP?\r\n\r\nONNX defines the spec for the ops. The implementation is done by ORT. You can see them [here](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/core/providers/cpu).\r\n\r\n> And also, how are these implemented by other EPs into their packages?\r\n\r\nI can't answer this. You can read about this on documentation pages for the individual EPs.\r\n\r\n> \r\n> If I am using TensorRT without ORT. It is fair to assume high-level flow as below:\r\n> **Input Graph->TensortRT IR Representation -> Optimizations -> Inference.** (1)\r\n> \r\n> Say now, if I am using TensorRT as EP in ORT.\r\n> May I ask to complete/update the below flow:\r\n> **ONNX Graph -> In-Memory -> Optimizations by ORT -> Partitioning -> Scheduling subgraphs -> Subgraph on TensorRT-> ....... -> ....... ->.........** (2)\r\n> \r\n> Does the TensorRT IR (similar to in (1) ) will be formed on the subgraph generated from the partitioning by ORT in (2)?\r\n> \r\n> [The above is asked to validate few conclusions on standalone EPs. Hope it doesn't confuse you]\r\n\r\nPlease see [this](https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/TensorRT-ExecutionProvider.md).\r\n\r\n> \r\n> * Will onnxruntime-gpu package enable CUDAExecutionP?\r\n\r\nYes.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/734680770/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/735665784",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5951#issuecomment-735665784",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5951",
        "id": 735665784,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczNTY2NTc4NA==",
        "user": {
            "login": "jay-karan",
            "id": 65300616,
            "node_id": "MDQ6VXNlcjY1MzAwNjE2",
            "avatar_url": "https://avatars.githubusercontent.com/u/65300616?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jay-karan",
            "html_url": "https://github.com/jay-karan",
            "followers_url": "https://api.github.com/users/jay-karan/followers",
            "following_url": "https://api.github.com/users/jay-karan/following{/other_user}",
            "gists_url": "https://api.github.com/users/jay-karan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jay-karan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jay-karan/subscriptions",
            "organizations_url": "https://api.github.com/users/jay-karan/orgs",
            "repos_url": "https://api.github.com/users/jay-karan/repos",
            "events_url": "https://api.github.com/users/jay-karan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jay-karan/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-11-30T09:27:11Z",
        "updated_at": "2020-11-30T09:27:11Z",
        "author_association": "NONE",
        "body": "- May I ask more about the extensibility options specified in [here](https://github.com/microsoft/onnxruntime/blob/master/docs/InferenceHighLevelDesign.md#extensibility-options). What is the need of adding a new operator?\r\n\r\n- The capability of the execution provider if a subgraph can be executed is determined using the GetCapability() API right. How do I use it? Are there any usage examples?\r\n\r\n- Is the tuning [here](https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Perf_Tuning.md#tuning-performance-for-specific-execution-providers) only limited to default execution providers or can we extend to openvino, tensorrt, etc also? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/735665784/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/736144996",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5951#issuecomment-736144996",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5951",
        "id": 736144996,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczNjE0NDk5Ng==",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-12-01T00:50:22Z",
        "updated_at": "2020-12-01T00:50:22Z",
        "author_association": "MEMBER",
        "body": "> * May I ask more about the extensibility options specified in [here](https://github.com/microsoft/onnxruntime/blob/master/docs/InferenceHighLevelDesign.md#extensibility-options). What is the need of adding a new operator?\r\n\r\nONNX supports only a limited set of operators (compared to Tensorflow or Pytorch). There are cases when your use case falls outside this list.\r\n\r\n> * The capability of the execution provider if a subgraph can be executed is determined using the GetCapability() API right. How do I use it? Are there any usage examples?\r\n\r\nYou don't use this API directly. It's called internally during the initialization phase.\r\n\r\n> * Is the tuning [here](https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Perf_Tuning.md#tuning-performance-for-specific-execution-providers) only limited to default execution providers or can we extend to openvino, tensorrt, etc also?\r\n\r\nTuning tips for individual EPs can be found on the individual EP pages here https://github.com/microsoft/onnxruntime/tree/master/docs/execution_providers. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/736144996/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/736987353",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/5951#issuecomment-736987353",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5951",
        "id": 736987353,
        "node_id": "MDEyOklzc3VlQ29tbWVudDczNjk4NzM1Mw==",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-12-02T04:47:24Z",
        "updated_at": "2020-12-02T04:47:24Z",
        "author_association": "MEMBER",
        "body": "Reopen if there are more questions.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/736987353/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]