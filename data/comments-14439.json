[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1405329529",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14439#issuecomment-1405329529",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14439",
        "id": 1405329529,
        "node_id": "IC_kwDOCVq1mM5Tw6B5",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-26T17:11:37Z",
        "updated_at": "2023-01-26T17:11:37Z",
        "author_association": "MEMBER",
        "body": "\"D:\\Projects_AI\\2023_01_26_quantize\\augmented_model.onnx\" is the model for calibration. It is not the quantized model. \r\n\r\nThere should be a calibrated table generated. You can use the calibrated table and original model to create InfereceSession to run in quantization mode for TRT EP. \r\n\r\nPlease refer to:https://github.com/microsoft/onnxruntime-inference-examples/blob/d8afbb845ff19853183f27fe57fc4e749694f727/quantization/image_classification/trt/resnet50/e2e_tensorrt_resnet_example.py#L358\r\n\r\n@stevenlix and @chilo-ms, please provide more details.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1405329529/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1407514697",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14439#issuecomment-1407514697",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14439",
        "id": 1407514697,
        "node_id": "IC_kwDOCVq1mM5T5PhJ",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-28T23:55:29Z",
        "updated_at": "2023-01-28T23:55:29Z",
        "author_association": "NONE",
        "body": "From what I read, for most GPUs, uint8 is going to be really slow unless you have some kind of super tensor-core thingamejig.\r\nI think float16 is the best speed for most commerical graphics cards such as GTX 1080. Someone correct me if I'm wrong.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1407514697/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1408216548",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14439#issuecomment-1408216548",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14439",
        "id": 1408216548,
        "node_id": "IC_kwDOCVq1mM5T763k",
        "user": {
            "login": "gianmarcocr",
            "id": 43784889,
            "node_id": "MDQ6VXNlcjQzNzg0ODg5",
            "avatar_url": "https://avatars.githubusercontent.com/u/43784889?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gianmarcocr",
            "html_url": "https://github.com/gianmarcocr",
            "followers_url": "https://api.github.com/users/gianmarcocr/followers",
            "following_url": "https://api.github.com/users/gianmarcocr/following{/other_user}",
            "gists_url": "https://api.github.com/users/gianmarcocr/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gianmarcocr/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gianmarcocr/subscriptions",
            "organizations_url": "https://api.github.com/users/gianmarcocr/orgs",
            "repos_url": "https://api.github.com/users/gianmarcocr/repos",
            "events_url": "https://api.github.com/users/gianmarcocr/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gianmarcocr/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-30T09:00:32Z",
        "updated_at": "2023-01-30T09:09:05Z",
        "author_association": "NONE",
        "body": "thank you @yufenglee. the model now works as expected and it's way faster.\r\n@pauldog from what I've understood both ways are helpful in accelerating the model. Still, for me, FP16 conversion never worked: I followed this https://natke.github.io/onnxruntime/docs/performance/float16.html but neither of the methods was able to produce a model that could work using TRT EP. the model was either running slower than fp32 counterparts or producing meaningless outputs.\r\nI'm running tests right now, but with INT8 quantization I'm getting something between 2x and 10x faster inference depending on the model",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1408216548/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]