[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1654141944",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1654141944",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1654141944,
        "node_id": "IC_kwDOCVq1mM5imDP4",
        "user": {
            "login": "jywu-msft",
            "id": 43355415,
            "node_id": "MDQ6VXNlcjQzMzU1NDE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/43355415?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jywu-msft",
            "html_url": "https://github.com/jywu-msft",
            "followers_url": "https://api.github.com/users/jywu-msft/followers",
            "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}",
            "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions",
            "organizations_url": "https://api.github.com/users/jywu-msft/orgs",
            "repos_url": "https://api.github.com/users/jywu-msft/repos",
            "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jywu-msft/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-27T18:07:07Z",
        "updated_at": "2023-07-27T18:07:07Z",
        "author_association": "MEMBER",
        "body": "Thanks for providing these details. \r\nIn step 6 , you mention that the model works fine when converting to native TRT format but you needed to use a special version of nvidia graphsurgeon. can you provide more details on that and share the steps for that conversion?\r\ntensorrt ep relies on a component from nvidia,  onnx-tensorrt parser to help us convert from onnx to tensorrt network, so it would be helpful for us to know what graph transform graphsurgeon is doing.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1654141944/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1657081131",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1657081131",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1657081131,
        "node_id": "IC_kwDOCVq1mM5ixQ0r",
        "user": {
            "login": "datinje",
            "id": 43935204,
            "node_id": "MDQ6VXNlcjQzOTM1MjA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/43935204?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/datinje",
            "html_url": "https://github.com/datinje",
            "followers_url": "https://api.github.com/users/datinje/followers",
            "following_url": "https://api.github.com/users/datinje/following{/other_user}",
            "gists_url": "https://api.github.com/users/datinje/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/datinje/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/datinje/subscriptions",
            "organizations_url": "https://api.github.com/users/datinje/orgs",
            "repos_url": "https://api.github.com/users/datinje/repos",
            "events_url": "https://api.github.com/users/datinje/events{/privacy}",
            "received_events_url": "https://api.github.com/users/datinje/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-30T08:43:58Z",
        "updated_at": "2023-08-02T18:18:09Z",
        "author_association": "NONE",
        "body": "You need to load nvidia onnx graphsurgeon for tensorrt \r\n(tensor rt is more strict than onnx runtime for onnx model to run : in the faster-rcnn model from detectron2 model zoo , it does not like an if-then-else branch to return a tensor with different dimensions , so the script below just fixes this. After that you can run trtexec.\r\n\r\nrun as : graph_surgeon_faster_rcnn.py --onnx   faster_rcnn_fpn.onnx --output onnx_output \r\nthen run : trtexec --onnx=faster_rcnn_fpn-graph_surgeon.onnx --verbose --saveEngine=faster_rcnn_fpn-graph_surgeon.trt\r\n\r\nBefore that ,  you need to install the following from nvidia \r\n```bash\r\n# INSTALL PYTHON MODULES polygraphy, tensorrt, onnx_graphsurgeon\r\npython -m pip install colored polygraph --trusted-host pypi.ngc.nvidia.com --extra-index-url https://pypi.ngc.nvidia.com\r\n# and build onnx-graphsurgeon since the pip installer did not work for me \r\n# pip install onnx_graphsurgeon>=0.3.21 --trusted-host pypi.ngc.nvidia.com  --extra-index-url=https://pypi.ngc.nvidia.com\r\ngit clone https://github.com/NVIDIA/TensorRT.git\r\ncd TensorRT/tools/onnx-graphsurgeon && make build && python3 -m pip install dist/onnx_graphsurgeon-*-py2.py3-none-any.whl\r\n```\r\nhere is the graph_surgeon_faster_rcnn.py (tested on python 3.10)\r\n(I provide the reference to the nice person from nvidia who helped me)\r\n```python\r\n#! /usr/bin/env python3\r\n## ---------------------------------------------------------------------------\r\n##\r\n## File: graph_surgeon_faster_rcnn.py for detectron2 faster-rcnn model zoo\r\n##\r\n## Created by Zhijin Li\r\n## E-mail:   <zhijinl@nvidia.com>\r\n##\r\n## Started on  Mon Jul 17 15:53:26 2023 Zhijin Li\r\n## Last update Tue Jul 18 02:08:41 2023 Zhijin Li\r\n## Modified    Sun Jul 30                2023 JC Datin\r\n## ---------------------------------------------------------------------------\r\nimport onnx\r\nimport onnx_graphsurgeon as gs\r\n\r\nimport os\r\nimport urllib\r\nimport argparse\r\nimport numpy as np\r\n\r\nfrom detectron2 import model_zoo\r\nfrom detectron2.config import get_cfg\r\nfrom detectron2.utils.logger import setup_logger\r\n\r\ndef parse_arguments():\r\n  \"\"\"\r\n  Parse input command line arguments.\r\n\r\n  Returns\r\n  ----------\r\n  Parsed argument object.\r\n\r\n  \"\"\"\r\n  parser = argparse.ArgumentParser(\r\n    description='graph surgeon an ONNX model for tensorRT conversion.')\r\n\r\n  parser.add_argument(\r\n    '--onnx',\r\n    type=str,\r\n    required=True,\r\n    help='Path to ONNX model')\r\n\r\n  parser.add_argument(\r\n    '--output',\r\n    type=str,\r\n    default='./output',\r\n    help='output directory for the graph surgeoned model')\r\n\r\n  parser.add_argument(\r\n    'opts',\r\n    help='Modify config options using the command-line',\r\n    default=None,\r\n    nargs=argparse.REMAINDER,\r\n  )\r\n\r\n  args = parser.parse_args()\r\n  return args\r\n\r\ndef sanitize(graph):\r\n  \"\"\"\r\n  Sanitize the graph by cleaning any unconnected nodes, do a topological resort, and fold constant inputs values.\r\n  When possible, run shape inference on the ONNX graph to determine tensor shapes.\r\n  \"\"\"\r\n\r\n  for i in range(3):\r\n\r\n    count_before = len(graph.nodes)\r\n    graph.cleanup().toposort()\r\n\r\n    try:\r\n      for node in graph.nodes:\r\n        for o in node.outputs:\r\n          o.shape = None\r\n\r\n      model = gs.export_onnx(graph)\r\n      model = shape_inference.infer_shapes(model)\r\n      graph = gs.import_onnx(model)\r\n\r\n    except Exception as e:\r\n      log.info(\"Shape inference could not be performed at this time:\\n{}\".format(e))\r\n\r\n    try:\r\n      graph.fold_constants(fold_shapes=True)\r\n    except TypeError as e:\r\n      log.error(\"This version of ONNX GraphSurgeon does not support folding shapes, please upgrade your \"\r\n                \"onnx_graphsurgeon module. Error:\\n{}\".format(e))\r\n      raise\r\n\r\n    count_after = len(graph.nodes)\r\n    if count_before == count_after:\r\n      # No new folding occurred in this iteration, so we can stop for now.\r\n      break\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n  logger = setup_logger()\r\n  args = parse_arguments()\r\n\r\n   # Load graph\r\n\r\n  logger.info(\"loading ONNX model: {}\".format(args.onnx))\r\n  original_model = args.onnx\r\n  graph = gs.import_onnx(onnx.load(original_model))\r\n  surgeoned_model_path = original_model.rsplit('.', 1)[0]\r\n  surgeoned_model_name = surgeoned_model_path.rsplit('/', 1)[-1] \r\n  surgeoned_model= surgeoned_model_name + \"-graph_surgeon\" + \".onnx\"\r\n  print(surgeoned_model_path)\r\n  print(surgeoned_model_name)\r\n  print(surgeoned_model)\r\n\r\n  graph = graph.cleanup().toposort()\r\n  graph = graph.fold_constants().cleanup()\r\n\r\n  # print(graph)\r\n\r\n  # for node in graph.nodes:\r\n  #   print(node)\r\n\r\n  for node in graph.nodes:\r\n    for o in node.outputs:\r\n      o.shape = None\r\n\r\n  model = gs.export_onnx(graph)\r\n  model = onnx.shape_inference.infer_shapes(model)\r\n  graph = gs.import_onnx(model)\r\n\r\n  # print(graph)\r\n\r\n  ###### FIX: /model/roi_heads/pooler/level_poolers.0/If\r\n  for node in graph.nodes:\r\n    if node.name == '/model/roi_heads/box_pooler/level_poolers.0/If':\r\n\r\n      else_branch_graph = node.attrs['else_branch']\r\n\r\n      squeeze_axes = gs.Constant(\r\n        name='axes',\r\n        values=np.array([1]))\r\n      squeeze_node = gs.Node(\r\n        \"Squeeze\",\r\n        name=\"squeeze_fix\")\r\n      squeeze_node.inputs = [*else_branch_graph.outputs, squeeze_axes]\r\n      squeeze_node.outputs = [gs.Variable('squeeze_fix_output', dtype=np.float32)]\r\n\r\n      node.attrs['else_branch'].nodes = [*node.attrs['else_branch'].nodes, squeeze_node]\r\n      node.attrs['else_branch'].outputs = squeeze_node.outputs\r\n      \r\n    if node.name == '/model/roi_heads/box_pooler/level_poolers.1/If':\r\n\r\n      else_branch_graph = node.attrs['else_branch']\r\n\r\n      squeeze_axes = gs.Constant(\r\n        name='axes',\r\n        values=np.array([1]))\r\n      squeeze_node = gs.Node(\r\n        \"Squeeze\",\r\n        name=\"squeeze_fix\")\r\n      squeeze_node.inputs = [*else_branch_graph.outputs, squeeze_axes]\r\n      squeeze_node.outputs = [gs.Variable('squeeze_fix_output', dtype=np.float32)]\r\n\r\n      node.attrs['else_branch'].nodes = [*node.attrs['else_branch'].nodes, squeeze_node]\r\n      node.attrs['else_branch'].outputs = squeeze_node.outputs\r\n\r\n    if node.name == '/model/roi_heads/box_pooler/level_poolers.2/If':\r\n\r\n      else_branch_graph = node.attrs['else_branch']\r\n\r\n      squeeze_axes = gs.Constant(\r\n        name='axes',\r\n        values=np.array([1]))\r\n      squeeze_node = gs.Node(\r\n        \"Squeeze\",\r\n        name=\"squeeze_fix\")\r\n      squeeze_node.inputs = [*else_branch_graph.outputs, squeeze_axes]\r\n      squeeze_node.outputs = [gs.Variable('squeeze_fix_output', dtype=np.float32)]\r\n\r\n      node.attrs['else_branch'].nodes = [*node.attrs['else_branch'].nodes, squeeze_node]\r\n      node.attrs['else_branch'].outputs = squeeze_node.outputs\r\n      \r\n    if node.name == '/model/roi_heads/box_pooler/level_poolers.3/If':\r\n\r\n      else_branch_graph = node.attrs['else_branch']\r\n\r\n      squeeze_axes = gs.Constant(\r\n        name='axes',\r\n        values=np.array([1]))\r\n      squeeze_node = gs.Node(\r\n        \"Squeeze\",\r\n        name=\"squeeze_fix\")\r\n      squeeze_node.inputs = [*else_branch_graph.outputs, squeeze_axes]\r\n      squeeze_node.outputs = [gs.Variable('squeeze_fix_output', dtype=np.float32)]\r\n\r\n      node.attrs['else_branch'].nodes = [*node.attrs['else_branch'].nodes, squeeze_node]\r\n      node.attrs['else_branch'].outputs = squeeze_node.outputs\r\n\r\n  ###### FIX: If_810\r\n  for node in graph.nodes:\r\n    if node.name == 'If_782':\r\n\r\n      sub_graph = node.attrs['else_branch']\r\n\r\n      for sub_node in sub_graph.nodes:\r\n        if sub_node.name == 'If_810':\r\n\r\n          else_branch_graph_810 = sub_node.attrs['else_branch']\r\n\r\n          squeeze_axes = gs.Constant(\r\n            name='axes',\r\n            values=np.array([1]))\r\n          squeeze_node = gs.Node(\r\n            \"Squeeze\",\r\n            name=\"squeeze_fix_if_810\")\r\n          squeeze_node.inputs = [*else_branch_graph_810.outputs, squeeze_axes]\r\n          squeeze_node.outputs = [gs.Variable('squeeze_810_fix_output', dtype=np.int64)]\r\n\r\n          sub_node.attrs['else_branch'].nodes = [*sub_node.attrs['else_branch'].nodes, squeeze_node]\r\n          sub_node.attrs['else_branch'].outputs = squeeze_node.outputs\r\n\r\n\r\n  graph = graph.cleanup().toposort()\r\n  graph = graph.fold_constants().cleanup()\r\n\r\n  model = gs.export_onnx(graph)\r\n  model = onnx.shape_inference.infer_shapes(model)\r\n  graph = gs.import_onnx(model)\r\n\r\n  for node in graph.nodes:\r\n    if node.name == '/model/roi_heads/box_pooler/level_poolers.0/If':\r\n       print (node)\r\n\r\n  save_path = os.path.join(args.output, surgeoned_model)\r\n  onnx.save(model, save_path)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1657081131/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1657081495",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1657081495",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1657081495,
        "node_id": "IC_kwDOCVq1mM5ixQ6X",
        "user": {
            "login": "datinje",
            "id": 43935204,
            "node_id": "MDQ6VXNlcjQzOTM1MjA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/43935204?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/datinje",
            "html_url": "https://github.com/datinje",
            "followers_url": "https://api.github.com/users/datinje/followers",
            "following_url": "https://api.github.com/users/datinje/following{/other_user}",
            "gists_url": "https://api.github.com/users/datinje/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/datinje/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/datinje/subscriptions",
            "organizations_url": "https://api.github.com/users/datinje/orgs",
            "repos_url": "https://api.github.com/users/datinje/repos",
            "events_url": "https://api.github.com/users/datinje/events{/privacy}",
            "received_events_url": "https://api.github.com/users/datinje/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-07-30T08:46:03Z",
        "updated_at": "2023-07-30T08:46:03Z",
        "author_association": "NONE",
        "body": "sorry , closed by mistake after entering previous comment",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1657081495/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1661680316",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1661680316",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1661680316,
        "node_id": "IC_kwDOCVq1mM5jCzq8",
        "user": {
            "login": "datinje",
            "id": 43935204,
            "node_id": "MDQ6VXNlcjQzOTM1MjA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/43935204?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/datinje",
            "html_url": "https://github.com/datinje",
            "followers_url": "https://api.github.com/users/datinje/followers",
            "following_url": "https://api.github.com/users/datinje/following{/other_user}",
            "gists_url": "https://api.github.com/users/datinje/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/datinje/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/datinje/subscriptions",
            "organizations_url": "https://api.github.com/users/datinje/orgs",
            "repos_url": "https://api.github.com/users/datinje/repos",
            "events_url": "https://api.github.com/users/datinje/events{/privacy}",
            "received_events_url": "https://api.github.com/users/datinje/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-02T07:44:23Z",
        "updated_at": "2023-08-02T07:44:23Z",
        "author_association": "NONE",
        "body": "to be clear : issue is still open : I guess we are waiting for the ort team to check it is running with native TRT but not on ORT + TRT EP demonstrating the pb is on ONRT TRT EP.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1661680316/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1662842445",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1662842445",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1662842445,
        "node_id": "IC_kwDOCVq1mM5jHPZN",
        "user": {
            "login": "yf711",
            "id": 109183385,
            "node_id": "U_kgDOBoIBmQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/109183385?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yf711",
            "html_url": "https://github.com/yf711",
            "followers_url": "https://api.github.com/users/yf711/followers",
            "following_url": "https://api.github.com/users/yf711/following{/other_user}",
            "gists_url": "https://api.github.com/users/yf711/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yf711/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yf711/subscriptions",
            "organizations_url": "https://api.github.com/users/yf711/orgs",
            "repos_url": "https://api.github.com/users/yf711/repos",
            "events_url": "https://api.github.com/users/yf711/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yf711/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-02T19:18:04Z",
        "updated_at": "2023-08-02T19:23:00Z",
        "author_association": "MEMBER",
        "body": "@datinje Thanks for providing the script that using onnx-graphsurgeon.\r\n\r\nHave you seen any inference failure after running  `trtexec --onnx=faster_rcnn_fpn-graph_surgeon.onnx --verbose --saveEngine=faster_rcnn_fpn-graph_surgeon.trt`? \r\nThe native trt inference failed at:\r\n```bash\r\n[08/02/2023-19:15:46] [I] Starting inference\r\n[08/02/2023-19:15:46] [E] Error[7]: [shapeMachine.cpp::executeContinuation::864] Error Code 7: Internal Error (If_1530_OutputLayer: dimensions not compatible for if-conditional outputs Condition '==' violated: 0 != 6. Instruction: CHECK_EQUAL 0 6.)\r\n[08/02/2023-19:15:46] [E] Error occurred during inference\r\n```\r\nOr would it be convenient to share `faster_rcnn_fpn-graph_surgeon.onnx` that you had tested?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1662842445/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1663942800",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1663942800",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1663942800,
        "node_id": "IC_kwDOCVq1mM5jLcCQ",
        "user": {
            "login": "datinje",
            "id": 43935204,
            "node_id": "MDQ6VXNlcjQzOTM1MjA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/43935204?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/datinje",
            "html_url": "https://github.com/datinje",
            "followers_url": "https://api.github.com/users/datinje/followers",
            "following_url": "https://api.github.com/users/datinje/following{/other_user}",
            "gists_url": "https://api.github.com/users/datinje/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/datinje/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/datinje/subscriptions",
            "organizations_url": "https://api.github.com/users/datinje/orgs",
            "repos_url": "https://api.github.com/users/datinje/repos",
            "events_url": "https://api.github.com/users/datinje/events{/privacy}",
            "received_events_url": "https://api.github.com/users/datinje/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-03T13:00:07Z",
        "updated_at": "2023-08-03T13:00:07Z",
        "author_association": "NONE",
        "body": "My mistake , I did not realize trtexec also failed on a similar node issue on the demonstrator  after all node surgeon -albeit towards the end.  trtexec worked on my real project once itself surgeoned. \r\nIf_1595_OutputLayer: dimensions not compatible for if-conditional outputs Condition '==' violated: 0 != 6. Instruction: CHECK_EQUAL 0 6.)\r\nI have to find a way to surgeon this node : according to Netron one of the if node block is also missing a fix to get the same dimensions on all blocks.\r\nMeanwhile : I will propose a much simpler reduceMax demonstrater.  ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1663942800/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1664190268",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16886#issuecomment-1664190268",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16886",
        "id": 1664190268,
        "node_id": "IC_kwDOCVq1mM5jMYc8",
        "user": {
            "login": "jywu-msft",
            "id": 43355415,
            "node_id": "MDQ6VXNlcjQzMzU1NDE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/43355415?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jywu-msft",
            "html_url": "https://github.com/jywu-msft",
            "followers_url": "https://api.github.com/users/jywu-msft/followers",
            "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}",
            "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions",
            "organizations_url": "https://api.github.com/users/jywu-msft/orgs",
            "repos_url": "https://api.github.com/users/jywu-msft/repos",
            "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jywu-msft/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-08-03T15:24:45Z",
        "updated_at": "2023-08-03T15:24:45Z",
        "author_association": "MEMBER",
        "body": "> My mistake , I did not realize trtexec also failed on a similar node issue on the demonstrator after all node surgeon -albeit towards the end. trtexec worked on my real project once itself surgeoned. If_1595_OutputLayer: dimensions not compatible for if-conditional outputs Condition '==' violated: 0 != 6. Instruction: CHECK_EQUAL 0 6.) I have to find a way to surgeon this node : according to Netron one of the if node block is also missing a fix to get the same dimensions on all blocks. Meanwhile : I will propose a much simpler reduceMax demonstrater.\r\n\r\nThanks for confirming. We await your new repro test case. If an onnx model runs on native TensorRT (trtexec), it should also run with OnnxRuntime TensorRT EP. If it doesn't , that means there's a bug in OnnxRuntime TensorRT EP. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1664190268/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]