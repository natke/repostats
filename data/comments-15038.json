[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468498619",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15038#issuecomment-1468498619",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15038",
        "id": 1468498619,
        "node_id": "IC_kwDOCVq1mM5Xh4K7",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T17:09:21Z",
        "updated_at": "2023-03-14T17:23:53Z",
        "author_association": "MEMBER",
        "body": "If your model has some operator need accumulation (like Softmax, LayerNormalization etc), CUDA result could be slightly different if partition has changed. Even without multi-threading, you can observe this when you run same inputs multiple times, and measure the variance of outputs. \r\n\r\nI guess Multithread GPU prediction might cause GPU changes its partition more frequently. For example, when some cores are used by other thread, then GPU might schedule less cores for new requests. That might cause minor change in accuracy.\r\n\r\nAnother possible cause is convolution algo tuning, which might depend on GPU memory free space. If you use multi-threading, that means each thread might use less GPU memory since some memory is used by other threads, then convolution algo might change because some algo might need more memory to run. Unlike [PyTorch](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms), ORT does not have option to choose deterministic algo right now, so nondeterministic algorithms might be selected.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468498619/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468696884",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15038#issuecomment-1468696884",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15038",
        "id": 1468696884,
        "node_id": "IC_kwDOCVq1mM5Xiok0",
        "user": {
            "login": "mg-yolo-enterprises",
            "id": 117874236,
            "node_id": "U_kgDOBwaePA",
            "avatar_url": "https://avatars.githubusercontent.com/u/117874236?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mg-yolo-enterprises",
            "html_url": "https://github.com/mg-yolo-enterprises",
            "followers_url": "https://api.github.com/users/mg-yolo-enterprises/followers",
            "following_url": "https://api.github.com/users/mg-yolo-enterprises/following{/other_user}",
            "gists_url": "https://api.github.com/users/mg-yolo-enterprises/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mg-yolo-enterprises/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mg-yolo-enterprises/subscriptions",
            "organizations_url": "https://api.github.com/users/mg-yolo-enterprises/orgs",
            "repos_url": "https://api.github.com/users/mg-yolo-enterprises/repos",
            "events_url": "https://api.github.com/users/mg-yolo-enterprises/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mg-yolo-enterprises/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T19:22:26Z",
        "updated_at": "2023-03-14T19:22:26Z",
        "author_association": "NONE",
        "body": "I appreciate your response! Unfortunately I'm not sure it gets to the root of this issue, because the issues I'm experiencing are not slight differences.\r\n\r\nHere's an experiment I set up this morning:\r\n- Run ~5000 images using GPU and Parallel.ForEach. The images are all part of the same class (Class 2 of 2), so Output.Last() should always be greater than Output.First().\r\n- I ran this experiment using GPU and ForEach, and all predictions were correct.\r\n- I then re-ran this experiment several times using GPU and Parallel.ForEach, which runs much faster due to parallelism. The prediction accuracy was no longer 100%. Out of 5000 images, I would usually see around 10 images whose scores were completely wrong. If I reran the exact same tensor object through a subsequent call to session.Run(). the results were correct. Further details below:\r\n\r\nIn the cases where an incorrect prediction has been given, if I re-run the exact same tensor a second time, the result is correct. Here's an example...\r\n\r\nFor the following block of code, with a breakpoint set as shown:\r\n![image](https://user-images.githubusercontent.com/117874236/225103488-d23bd4c0-c479-4bb5-976a-be81c6272b93.png)\r\n\r\n...the first call to session.Run() produces a completely different result than the second. The first is incorrect, the second is correct:\r\n![image](https://user-images.githubusercontent.com/117874236/225103660-2c713304-845b-49d9-a612-91c3e896c553.png)\r\n\r\nIn the screenshot above, the first call to session.Run() results in a 96% score for class 1 of 2, which is wrong. Calling session.Run() a second time with the same List<NamedOnnxValue> produces a correct prediction.\r\n\r\nI'm only able to catch this behavior when running thousands of datasets, using GPU, using Parallel.ForEach.\r\n\r\nNote that in C# Parallel.ForEach, it is possible to provide a parameter MaxDegreeOfParallelism, which limits the number of concurrent threads operating. If this value is _not_ set, the loop runs as fast as possible and the problems described above are experienced. But, I found that if I set MaxDegreeOfParallelism to 1 or 2, I never encountered any incorrect predictions. Any value 3 or greater (or no value set) produces some incorrect predictions, and the number of incorrect predictions increases as the MaxDegreeOfParallelism increases.\r\n\r\nIt looks like there's plenty of GPU free memory while running:\r\n![image](https://user-images.githubusercontent.com/117874236/225104325-12c89cba-f06c-4611-a900-10906e0cc75f.png)\r\n\r\n**Are there any reasons why a tensor passed to session.Run(), which results in an incorrect prediction, could result in a very different (correct) prediction when passed a second time? Keeping in mind that the incorrect prediction behavior disappears with any of the following changes:**\r\n- Changing GPU package to CPU\r\n- Using GPU with <=2 parallel foreach loops\r\n\r\nIt's desirable to solve this problem, because with GPU and 2 concurrent threads, the framerate is around 44fps. Allowing unlimited loops reaches 189fps, but with about 1 incorrect prediction per 500 frames.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1468696884/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1470612231",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15038#issuecomment-1470612231",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15038",
        "id": 1470612231,
        "node_id": "IC_kwDOCVq1mM5Xp8MH",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-15T18:58:42Z",
        "updated_at": "2023-03-15T19:48:54Z",
        "author_association": "MEMBER",
        "body": "@mg-yolo-enterprises, could you try the following: Create multiple inference sessions of the model, and parallel inference of these sessions. No parallel within each session: sequential inference of images within a session. If it could reproduce accuracy loss, the root cause is what I described previously.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1470612231/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1472968199",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15038#issuecomment-1472968199",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15038",
        "id": 1472968199,
        "node_id": "IC_kwDOCVq1mM5Xy7YH",
        "user": {
            "login": "mg-yolo-enterprises",
            "id": 117874236,
            "node_id": "U_kgDOBwaePA",
            "avatar_url": "https://avatars.githubusercontent.com/u/117874236?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mg-yolo-enterprises",
            "html_url": "https://github.com/mg-yolo-enterprises",
            "followers_url": "https://api.github.com/users/mg-yolo-enterprises/followers",
            "following_url": "https://api.github.com/users/mg-yolo-enterprises/following{/other_user}",
            "gists_url": "https://api.github.com/users/mg-yolo-enterprises/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mg-yolo-enterprises/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mg-yolo-enterprises/subscriptions",
            "organizations_url": "https://api.github.com/users/mg-yolo-enterprises/orgs",
            "repos_url": "https://api.github.com/users/mg-yolo-enterprises/repos",
            "events_url": "https://api.github.com/users/mg-yolo-enterprises/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mg-yolo-enterprises/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-17T01:15:24Z",
        "updated_at": "2023-03-17T01:15:24Z",
        "author_association": "NONE",
        "body": "I ended up putting a simple Lock() around the call to session.Run, which eliminated the problem I was experiencing of accuracy reduction during parallel inferences, without sacrificing any performance - probably because the preprocessing steps are my main bottleneck.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1472968199/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]