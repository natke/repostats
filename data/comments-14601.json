[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1419513960",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14601#issuecomment-1419513960",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14601",
        "id": 1419513960,
        "node_id": "IC_kwDOCVq1mM5UnBBo",
        "user": {
            "login": "guschmue",
            "id": 22941064,
            "node_id": "MDQ6VXNlcjIyOTQxMDY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/22941064?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guschmue",
            "html_url": "https://github.com/guschmue",
            "followers_url": "https://api.github.com/users/guschmue/followers",
            "following_url": "https://api.github.com/users/guschmue/following{/other_user}",
            "gists_url": "https://api.github.com/users/guschmue/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guschmue/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guschmue/subscriptions",
            "organizations_url": "https://api.github.com/users/guschmue/orgs",
            "repos_url": "https://api.github.com/users/guschmue/repos",
            "events_url": "https://api.github.com/users/guschmue/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guschmue/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-06T17:59:37Z",
        "updated_at": "2023-02-06T17:59:37Z",
        "author_association": "MEMBER",
        "body": "quantization should work with ort-web when using wasm. \r\nfp16 should work but is going to be real slow.\r\nIs there a specific model that doesn't work?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1419513960/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1421016195",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14601#issuecomment-1421016195",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14601",
        "id": 1421016195,
        "node_id": "IC_kwDOCVq1mM5UsvyD",
        "user": {
            "login": "jkla139",
            "id": 44624598,
            "node_id": "MDQ6VXNlcjQ0NjI0NTk4",
            "avatar_url": "https://avatars.githubusercontent.com/u/44624598?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jkla139",
            "html_url": "https://github.com/jkla139",
            "followers_url": "https://api.github.com/users/jkla139/followers",
            "following_url": "https://api.github.com/users/jkla139/following{/other_user}",
            "gists_url": "https://api.github.com/users/jkla139/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jkla139/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jkla139/subscriptions",
            "organizations_url": "https://api.github.com/users/jkla139/orgs",
            "repos_url": "https://api.github.com/users/jkla139/repos",
            "events_url": "https://api.github.com/users/jkla139/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jkla139/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-07T16:02:02Z",
        "updated_at": "2023-02-07T16:05:34Z",
        "author_association": "NONE",
        "body": "> quantization should work with ort-web when using wasm. fp16 should work but is going to be real slow. Is there a specific model that doesn't work?\r\n\r\nFor quantization,\r\n\r\nwebgl does not support as in [js/web/docs/operators.md](https://github.com/microsoft/onnxruntime/blob/main/js/web/docs/operators.md),\r\n\r\nand if use wasm as backend to run quantizated model,\r\n`<script src=\"https://cdn.bootcdn.net/ajax/libs/onnxruntime-web/1.13.1/ort.min.js\"></script>`\r\n\r\ntry {\r\n      ......\r\n      Inferer.session = await ort.InferenceSession.create(onnx, sessionOption);\r\n      ......\r\n);\r\n} catch (e) {\r\n      document.write(`failed to create session: ${e}.`);\r\n      console.log('e.stack:', e.stack);\r\n}\r\n\r\nI catch an exception without any useful error message, and get a number:\r\n![image](https://user-images.githubusercontent.com/44624598/217289217-b381225a-2604-4579-ad6f-057cf2730019.png)\r\nthe call stack is undefined:\r\n![image](https://user-images.githubusercontent.com/44624598/217289353-9f9369ea-4ccf-43ff-b55d-82d02e40e78b.png)\r\n\r\n==========================================\r\n\r\nFor fp16,\r\n\r\n`const results = await Inferer.session.run(feeds);`\r\n\r\nraise this error:\r\n![image](https://user-images.githubusercontent.com/44624598/217294583-fa5fa74e-df6b-4b1a-bc45-3f3984cca032.png)\r\n\r\nThe input type is float32. Does it caused by wrong type inputs ?\r\nBut there are no float16 [TensorConstructor](https://onnxruntime.ai/docs/api/js/interfaces/TensorConstructor.html) ...\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1421016195/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1421570075",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14601#issuecomment-1421570075",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14601",
        "id": 1421570075,
        "node_id": "IC_kwDOCVq1mM5Uu3Ab",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-07T22:41:10Z",
        "updated_at": "2023-02-07T22:41:10Z",
        "author_association": "MEMBER",
        "body": "ONNX Runtime Web does not support float16 as input types. It still works if there are float16 initializers inside the model. we cannot support as there is no such typed array `Float16Array` in JavaScript",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1421570075/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1422238122",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14601#issuecomment-1422238122",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14601",
        "id": 1422238122,
        "node_id": "IC_kwDOCVq1mM5UxaGq",
        "user": {
            "login": "jkla139",
            "id": 44624598,
            "node_id": "MDQ6VXNlcjQ0NjI0NTk4",
            "avatar_url": "https://avatars.githubusercontent.com/u/44624598?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jkla139",
            "html_url": "https://github.com/jkla139",
            "followers_url": "https://api.github.com/users/jkla139/followers",
            "following_url": "https://api.github.com/users/jkla139/following{/other_user}",
            "gists_url": "https://api.github.com/users/jkla139/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jkla139/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jkla139/subscriptions",
            "organizations_url": "https://api.github.com/users/jkla139/orgs",
            "repos_url": "https://api.github.com/users/jkla139/repos",
            "events_url": "https://api.github.com/users/jkla139/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jkla139/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-08T08:47:27Z",
        "updated_at": "2023-02-08T08:49:11Z",
        "author_association": "NONE",
        "body": "> ONNX Runtime Web does not support float16 as input types. It still works if there are float16 initializers inside the model. we cannot support as there is no such typed array `Float16Array` in JavaScript\r\n\r\nYes, I know. But the input type in \"feeds\" is float32, and since the model is converted by code:\r\n`\r\nimport onnx\r\nfrom onnxmltools.utils import float16_converter\r\n\r\nmodel = onnx.load('model.onnx')\r\nmodel = float16_converter.convert_float_to_float16(model)\r\nonnx.save(model, 'model-fp16.onnx')\r\n`\r\n\r\nbut get the following error:\r\n![5a3949355dc2f1198f6b3c7ac91b9643_217294583-fa5fa74e-df6b-4b1a-bc45-3f3984cca032](https://user-images.githubusercontent.com/44624598/217478688-f6e9333b-19c7-42d6-8c33-cf87d9907924.png)\r\n\r\nSo I guess the error comes from incompatible input type.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1422238122/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1423005888",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14601#issuecomment-1423005888",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14601",
        "id": 1423005888,
        "node_id": "IC_kwDOCVq1mM5U0VjA",
        "user": {
            "login": "guschmue",
            "id": 22941064,
            "node_id": "MDQ6VXNlcjIyOTQxMDY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/22941064?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/guschmue",
            "html_url": "https://github.com/guschmue",
            "followers_url": "https://api.github.com/users/guschmue/followers",
            "following_url": "https://api.github.com/users/guschmue/following{/other_user}",
            "gists_url": "https://api.github.com/users/guschmue/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/guschmue/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/guschmue/subscriptions",
            "organizations_url": "https://api.github.com/users/guschmue/orgs",
            "repos_url": "https://api.github.com/users/guschmue/repos",
            "events_url": "https://api.github.com/users/guschmue/events{/privacy}",
            "received_events_url": "https://api.github.com/users/guschmue/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-08T17:44:24Z",
        "updated_at": "2023-02-08T17:44:24Z",
        "author_association": "MEMBER",
        "body": "If I read the error message correctly, the application passed in float32 but the model wanted float16, hence the error.\r\nJust no way in javascript to express the float16, even if onnxruntime-web supports it internally.\r\nOne could change the model to accept float32 and cast it to float16.\r\nFloat16 on CPU is not optimized in onnxruntime so it is really slow. You don't want to use it.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1423005888/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1534058216",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14601#issuecomment-1534058216",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14601",
        "id": 1534058216,
        "node_id": "IC_kwDOCVq1mM5bb97o",
        "user": {
            "login": "fdwr",
            "id": 1809166,
            "node_id": "MDQ6VXNlcjE4MDkxNjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1809166?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fdwr",
            "html_url": "https://github.com/fdwr",
            "followers_url": "https://api.github.com/users/fdwr/followers",
            "following_url": "https://api.github.com/users/fdwr/following{/other_user}",
            "gists_url": "https://api.github.com/users/fdwr/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fdwr/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fdwr/subscriptions",
            "organizations_url": "https://api.github.com/users/fdwr/orgs",
            "repos_url": "https://api.github.com/users/fdwr/repos",
            "events_url": "https://api.github.com/users/fdwr/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fdwr/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-04T03:47:18Z",
        "updated_at": "2023-05-04T03:47:18Z",
        "author_association": "MEMBER",
        "body": "I'm content just passing a bitwise representation of float16f10e5s1 via `Uint16Array` for now until we get a proper `Float16Array` (e.g. https://github.com/webmachinelearning/webnn/issues/373), so long as ORT can distinguish that it really is float16 (and not uint16) somehow when creating the `ort.Tensor`, but it's important to have float16 inputs/outputs for GPU's for a backend like [WebNN via DML](https://github.com/fdwr/chromium-src-webnn-dml/pull/1) where the model file size is 1/2 the size and can execute faster.\r\n\r\nI suppose as a work-around, we could temporarily hack the model to insert a Cast at the graph input and output, so that at least all the internal computation is float16.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1534058216/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]