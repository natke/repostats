[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499436639",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15394#issuecomment-1499436639",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15394",
        "id": 1499436639,
        "node_id": "IC_kwDOCVq1mM5ZX5Zf",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-06T18:07:44Z",
        "updated_at": "2023-04-06T18:24:49Z",
        "author_association": "NONE",
        "body": "Here is some data. Notice how the times leap up when you change the input size from 512 tokens to 511 tokens (even though the input is now smaller!) The test is done by doing 10 passes with 512 tokens followed by then 10 passes with 511 tokens:\r\n(The model used is [cerebras 111M](https://huggingface.co/cerebras/Cerebras-GPT-111M))\r\n\r\n**DIRECTML RESULTS**\r\n```\r\nfloat32 DirectML \r\n0.19ms 512\r\n0.18ms 512\r\n0.18ms 512\r\n0.19ms 512\r\n0.15ms 512\r\n0.15ms 512\r\n0.17ms 512\r\n0.18ms 512\r\n0.18ms 512\r\n0.18ms 512\r\n-------   (15% slow down)\r\n0.18ms 511\r\n0.18ms 511\r\n0.20ms 511\r\n0.21ms 511\r\n0.22ms 511\r\n0.18ms 511\r\n0.19ms 511\r\n0.20ms 511\r\n0.21ms 511\r\n0.22ms 511\r\n\r\nfloat16-directml\r\n0.11ms 512\r\n0.11ms 512\r\n0.11ms 512\r\n0.12ms 512\r\n0.12ms 512\r\n0.12ms 512\r\n0.12ms 512\r\n0.12ms 512\r\n0.12ms 512\r\n0.11ms 512\r\n---------- (36% slow down)\r\n0.15ms 511\r\n0.15ms 511\r\n0.15ms 511\r\n0.15ms 511\r\n0.16ms 511\r\n0.14ms 511\r\n0.14ms 511\r\n0.14ms 511\r\n0.19ms 511\r\n0.16ms 511\r\n\r\n\r\n\r\nint8 static quantization\r\n0.22ms 512\r\n0.25ms 512\r\n0.24ms 512\r\n0.24ms 512\r\n0.23ms 512\r\n0.23ms 512\r\n0.24ms 512\r\n0.24ms 512\r\n0.24ms 512\r\n0.22ms 512\r\n-------------- (33% slow down)\r\n0.33ms 511\r\n0.32ms 511\r\n0.30ms 511\r\n0.30ms 511\r\n0.32ms 511\r\n0.30ms 511\r\n0.30ms 511\r\n0.32ms 511\r\n0.32ms 511\r\n0.31ms 511\r\n\r\nint8 dynamic quantization\r\n0.22ms 512\r\n0.24ms 512\r\n0.24ms 512\r\n0.23ms 512\r\n0.23ms 512\r\n0.24ms 512\r\n0.24ms 512\r\n0.24ms 512\r\n0.23ms 512\r\n0.23ms 512\r\n------------    (20% slow down)\r\n0.29ms 511\r\n0.29ms 511\r\n0.29ms 511\r\n0.28ms 511\r\n0.28ms 511\r\n0.25ms 511\r\n0.27ms 511\r\n0.28ms 511\r\n0.28ms 511\r\n0.26ms 511\r\n```\r\n\r\nAs you can see in DirectML changing the size of the input leads to a significant slow down that it never recovers from. When changing the input size several times I have found up to 2x slow down in performance. Is there a way round this for example, allocating sufficient memory on the GPU? I think this might be a DirectML only problem.\r\n\r\nPadding the input is not a great solution because we want to take advantage of when the inputs are small to reduce times.\r\n\r\n**CUDA RESULTS**\r\n```\r\nfloat32-cuda\r\n0.11ms 512\r\n0.11ms 512\r\n0.11ms 512\r\n0.12ms 512\r\n0.10ms 512\r\n0.10ms 512\r\n0.11ms 512\r\n0.12ms 512\r\n0.11ms 512\r\n0.11ms 512\r\n------------  0% slowdown\r\n0.11ms 511\r\n0.12ms 511\r\n0.11ms 511\r\n0.10ms 511\r\n0.10ms 511\r\n0.11ms 511\r\n0.11ms 511\r\n0.11ms 511\r\n0.11ms 511\r\n0.12ms 511\r\n\r\nfloat16-cuda\r\n0.07ms 512\r\n0.06ms 512\r\n0.07ms 512\r\n0.07ms 512\r\n0.07ms 512\r\n0.07ms 512\r\n0.07ms 512\r\n0.07ms 512\r\n0.07ms 512\r\n0.06ms 512\r\n------------- 0% slowdown\r\n0.06ms 511\r\n0.06ms 511\r\n0.06ms 511\r\n0.06ms 511\r\n0.07ms 511\r\n0.07ms 511\r\n0.07ms 511\r\n0.06ms 511\r\n0.07ms 511\r\n0.06ms 511\r\n```\r\n\r\nAs you can see, for CUDA there is 0% slow down when changing the input sizes. i.e. number of tokens.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499436639/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499545826",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15394#issuecomment-1499545826",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15394",
        "id": 1499545826,
        "node_id": "IC_kwDOCVq1mM5ZYUDi",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-06T19:58:02Z",
        "updated_at": "2023-04-06T20:57:37Z",
        "author_association": "NONE",
        "body": "**I have made a new discovery!** ðŸ˜¯ (Using DirectML, dynamic quantized model) Seems to only occur with certain big models...\r\n\r\nWhen you continually increase the input each time, the inference rate slowly increases until it reaches a **power of 2** starting at 32:\r\n\r\n`32, 64, 128,....`\r\n\r\nThen there is a dramatic slow down at each of these input sizes.\r\n\r\nSo it seems like, what it is doing it is thinking \"the user is changing using a dynamic input size so I better increase the input memory to the next highest power of 2\"\r\n\r\nThus DirectML is allocating memory in terms of powers of 2. \r\n\r\nThis seems to slow down the inference as if the input is padded to the next power of 2 in size. \r\n\r\nWhen ever you increase the input size beyond the next power of two you get a 50% performance hit from then on.\r\n\r\nThis behaviour is not documented. Perhaps it is designed this way to be based internally on powers of 2-textures.\r\n\r\nIt would be nice if this could be turned off.\r\n\r\n\r\n\r\nThis is bad because if you have an input of size 129 it is as slow as an input of size 256 or even 512.\r\n\r\ncerebras 1.3B dynamic quantized to int8:\r\n```\r\n0.09ms 1 \r\n0.18ms 2 ðŸ”¥\r\n0.17ms 3\r\n0.17ms 4\r\n0.17ms 5\r\n0.18ms 6\r\n0.17ms 7\r\n0.18ms 8\r\n0.20ms 9\r\n0.17ms 10\r\n0.18ms 11\r\n0.18ms 12\r\n0.18ms 13\r\n0.18ms 14\r\n0.19ms 15\r\n0.19ms 16\r\n0.22ms 17\r\n0.20ms 18\r\n0.21ms 19\r\n0.21ms 20\r\n0.21ms 21\r\n0.20ms 22\r\n0.20ms 23\r\n0.21ms 24\r\n0.21ms 25\r\n0.21ms 26\r\n0.20ms 27\r\n0.20ms 28\r\n0.21ms 29\r\n0.22ms 30\r\n0.21ms 31\r\n0.21ms 32\r\n0.29ms 33 ðŸ”¥\r\n0.26ms 34\r\n0.27ms 35\r\n0.26ms 36\r\n0.26ms 37\r\n0.27ms 38\r\n0.26ms 39\r\n0.27ms 40\r\n0.28ms 41\r\n0.27ms 42\r\n0.27ms 43\r\n0.27ms 44\r\n0.27ms 45\r\n0.28ms 46\r\n0.27ms 47\r\n0.27ms 48\r\n0.28ms 49\r\n0.27ms 50\r\n0.28ms 51\r\n0.29ms 52\r\n0.28ms 53\r\n0.28ms 54\r\n0.28ms 55\r\n0.28ms 56\r\n0.29ms 57\r\n0.29ms 58\r\n0.29ms 59\r\n0.29ms 60\r\n0.28ms 61\r\n0.30ms 62\r\n0.30ms 63\r\n0.28ms 64\r\n0.40ms 65 ðŸ”¥\r\n0.39ms 66\r\n0.39ms 67\r\n0.39ms 68\r\n0.39ms 69\r\n0.39ms 70\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499545826/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]