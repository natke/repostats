[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/458693287",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/396#issuecomment-458693287",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/396",
        "id": 458693287,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ1ODY5MzI4Nw==",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-01-29T20:23:49Z",
        "updated_at": "2019-01-29T20:23:49Z",
        "author_association": "MEMBER",
        "body": "> I managed to build and take out onnxruntime for a spin and I had a bunch of absolute newbie questions. I apologize if this is not the right forum for this or if this is already available in the API docs.\r\n> \r\n> 1. Is it fair to say that we can use onnxruntime to build an inference application specifically for ONNX models, with some additional code on top ? (something similar to tensorflow-serving)\r\n**Answer** onnxruntime is a library to infer ONNX models. You're free to develop any application on top of it.\r\n\r\n> 2. We do not do any sort of training in onnxruntime. This is purely for running inference from the pre-trained ONNX model. Is that correct?\r\n**Answer** As of now this is correct.\r\n\r\n> 3. Question on concurrency: I noticed the API provides parallel execution with a session thread pool size. Is this like application threads as opposed to the concurrency that we can control by setting the OMP_NUM_THREADS (in case we use the MKL_DNN library as execution provider) How do these two relate? Does session_thread_pool_size override OMP_NUM_THREADS ?\r\n**Answer** As of now the session threadpool size affects parallel execution mode only and is independent of the OMP_NUM_THREADS.\r\n\r\n> 4. Is it possible to batch multiple input requests together while running it via the InferenceSession ?\r\n**Answer** Use the batch dimension in your input tensor.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/458693287/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/458737771",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/396#issuecomment-458737771",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/396",
        "id": 458737771,
        "node_id": "MDEyOklzc3VlQ29tbWVudDQ1ODczNzc3MQ==",
        "user": {
            "login": "minus-one",
            "id": 666952,
            "node_id": "MDQ6VXNlcjY2Njk1Mg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/666952?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/minus-one",
            "html_url": "https://github.com/minus-one",
            "followers_url": "https://api.github.com/users/minus-one/followers",
            "following_url": "https://api.github.com/users/minus-one/following{/other_user}",
            "gists_url": "https://api.github.com/users/minus-one/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/minus-one/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/minus-one/subscriptions",
            "organizations_url": "https://api.github.com/users/minus-one/orgs",
            "repos_url": "https://api.github.com/users/minus-one/repos",
            "events_url": "https://api.github.com/users/minus-one/events{/privacy}",
            "received_events_url": "https://api.github.com/users/minus-one/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-01-29T22:45:14Z",
        "updated_at": "2019-01-29T22:45:14Z",
        "author_association": "NONE",
        "body": "Thank you so much for replying to all the questions!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/458737771/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]