[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014616956",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1014616956",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1014616956,
        "node_id": "IC_kwDOCVq1mM48edN8",
        "user": {
            "login": "adepierre",
            "id": 24371370,
            "node_id": "MDQ6VXNlcjI0MzcxMzcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/24371370?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adepierre",
            "html_url": "https://github.com/adepierre",
            "followers_url": "https://api.github.com/users/adepierre/followers",
            "following_url": "https://api.github.com/users/adepierre/following{/other_user}",
            "gists_url": "https://api.github.com/users/adepierre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adepierre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adepierre/subscriptions",
            "organizations_url": "https://api.github.com/users/adepierre/orgs",
            "repos_url": "https://api.github.com/users/adepierre/repos",
            "events_url": "https://api.github.com/users/adepierre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adepierre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-17T14:43:04Z",
        "updated_at": "2022-01-17T14:43:29Z",
        "author_association": "NONE",
        "body": "I think the problem is that you measure CPU time instead of GPU time in the pytorch case. Cuda is asynchronous, so CPU timers like time.time() won't measure the right inference time. You can try the solution provided in [this pytorch discussion](https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964) and see if it's more consistent with onnx time.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014616956/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014769457",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1014769457",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1014769457,
        "node_id": "IC_kwDOCVq1mM48fCcx",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-17T17:37:02Z",
        "updated_at": "2022-01-17T17:37:02Z",
        "author_association": "NONE",
        "body": "> I think the problem is that you measure CPU time instead of GPU time in the pytorch case. Cuda is asynchronous, so CPU timers like time.time() won't measure the right inference time. You can try the solution provided in [this pytorch discussion](https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964) and see if it's more consistent with onnx time.\r\n\r\nThanks for the reply\r\nI tried what was mentioned in the link you provided by modifying my ```infer_pytorch(resnet)``` function as \r\n```\r\n   start = torch.cuda.Event(enable_timing=True)\r\n   end = torch.cuda.Event(enable_timing=True)\r\n   start.record()\r\n   for i in range(total_samples):\r\n      resnet.eval()\r\n      with torch.no_grad():\r\n         out = resnet(x)\r\n   end.record()\r\n   torch.cuda.synchronize()\r\n   print('Number of runs:', total_samples)\r\n   print(\"Average PyTorch {} Inference time = {} ms\".format(device.type, start.elapsed_time(end)/total_samples))  \r\n   ```\r\n   Unfortunately, still see **not much of a difference in the pytorch inference time** \r\n   ```\r\n   Average PyTorch cuda Inference time = 9.928244140625 ms\r\n   ```\r\n   The onnxruntime inference is still the same (around 48 ms), so its still around 5 times slower",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014769457/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014773186",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1014773186",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1014773186,
        "node_id": "IC_kwDOCVq1mM48fDXC",
        "user": {
            "login": "adepierre",
            "id": 24371370,
            "node_id": "MDQ6VXNlcjI0MzcxMzcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/24371370?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adepierre",
            "html_url": "https://github.com/adepierre",
            "followers_url": "https://api.github.com/users/adepierre/followers",
            "following_url": "https://api.github.com/users/adepierre/following{/other_user}",
            "gists_url": "https://api.github.com/users/adepierre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adepierre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adepierre/subscriptions",
            "organizations_url": "https://api.github.com/users/adepierre/orgs",
            "repos_url": "https://api.github.com/users/adepierre/repos",
            "events_url": "https://api.github.com/users/adepierre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adepierre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-17T17:43:29Z",
        "updated_at": "2022-01-17T17:44:07Z",
        "author_association": "NONE",
        "body": "I think you need to put the record and the synchronize inside your loop.  Something like that\r\n\r\n```python\r\n\r\nwith torch.no_grad():\r\n    resnet.eval()\r\n    for i in range(total_samples):\r\n        start.record()\r\n        out = resnet(x)\r\n        end.record()\r\n        torch.cuda.synchronize()\r\n        total_time += start.elapsed_time(end)\r\n``` ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014773186/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014777610",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1014777610",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1014777610,
        "node_id": "IC_kwDOCVq1mM48fEcK",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-17T17:51:20Z",
        "updated_at": "2022-01-17T17:51:20Z",
        "author_association": "NONE",
        "body": "> I think you need to put the record and the synchronize inside your loop. Something like that\r\n> \r\n> ```python\r\n> with torch.no_grad():\r\n>     resnet.eval()\r\n>     for i in range(total_samples):\r\n>         start.record()\r\n>         out = resnet(x)\r\n>         end.record()\r\n>         torch.cuda.synchronize()\r\n>         total_time += start.elapsed_time(end)\r\n> ```\r\n\r\nChanged it the way you have suggested. \r\nStill get it as,\r\n```Average PyTorch cuda Inference time = 9.747972169876098 ms```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014777610/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014778738",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1014778738",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1014778738,
        "node_id": "IC_kwDOCVq1mM48fEty",
        "user": {
            "login": "adepierre",
            "id": 24371370,
            "node_id": "MDQ6VXNlcjI0MzcxMzcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/24371370?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adepierre",
            "html_url": "https://github.com/adepierre",
            "followers_url": "https://api.github.com/users/adepierre/followers",
            "following_url": "https://api.github.com/users/adepierre/following{/other_user}",
            "gists_url": "https://api.github.com/users/adepierre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adepierre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adepierre/subscriptions",
            "organizations_url": "https://api.github.com/users/adepierre/orgs",
            "repos_url": "https://api.github.com/users/adepierre/repos",
            "events_url": "https://api.github.com/users/adepierre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adepierre/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-17T17:53:24Z",
        "updated_at": "2022-01-17T17:53:24Z",
        "author_association": "NONE",
        "body": "Ok so it's probably not that",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1014778738/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015986464",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1015986464",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1015986464,
        "node_id": "IC_kwDOCVq1mM48jrkg",
        "user": {
            "login": "edgchen1",
            "id": 18449977,
            "node_id": "MDQ6VXNlcjE4NDQ5OTc3",
            "avatar_url": "https://avatars.githubusercontent.com/u/18449977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/edgchen1",
            "html_url": "https://github.com/edgchen1",
            "followers_url": "https://api.github.com/users/edgchen1/followers",
            "following_url": "https://api.github.com/users/edgchen1/following{/other_user}",
            "gists_url": "https://api.github.com/users/edgchen1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/edgchen1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/edgchen1/subscriptions",
            "organizations_url": "https://api.github.com/users/edgchen1/orgs",
            "repos_url": "https://api.github.com/users/edgchen1/repos",
            "events_url": "https://api.github.com/users/edgchen1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/edgchen1/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-19T01:19:40Z",
        "updated_at": "2022-01-19T01:19:40Z",
        "author_association": "MEMBER",
        "body": "Is the input actually on GPU? In the code you provided, I see:\r\n`io_binding.bind_cpu_input(input_names, x)`\r\n\r\nSome other things to try:\r\nORT has some support for profiling: https://onnxruntime.ai/docs/performance/tune-performance.html#profiling-and-performance-report\r\n\r\nYou can also use [CUDA profiling tools](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) like nvprof to get more detailed GPU usage info.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015986464/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1018755693",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1018755693",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1018755693,
        "node_id": "IC_kwDOCVq1mM48uPpt",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-21T18:25:17Z",
        "updated_at": "2022-01-21T18:25:17Z",
        "author_association": "MEMBER",
        "body": "@nssrivathsa, it is not fair comparison since latency for ORT including copying input tensor from cpu to GPU, while pytorch does not. Could  you bind GPU input, and measure the latency again?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1018755693/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1019739997",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1019739997",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1019739997,
        "node_id": "IC_kwDOCVq1mM48x_9d",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-24T05:40:49Z",
        "updated_at": "2022-01-24T05:40:49Z",
        "author_association": "NONE",
        "body": "> Is the input actually on GPU? In the code you provided, I see: `io_binding.bind_cpu_input(input_names, x)`\r\n> \r\n> Some other things to try: ORT has some support for profiling: https://onnxruntime.ai/docs/performance/tune-performance.html#profiling-and-performance-report\r\n> \r\n> You can also use [CUDA profiling tools](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) like nvprof to get more detailed GPU usage info.\r\n\r\n@edgchen1 \r\n1) Yes, the input is on the GPU. io_binding.bind_cpu_input puts the input from the CPU to GPU as per 'Scenario 1' in this - http://www.xavierdupre.fr/app/onnxruntime/helpsphinx/api_summary.html under IOBinding. Nevertheless, I have modified it as \r\n```\r\ndata = onnxruntime.OrtValue.ortvalue_from_numpy(x, device.type, 0)\r\nio_binding.bind_input(input_names, device, 0, np.float32, [batch_size, 3, 224, 224], data.data_ptr())\r\n```\r\nand I still see no difference in the timing\r\n\r\n2) I did go through the profiling stuff from onnx and generated the runtime json file but could not infer much from it once I opened it from Google Chrome. Is there any help available on how to interpret the results?\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1019739997/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1019742421",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1019742421",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1019742421,
        "node_id": "IC_kwDOCVq1mM48yAjV",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-24T05:47:29Z",
        "updated_at": "2022-01-24T05:47:29Z",
        "author_association": "NONE",
        "body": "> @nssrivathsa, it is not fair comparison since latency for ORT including copying input tensor from cpu to GPU, while pytorch does not. Could you bind GPU input, and measure the latency again?\r\n\r\n@tianleiwu \r\n\r\nAs you can see for my above reply, io_binding.bind_cpu_input should also do what you are suggesting. Nevertheless, once I modify my code as - \r\n```\r\ndata = onnxruntime.OrtValue.ortvalue_from_numpy(x, device.type, 0)\r\nio_binding.bind_input(input_names, device, 0, np.float32, [batch_size, 3, 224, 224], data.data_ptr())\r\n```\r\nstill the timings on GPU for 1000 samples are -\r\n```\r\nAverage onnxruntime cuda Inference time = 44.55 ms\r\nAverage PyTorch cuda Inference time = 9.780553787231446 ms\r\n``` \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1019742421/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1020328207",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1020328207",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1020328207,
        "node_id": "IC_kwDOCVq1mM480PkP",
        "user": {
            "login": "edgchen1",
            "id": 18449977,
            "node_id": "MDQ6VXNlcjE4NDQ5OTc3",
            "avatar_url": "https://avatars.githubusercontent.com/u/18449977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/edgchen1",
            "html_url": "https://github.com/edgchen1",
            "followers_url": "https://api.github.com/users/edgchen1/followers",
            "following_url": "https://api.github.com/users/edgchen1/following{/other_user}",
            "gists_url": "https://api.github.com/users/edgchen1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/edgchen1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/edgchen1/subscriptions",
            "organizations_url": "https://api.github.com/users/edgchen1/orgs",
            "repos_url": "https://api.github.com/users/edgchen1/repos",
            "events_url": "https://api.github.com/users/edgchen1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/edgchen1/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-24T17:03:14Z",
        "updated_at": "2022-01-24T17:03:14Z",
        "author_association": "MEMBER",
        "body": "> Yes, the input is on the GPU. io_binding.bind_cpu_input puts the input from the CPU to GPU as per 'Scenario 1' in this - http://www.xavierdupre.fr/app/onnxruntime/helpsphinx/api_summary.html under IOBinding.\r\n\r\nThe official docs are here: https://onnxruntime.ai/docs/api/python/api_summary.html\r\n\r\nhttps://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.IOBinding.bind_cpu_input\r\nThe input is on CPU so it will need to be copied to the GPU.\r\n\r\n> I did go through the profiling stuff from onnx and generated the runtime json file but could not infer much from it once I opened it from Google Chrome. Is there any help available on how to interpret the results?\r\n\r\n@RandySheriffH is there some documentation about CUDA kernel profiling in ORT?\r\n\r\nYou can look for host to device memcpy's with nvprof to see whether the input is actually getting copied from CPU to GPU.\r\nhttps://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview\r\nVisual profiler can be used to view the nvprof results.\r\nhttps://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1020328207/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1057712246",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1057712246",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1057712246,
        "node_id": "IC_kwDOCVq1mM4_C2h2",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-03T06:13:16Z",
        "updated_at": "2022-03-03T06:13:16Z",
        "author_association": "NONE",
        "body": "I could not find the issue with profiling, as everything seems to take more time\r\nBut, I observed that running it on a machine with a more advanced GPU ensures that onnxruntime is faster than Pytorch\r\nFor e.g. on a Tesla T4 or Tesla V100 GPU, onnxruntime is around 4 times faster than pytorch",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1057712246/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1165792483",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1165792483",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1165792483,
        "node_id": "IC_kwDOCVq1mM5FfJTj",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-24T17:39:16Z",
        "updated_at": "2022-06-24T17:39:16Z",
        "author_association": "MEMBER",
        "body": "@nssrivathsa, any update on this? Are you able to find the cause?\r\nIf you have nvprof results of ORT and PyTorch. You can compare the profiling result to see which cuda kernel or memory copy that ORT spent more time.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1165792483/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1171154035",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1171154035",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1171154035,
        "node_id": "IC_kwDOCVq1mM5FzmRz",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-30T12:24:08Z",
        "updated_at": "2022-06-30T12:24:08Z",
        "author_association": "NONE",
        "body": "> \r\n@tianleiwu No, i am still not able to find the cause. But, like I mentioned, it does not happen when i run it with some advanced GPUs\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1171154035/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1171985403",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1171985403",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1171985403,
        "node_id": "IC_kwDOCVq1mM5F2xP7",
        "user": {
            "login": "yunjiangster",
            "id": 1061224,
            "node_id": "MDQ6VXNlcjEwNjEyMjQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1061224?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yunjiangster",
            "html_url": "https://github.com/yunjiangster",
            "followers_url": "https://api.github.com/users/yunjiangster/followers",
            "following_url": "https://api.github.com/users/yunjiangster/following{/other_user}",
            "gists_url": "https://api.github.com/users/yunjiangster/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yunjiangster/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yunjiangster/subscriptions",
            "organizations_url": "https://api.github.com/users/yunjiangster/orgs",
            "repos_url": "https://api.github.com/users/yunjiangster/repos",
            "events_url": "https://api.github.com/users/yunjiangster/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yunjiangster/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-01T06:27:44Z",
        "updated_at": "2022-07-01T06:27:44Z",
        "author_association": "NONE",
        "body": "@nssrivathsa Even on Tesla T4, I am seeing 2x slow down using onnx versus pytorch + fp16.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1171985403/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1174858731",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1174858731",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1174858731,
        "node_id": "IC_kwDOCVq1mM5GBuvr",
        "user": {
            "login": "nssrivathsa",
            "id": 97871027,
            "node_id": "U_kgDOBdVksw",
            "avatar_url": "https://avatars.githubusercontent.com/u/97871027?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nssrivathsa",
            "html_url": "https://github.com/nssrivathsa",
            "followers_url": "https://api.github.com/users/nssrivathsa/followers",
            "following_url": "https://api.github.com/users/nssrivathsa/following{/other_user}",
            "gists_url": "https://api.github.com/users/nssrivathsa/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nssrivathsa/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nssrivathsa/subscriptions",
            "organizations_url": "https://api.github.com/users/nssrivathsa/orgs",
            "repos_url": "https://api.github.com/users/nssrivathsa/repos",
            "events_url": "https://api.github.com/users/nssrivathsa/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nssrivathsa/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-05T09:49:39Z",
        "updated_at": "2022-07-05T09:49:39Z",
        "author_association": "NONE",
        "body": "@yunjiangster I haven't used any quantization yet, the script is same as what I have shared above and with that I see improvements when i use it with Tesla T4 on AWS",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1174858731/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1239366938",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1239366938",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1239366938,
        "node_id": "IC_kwDOCVq1mM5J3z0a",
        "user": {
            "login": "thomas-beznik",
            "id": 53873742,
            "node_id": "MDQ6VXNlcjUzODczNzQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/53873742?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thomas-beznik",
            "html_url": "https://github.com/thomas-beznik",
            "followers_url": "https://api.github.com/users/thomas-beznik/followers",
            "following_url": "https://api.github.com/users/thomas-beznik/following{/other_user}",
            "gists_url": "https://api.github.com/users/thomas-beznik/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thomas-beznik/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thomas-beznik/subscriptions",
            "organizations_url": "https://api.github.com/users/thomas-beznik/orgs",
            "repos_url": "https://api.github.com/users/thomas-beznik/repos",
            "events_url": "https://api.github.com/users/thomas-beznik/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thomas-beznik/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-07T13:08:00Z",
        "updated_at": "2022-09-07T14:55:02Z",
        "author_association": "NONE",
        "body": "We're having the same issues with our models - seeing a ~2x slow-down between running our models on GPU with PyTorch vs with ONNX-runtime. This is very problematic, and forces us to search for another solution for putting our models in production... Any help / update on this issue would be greatly appreciated! I'm happy to assist in the debugging if it can help, thanks!\r\n\r\nFYI, using the above example seems to work for us though, we are seeing similar speeds between the ONNX and PyTorch models. In our case, we are using a 3D UNet model (see [here](https://github.com/wolny/pytorch-3dunet)), with similar options as above to convert to ONNX). What could be the causes of such a slow-down? Could it be due to some unsupported operations for example? I can attach the model graph if this can help",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1239366938/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1239685542",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1239685542",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1239685542,
        "node_id": "IC_kwDOCVq1mM5J5Bmm",
        "user": {
            "login": "CanyonWind",
            "id": 8852730,
            "node_id": "MDQ6VXNlcjg4NTI3MzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8852730?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/CanyonWind",
            "html_url": "https://github.com/CanyonWind",
            "followers_url": "https://api.github.com/users/CanyonWind/followers",
            "following_url": "https://api.github.com/users/CanyonWind/following{/other_user}",
            "gists_url": "https://api.github.com/users/CanyonWind/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/CanyonWind/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/CanyonWind/subscriptions",
            "organizations_url": "https://api.github.com/users/CanyonWind/orgs",
            "repos_url": "https://api.github.com/users/CanyonWind/repos",
            "events_url": "https://api.github.com/users/CanyonWind/events{/privacy}",
            "received_events_url": "https://api.github.com/users/CanyonWind/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-07T17:33:48Z",
        "updated_at": "2022-09-07T17:33:48Z",
        "author_association": "NONE",
        "body": "Seeing similar problems as well -- we saw 2.3x slower on `ORT + CUDA EP` compared to native PyTorch inference. In our case, the model is a large complicated diffusion model. I have a hunch that it's caused by the exported onnx graph having too many glue operators when exporting.\r\n\r\nCould people from Microsoft step out and confirm whether it's the case? Thanks ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1239685542/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1239739419",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1239739419",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1239739419,
        "node_id": "IC_kwDOCVq1mM5J5Owb",
        "user": {
            "login": "thomas-beznik",
            "id": 53873742,
            "node_id": "MDQ6VXNlcjUzODczNzQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/53873742?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thomas-beznik",
            "html_url": "https://github.com/thomas-beznik",
            "followers_url": "https://api.github.com/users/thomas-beznik/followers",
            "following_url": "https://api.github.com/users/thomas-beznik/following{/other_user}",
            "gists_url": "https://api.github.com/users/thomas-beznik/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thomas-beznik/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thomas-beznik/subscriptions",
            "organizations_url": "https://api.github.com/users/thomas-beznik/orgs",
            "repos_url": "https://api.github.com/users/thomas-beznik/repos",
            "events_url": "https://api.github.com/users/thomas-beznik/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thomas-beznik/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-07T18:35:23Z",
        "updated_at": "2022-09-07T18:35:23Z",
        "author_association": "NONE",
        "body": "> We're having the same issues with our models - seeing a ~2x slow-down between running our models on GPU with PyTorch vs with ONNX-runtime. This is very problematic, and forces us to search for another solution for putting our models in production... Any help / update on this issue would be greatly appreciated! I'm happy to assist in the debugging if it can help, thanks!\r\n> \r\n> FYI, using the above example seems to work for us though, we are seeing similar speeds between the ONNX and PyTorch models. In our case, we are using a 3D UNet model (see [here](https://github.com/wolny/pytorch-3dunet)), with similar options as above to convert to ONNX). What could be the causes of such a slow-down? Could it be due to some unsupported operations for example? I can attach the model graph if this can help\r\n\r\nI've made a new issue for our problem as it might not be fully related to this one, since I cannot reproduce the same slow-down with the ResNet model\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1239739419/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1260015826",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1260015826",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1260015826,
        "node_id": "IC_kwDOCVq1mM5LGlDS",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-27T20:32:45Z",
        "updated_at": "2022-09-28T21:33:42Z",
        "author_association": "MEMBER",
        "body": "@CanyonWind &[thomas-beznik](https://github.com/thomas-beznik), I recommend nvprof tool to profile your model. It will tell you which kernel uses most kernel time. For example, older version of huggingface stable diffusion pipeline, the UNet model has ScatterND, which is very slow in ORT. It is able to modify the modeling code to avoid those slow operators.\r\n\r\nFor diffusion model with Conv operator, I suggest try cudnn_conv_use_max_workspace in CUDA provider option like [this](https://github.com/tianleiwu/diffusers/blob/4d77a9b9d792f7d3d319002092fc5a05c3e58fdc/scripts/benchmark.py#L133-L134). Sometime, it could make big difference.\r\n\r\nIn my benchmark of huggingface stable diffusion pipeline, if you use original ONNX FP32 model, it could be 2x slower. If you combine fp16 and conv algo search, ORT could be 25% faster than Pytorch:\r\n\r\nLatency   (Seconds per Query) | T4 | V100 | A100\r\n-- | -- | -- | --\r\nPyTorch FP16 | 12.8 | 5.1 | 3.1\r\nORT FP32 | 28.4 | 10.1 | 7.2\r\nORT FP16 | 13.1 | 6.4 | 4.3\r\nORT FP16 + Conv Algo Search | 9.6 | 3.8 | 2.4\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1260015826/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1260062482",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1260062482",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1260062482,
        "node_id": "IC_kwDOCVq1mM5LGwcS",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-27T21:15:30Z",
        "updated_at": "2022-09-27T21:21:35Z",
        "author_association": "MEMBER",
        "body": "@nssrivathsa, I run your script in V100 GPU with PyTorch 1.12.1+cu116, OnnxRuntime-gpu 1.12.1, with latest CUDA 11.7, and latest cuDNN 8.5.0.96. Here is the output:\r\n![image](https://user-images.githubusercontent.com/30328909/192636395-0acf3ef9-e456-49a3-a65c-2727fe11c98d.png)\r\n\r\nIt seems that ORT is much faster than PyTorch in V100. \r\n\r\nHere is the script:\r\n```\r\nimport os\r\nimport torch\r\nimport onnx\r\nimport torchvision.models as models\r\nimport onnxruntime\r\nimport time\r\n\r\nbatch_size = 1\r\ntotal_samples = 1000\r\ndevice = torch.device('cuda:0')\r\n\r\ndef convert_to_onnx(resnet):\r\n   resnet.eval()\r\n   dummy_input = (torch.randn(batch_size, 3, 224, 224, device=device)).to(device=device)\r\n   input_names = [ 'input' ]\r\n   output_names = [ 'output' ]\r\n   torch.onnx.export(resnet, \r\n               dummy_input,\r\n               \"resnet18.onnx\",\r\n               verbose=True,\r\n               opset_version=13,\r\n               input_names=input_names,\r\n               output_names=output_names,\r\n               export_params=True,\r\n               do_constant_folding=True,\r\n               dynamic_axes={\r\n                  'input': {0: 'batch_size'},  # variable length axes\r\n                  'output': {0: 'batch_size'}}\r\n               )\r\n\r\ndef infer_pytorch(resnet):\r\n   print('Pytorch Inference')\r\n   print('==========================')\r\n   print()\r\n\r\n   x = torch.randn((batch_size, 3, 224, 224))\r\n   x = x.to(device=device)\r\n\r\n   latency = []\r\n   for i in range(total_samples):\r\n      t0 = time.time()\r\n      resnet.eval()\r\n      with torch.no_grad():\r\n         out = resnet(x)\r\n      latency.append(time.time() - t0)\r\n\r\n   print('Number of runs:', len(latency))\r\n   print(\"Average PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))  \r\n\r\ndef to_numpy(tensor):\r\n   return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\r\n\r\ndef infer_onnxruntime():\r\n   print('Onnxruntime Inference')\r\n   print('==========================')\r\n   print()\r\n\r\n   onnx_model = onnx.load(\"resnet18.onnx\")\r\n   onnx.checker.check_model(onnx_model)\r\n\r\n   # Input\r\n   x = torch.randn((batch_size, 3, 224, 224))\r\n   x = x.to(device=device)\r\n   x = to_numpy(x)\r\n\r\n   so = onnxruntime.SessionOptions()\r\n   exproviders = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\r\n\r\n   model_onnx_path = os.path.join(\".\", \"resnet18.onnx\")\r\n   ort_session = onnxruntime.InferenceSession(model_onnx_path, so, providers=exproviders)\r\n\r\n   #IOBinding\r\n   input_names = ort_session.get_inputs()[0].name\r\n   output_names = ort_session.get_outputs()[0].name\r\n   io_binding = ort_session.io_binding()\r\n\r\n   io_binding.bind_cpu_input(input_names, x)\r\n   io_binding.bind_output(output_names, 'cuda')\r\n\r\n   #warm up run\r\n   ort_session.run_with_iobinding(io_binding)\r\n   ort_outs = io_binding.copy_outputs_to_cpu()\r\n\r\n   latency = []\r\n\r\n   for i in range(total_samples):\r\n      t0 = time.time()\r\n      ort_session.run_with_iobinding(io_binding)\r\n      latency.append(time.time() - t0)\r\n      ort_outs = io_binding.copy_outputs_to_cpu()\r\n   print('Number of runs:', len(latency))\r\n   print(\"Average onnxruntime {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))   \r\n\r\nif __name__ == '__main__':\r\n   torch.cuda.empty_cache()\r\n   resnet = (models.resnet18(pretrained=True)).to(device=device)\r\n   convert_to_onnx(resnet)\r\n   infer_onnxruntime()\r\n   infer_pytorch(resnet)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1260062482/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1330443330",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10303#issuecomment-1330443330",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10303",
        "id": 1330443330,
        "node_id": "IC_kwDOCVq1mM5PTPRC",
        "user": {
            "login": "davidmezzetti",
            "id": 561939,
            "node_id": "MDQ6VXNlcjU2MTkzOQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/561939?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/davidmezzetti",
            "html_url": "https://github.com/davidmezzetti",
            "followers_url": "https://api.github.com/users/davidmezzetti/followers",
            "following_url": "https://api.github.com/users/davidmezzetti/following{/other_user}",
            "gists_url": "https://api.github.com/users/davidmezzetti/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/davidmezzetti/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/davidmezzetti/subscriptions",
            "organizations_url": "https://api.github.com/users/davidmezzetti/orgs",
            "repos_url": "https://api.github.com/users/davidmezzetti/repos",
            "events_url": "https://api.github.com/users/davidmezzetti/events{/privacy}",
            "received_events_url": "https://api.github.com/users/davidmezzetti/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-29T10:54:10Z",
        "updated_at": "2022-11-29T10:54:10Z",
        "author_association": "NONE",
        "body": "Similar issue described here - https://github.com/microsoft/onnxruntime/issues/12880#issuecomment-1330442188",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1330443330/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]