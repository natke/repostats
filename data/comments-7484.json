[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/829002747",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7484#issuecomment-829002747",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7484",
        "id": 829002747,
        "node_id": "MDEyOklzc3VlQ29tbWVudDgyOTAwMjc0Nw==",
        "user": {
            "login": "oliviajain",
            "id": 25233514,
            "node_id": "MDQ6VXNlcjI1MjMzNTE0",
            "avatar_url": "https://avatars.githubusercontent.com/u/25233514?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/oliviajain",
            "html_url": "https://github.com/oliviajain",
            "followers_url": "https://api.github.com/users/oliviajain/followers",
            "following_url": "https://api.github.com/users/oliviajain/following{/other_user}",
            "gists_url": "https://api.github.com/users/oliviajain/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/oliviajain/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/oliviajain/subscriptions",
            "organizations_url": "https://api.github.com/users/oliviajain/orgs",
            "repos_url": "https://api.github.com/users/oliviajain/repos",
            "events_url": "https://api.github.com/users/oliviajain/events{/privacy}",
            "received_events_url": "https://api.github.com/users/oliviajain/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-04-29T07:26:33Z",
        "updated_at": "2021-04-29T07:27:33Z",
        "author_association": "CONTRIBUTOR",
        "body": "You may follow the instructions found here https://www.onnxruntime.ai/docs/how-to/build.html#nvidia-jetson-tx1tx2nanoxavier to build onnxruntime from source with CUDA or TensorRT support. After you run the build command, you can locate the wheel file (generated with the argument --build_wheel) and pip3 install it. \r\n\r\nInstructions: \r\n1. Follow the commands 1-5 on the build doc. \r\n2. pip3 (or pip) install build/Linux/Release/dist/*.whl\r\n3. Now you can inference with CUDA or TensorRT depending on your build command.\r\n\r\nPip (pip3 for python3) is a python package manager. It allows you to install all the files you would need to run a certain module. If you would like to run CUDA or TensorRT EP with python, you would need to have the proper dependencies and libraries. The wheel file is essentially a zip file that contains all these dependencies, and you can install them with the command pip3 install *.whl .",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/829002747/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/842486084",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/7484#issuecomment-842486084",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/7484",
        "id": 842486084,
        "node_id": "MDEyOklzc3VlQ29tbWVudDg0MjQ4NjA4NA==",
        "user": {
            "login": "Noam-M",
            "id": 45801356,
            "node_id": "MDQ6VXNlcjQ1ODAxMzU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/45801356?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Noam-M",
            "html_url": "https://github.com/Noam-M",
            "followers_url": "https://api.github.com/users/Noam-M/followers",
            "following_url": "https://api.github.com/users/Noam-M/following{/other_user}",
            "gists_url": "https://api.github.com/users/Noam-M/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Noam-M/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Noam-M/subscriptions",
            "organizations_url": "https://api.github.com/users/Noam-M/orgs",
            "repos_url": "https://api.github.com/users/Noam-M/repos",
            "events_url": "https://api.github.com/users/Noam-M/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Noam-M/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-05-17T17:02:19Z",
        "updated_at": "2021-05-17T17:02:19Z",
        "author_association": "NONE",
        "body": "Hello @oliviajain \r\n\r\nThank you for your help.\r\nIt took a while - but I have successfully built cmake and onnxruntime on my Jetson device, and now the model runs properly on GPU (I have checked `onnxruntime.get_device()` and got `GPU`)\r\n\r\nNow I would like to incorporate optimizations to improve runtime performance.\r\nDo you have a suggestion regarding how to do that?\r\nMaybe running TensorRT (I'm not sure how to do that)\r\n\r\nThank you!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/842486084/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]