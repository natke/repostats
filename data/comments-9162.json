[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/925405448",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-925405448",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 925405448,
        "node_id": "IC_kwDOCVq1mM43KJEI",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-22T23:42:54Z",
        "updated_at": "2021-09-22T23:42:54Z",
        "author_association": "MEMBER",
        "body": "I wonder if one or more CUDA kernels are not performant enough for the batched input use case (which we can identify and fix by profiling). Just curious, (if it is possible) can you benchmark with batch size = 1 ? If batching is the problem for some kernels, the performance when there is only a single input should be better. FWIW - Can you try with the [latest nightly](https://test.pypi.org/project/ort-nightly-gpu/) to see if the performance is better ?\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/925405448/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/925422804",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-925422804",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 925422804,
        "node_id": "IC_kwDOCVq1mM43KNTU",
        "user": {
            "login": "yetingqiaqia",
            "id": 6299908,
            "node_id": "MDQ6VXNlcjYyOTk5MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6299908?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yetingqiaqia",
            "html_url": "https://github.com/yetingqiaqia",
            "followers_url": "https://api.github.com/users/yetingqiaqia/followers",
            "following_url": "https://api.github.com/users/yetingqiaqia/following{/other_user}",
            "gists_url": "https://api.github.com/users/yetingqiaqia/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yetingqiaqia/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yetingqiaqia/subscriptions",
            "organizations_url": "https://api.github.com/users/yetingqiaqia/orgs",
            "repos_url": "https://api.github.com/users/yetingqiaqia/repos",
            "events_url": "https://api.github.com/users/yetingqiaqia/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yetingqiaqia/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-23T00:23:09Z",
        "updated_at": "2021-09-23T00:23:09Z",
        "author_association": "MEMBER",
        "body": "Thanks @hariharans29 . I tested batch_size=1 with 2k samples. \r\nHere is result:\r\n\r\n- PyTorch: 115.77s, i.e., 17.28 samples/s/gpu\r\n- Onnx+opset13: 190.23s, i.e., 10.51 samples/s/gpu\r\n\r\nThe gap is slightly narrowed, but it is still 1.67X slower. \r\n\r\nFor latest nightly version, I will have a try and report it later. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/925422804/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/925445349",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-925445349",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 925445349,
        "node_id": "IC_kwDOCVq1mM43KSzl",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-23T01:29:26Z",
        "updated_at": "2021-09-23T01:29:26Z",
        "author_association": "MEMBER",
        "body": "Thanks.\r\n\r\nFor the Torch evaluation, you are copying the data to the device and then starting the timer:\r\n\r\n'''\r\n            data = data.to(args.device)\r\n            #below is pytorch inference code\r\n            start_stamp = time.time()\r\n'''\r\n\r\nThe result from Torch stays on CUDA (I think!)\r\n\r\nWhile evaluating ORT, the data is copied over to CPU and then taken to the device (as part of Run()) after the timer is turned on and the measured time also includes result data transfer from GPU to CPU\r\n\r\nTo truly get an apple-to-apple comparison with Torch, can you modify the benchmarking script to use [IOBinding](https://github.com/microsoft/onnxruntime/blob/6e83392ff134f450aa5b8798ccbb138ec32b5379/onnxruntime/test/python/onnxruntime_test_python_iobinding.py#L97) and turn on the timer after the data has been copied over to the device (just like the Torch evaluation part) ? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/925445349/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926841345",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-926841345",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 926841345,
        "node_id": "IC_kwDOCVq1mM43PnoB",
        "user": {
            "login": "yetingqiaqia",
            "id": 6299908,
            "node_id": "MDQ6VXNlcjYyOTk5MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6299908?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yetingqiaqia",
            "html_url": "https://github.com/yetingqiaqia",
            "followers_url": "https://api.github.com/users/yetingqiaqia/followers",
            "following_url": "https://api.github.com/users/yetingqiaqia/following{/other_user}",
            "gists_url": "https://api.github.com/users/yetingqiaqia/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yetingqiaqia/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yetingqiaqia/subscriptions",
            "organizations_url": "https://api.github.com/users/yetingqiaqia/orgs",
            "repos_url": "https://api.github.com/users/yetingqiaqia/repos",
            "events_url": "https://api.github.com/users/yetingqiaqia/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yetingqiaqia/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-24T18:34:18Z",
        "updated_at": "2021-09-24T18:41:37Z",
        "author_association": "MEMBER",
        "body": "@hariharans29 , for latest nightly version (v1.9.0). It performs quite bad. I can't use Batch_Size=32 any more because of \"out of shared memory\" issue. So, I switch to BS=16 to test. The inference time on 15k is 2333.4 s, which is about 3.2X worse than using official 1.8.1 version, and 9.37X slower than pyTorch 1.9.1. \r\n- The command I used to install latest nightly version: ```pip install -i https://test.pypi.org/simple/ ort-nightly-gpu```\r\n\r\nFor the input data part, according to my previous experience, it doesn't matter much, especially for heavy models like this. I don't think it could reduce the 65% (the time gap between pytorch and ORT on this model) by changing IO. \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926841345/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926861475",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-926861475",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 926861475,
        "node_id": "IC_kwDOCVq1mM43Psij",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-24T19:11:35Z",
        "updated_at": "2021-09-24T19:11:35Z",
        "author_association": "MEMBER",
        "body": "Thanks.\r\n\r\n1) You're right - usually IOBinding doesn't matter too much if the batch size is 1. But here since batch size is >> 1, I would think it makes a bigger difference than usual (as the input buffer and output buffer is correspondingly large). It is just 2-3 lines of code change. Please consider doing it to get the \"real\" gap.\r\n\r\n 2) For Conv heavy models, it is important to do a warm-up prior to benchmarking. The very first run is very expensive compared to the other runs because the best Conv algo is established only on the first run. So, please add a warm-up run with the same input shape as the rest of the runs. This is to  establish the \"true\" gap. \r\n\r\n3) The regression in the nightly (1.9's performance should be the same as the nightly) compared to 1.8 is to be investigated as is the original poor perf in 1.8 (with warm-up and IOBinding, the true gap for this will be known).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926861475/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926976283",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-926976283",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 926976283,
        "node_id": "IC_kwDOCVq1mM43QIkb",
        "user": {
            "login": "yetingqiaqia",
            "id": 6299908,
            "node_id": "MDQ6VXNlcjYyOTk5MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6299908?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yetingqiaqia",
            "html_url": "https://github.com/yetingqiaqia",
            "followers_url": "https://api.github.com/users/yetingqiaqia/followers",
            "following_url": "https://api.github.com/users/yetingqiaqia/following{/other_user}",
            "gists_url": "https://api.github.com/users/yetingqiaqia/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yetingqiaqia/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yetingqiaqia/subscriptions",
            "organizations_url": "https://api.github.com/users/yetingqiaqia/orgs",
            "repos_url": "https://api.github.com/users/yetingqiaqia/repos",
            "events_url": "https://api.github.com/users/yetingqiaqia/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yetingqiaqia/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-25T00:04:49Z",
        "updated_at": "2021-09-25T00:04:49Z",
        "author_association": "MEMBER",
        "body": "@hariharans29 for IOBinding, I tried to follow the example script you shared and tested with top 10 samples.\r\nHowever, I tested two ways and both met some issues:\r\n1. Using a static input shape [1, 3, 480, 480]. \r\n            ```io_binding.bind_input(input_name, 'cuda', 0, np.float32, [1, 3, 480, 480], data.data_ptr())```\r\n     This one is runnable, but it will only process the first sample of each batch:\r\n      - Expected output: \r\n![image](https://user-images.githubusercontent.com/6299908/134750278-1da95a5e-72df-4ea9-aae4-898067b27e47.png)\r\n      - Real output:\r\n![image](https://user-images.githubusercontent.com/6299908/134750296-46cc1b23-06ef-47d9-afbe-20595d0470c1.png)\r\n       \r\n      As you can see, by this way, it only processes the first sample.\r\n2. To avoid issue in case1, I calculated the data_shape instead of using a static shape\r\n            ```data_shape = list(to_numpy(data).shape)```\r\n            ```io_binding.bind_input(input_name, 'cuda', 0, np.float32, data_shape, data.data_ptr())```\r\n      However, this way, the script failed because of \"OrtValue shape verification failed. Current shape:{1,3} Requested shape:{2,3}\" error.\r\n![image](https://user-images.githubusercontent.com/6299908/134750525-14db6b1b-62b5-411a-aa51-0cbafe3f81a0.png)\r\n\r\nI am not familiar with ORT IOBinding. **Coud you help check what's wrong and how to make the code runnable?**\r\nAlso, for the output ```pred = io_binding.get_outputs()[0].numpy()```, **what is the meaning of \"[0]\" here? Does it mean the output of the first output tensor? If we have 2 tensors, then there will be [1] for the second output tensor?** \r\n\r\nBelow is my modified code:\r\n```python\r\ndef onnx_evaluate(args):\r\n    val_loader = get_val_loader(args)\r\n    import onnxruntime as ort\r\n    sess_options = ort.SessionOptions()\r\n    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\r\n    sess_options.intra_op_num_threads = 0\r\n    sess = ort.InferenceSession(args.onnx_model_path, sess_options)\r\n    input_name = sess.get_inputs()[0].name\r\n    label_name = sess.get_outputs()[0].name\r\n\r\n    io_binding = sess.io_binding()\r\n    io_binding.bind_output(label_name, 'cuda')\r\n    def to_numpy(torch_tensor):\r\n        return torch_tensor.detach().cpu().numpy() if torch_tensor.requires_grad else torch_tensor.cpu().numpy()\r\n\r\n    accumulated_inference_time = 0\r\n\r\n    f = open(args.outputfilename, 'w', encoding='utf-8')\r\n    f.write('\\t'.join(['Id', 'Score']) + '\\n')\r\n\r\n    with torch.no_grad():\r\n        for i, (id, data, target) in enumerate(val_loader):\r\n            data = data.to(args.device)\r\n            data_shape = list(to_numpy(data).shape)\r\n            print(data_shape)\r\n            #io_binding.bind_input(input_name, 'cuda', 0, np.float32, [1, 3, 480, 480], data.data_ptr())\r\n            io_binding.bind_input(input_name, 'cuda', 0, np.float32, data_shape, data.data_ptr())\r\n            #below is onnx inference code\r\n            start_stamp = time.time()\r\n            sess.run_with_iobinding(io_binding)\r\n            #pred = sess.run([label_name], {input_name: to_numpy(data)})[0]\r\n            accumulated_inference_time += time.time() - start_stamp\r\n            pred = io_binding.get_outputs()[0].numpy()\r\n            f.write('\\t'.join([str(id),\r\n                               ','.join([str(l) for l in pred])]) + '\\n')\r\n    print(f\"Total Onnx model inference time is {accumulated_inference_time}\")\r\n\r\n    print(f\"The file is present at location: {args.outputfilename}\")\r\n    f.flush()\r\n    f.close()\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926976283/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926988032",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-926988032",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 926988032,
        "node_id": "IC_kwDOCVq1mM43QLcA",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-09-25T01:14:10Z",
        "updated_at": "2021-09-25T01:14:10Z",
        "author_association": "MEMBER",
        "body": "1) This is expected. The data being bound is just the first sample because the input shape of bound data is [**1**, 3, 480, 480] and so processing is only being done for that sample of the batch. Your second approach is correct.\r\n\r\n2) You are seeing this because the input shape is changing (batch size changes from 1 to 2) and the output is not re-bound. Please add this line  `io_binding.bind_output(label_name, 'cuda')` after bind_input() and before starting the timer. Ideally, the input shape will remain the same across all iterations of the benchmark if that is your prod scenario. Please run a warm-up run for that input shape to cache some CuDNN details prior to bench-marking.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/926988032/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/983251473",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-983251473",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 983251473,
        "node_id": "IC_kwDOCVq1mM46mzoR",
        "user": {
            "login": "hariharans29",
            "id": 9969784,
            "node_id": "MDQ6VXNlcjk5Njk3ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9969784?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hariharans29",
            "html_url": "https://github.com/hariharans29",
            "followers_url": "https://api.github.com/users/hariharans29/followers",
            "following_url": "https://api.github.com/users/hariharans29/following{/other_user}",
            "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions",
            "organizations_url": "https://api.github.com/users/hariharans29/orgs",
            "repos_url": "https://api.github.com/users/hariharans29/repos",
            "events_url": "https://api.github.com/users/hariharans29/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hariharans29/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-01T03:27:34Z",
        "updated_at": "2021-12-01T03:28:46Z",
        "author_association": "MEMBER",
        "body": "Closing this as all the issues pertaining to this model were resolved through : https://github.com/microsoft/onnxruntime/pull/9826, https://github.com/microsoft/onnxruntime/pull/9879, using IOBinding (see script below), and allowing CuDNN to use maximum workspace memory during Conv algo picking (see script below).\r\n\r\nFinal inferencing script for reference (formatting may be off):\r\n\r\n'''\r\nimport numpy as np\r\nimport time\r\nimport onnxruntime as ort\r\n\r\n#Batch size  = 8\r\ninput = np.random.rand(8, 3, 480, 480).astype(np.float32)\r\nnum_batches = 200\r\n\r\nsess_options = ort.SessionOptions()\r\nsess = ort.InferenceSession(\"resnet_fp16.onnx\", sess_options, providers=['CUDAExecutionProvider'])\r\ninput_name = sess.get_inputs()[0].name\r\nlabel_name = sess.get_outputs()[0].name\r\n\r\noptions = sess.get_provider_options()\r\ncuda_options = options['CUDAExecutionProvider']\r\ncuda_options['cudnn_conv_use_max_workspace'] = '1'\r\nsess.set_providers(['CUDAExecutionProvider'], [cuda_options])\r\n\r\n#IOBinding\r\nio_binding = sess.io_binding()\r\nio_binding.bind_output(label_name, 'cuda')\r\ndata = ort.OrtValue.ortvalue_from_numpy(input, 'cuda', 0)\r\nio_binding.bind_input(input_name, 'cuda', 0, np.float32, [8, 3, 480, 480], data.data_ptr())\r\n\r\n#warm-up run (dis-regard latency)\r\nwarm_up_start_stamp = time.time()\r\nsess.run_with_iobinding(io_binding)\r\npred = io_binding.copy_outputs_to_cpu()[0]\r\nprint(f\"It takes {time.time()-warm_up_start_stamp} to finish warm-up.\\n\"\r\n+f\" - cudnn_conv_use_max_workspace is {sess.get_provider_options()['CUDAExecutionProvider']['cudnn_conv_use_max_workspace']}\\n\"\r\n+f\" - io_binding\")\r\n#print(pred)\r\n\r\n#Perf benchmark\r\naccumulated_inference_time = 0\r\nfor i in range(num_batches):\r\n#data = ort.OrtValue.ortvalue_from_numpy(input, 'cuda', 0)\r\n#io_binding.bind_input(input_name, 'cuda', 0, np.float32, [8, 3, 480, 480], data.data_ptr())\r\n#below is onnx inference code\r\nstart_stamp = time.time()\r\nsess.run_with_iobinding(io_binding)\r\n#pred = sess.run([label_name], {input_name: to_numpy(data)})[0]\r\naccumulated_inference_time += time.time() - start_stamp\r\npred = io_binding.copy_outputs_to_cpu()[0]\r\nprint(pred)\r\n\r\nprint(f\"It takes {accumulated_inference_time} to finish {num_batches} batches.\\n\"\r\n+f\" - cudnn_conv_use_max_workspace is {sess.get_provider_options()['CUDAExecutionProvider']['cudnn_conv_use_max_workspace']}\\n\"\r\n+f\" - io_binding\")\r\n'''\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/983251473/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/988169225",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/9162#issuecomment-988169225",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/9162",
        "id": 988169225,
        "node_id": "IC_kwDOCVq1mM465kQJ",
        "user": {
            "login": "yetingqiaqia",
            "id": 6299908,
            "node_id": "MDQ6VXNlcjYyOTk5MDg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6299908?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yetingqiaqia",
            "html_url": "https://github.com/yetingqiaqia",
            "followers_url": "https://api.github.com/users/yetingqiaqia/followers",
            "following_url": "https://api.github.com/users/yetingqiaqia/following{/other_user}",
            "gists_url": "https://api.github.com/users/yetingqiaqia/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yetingqiaqia/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yetingqiaqia/subscriptions",
            "organizations_url": "https://api.github.com/users/yetingqiaqia/orgs",
            "repos_url": "https://api.github.com/users/yetingqiaqia/repos",
            "events_url": "https://api.github.com/users/yetingqiaqia/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yetingqiaqia/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-07T18:29:01Z",
        "updated_at": "2021-12-07T18:29:01Z",
        "author_association": "MEMBER",
        "body": "Thanks @hariharans29 for driving this. It is good to see ORT is faster than torch on this model finally! ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/988169225/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]