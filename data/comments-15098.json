[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474411351",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15098#issuecomment-1474411351",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15098",
        "id": 1474411351,
        "node_id": "IC_kwDOCVq1mM5X4btX",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-17T21:17:44Z",
        "updated_at": "2023-03-17T21:25:47Z",
        "author_association": "MEMBER",
        "body": "T4 has tensor core, so it has more choices of convolution algorithms in cuDNN. Different convolution algorithm uses different size of workspace.\r\n\r\nYou can try tune a few parameters: `gpu_mem_limit`, [`cudnn_conv_algo_search`](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cudnn_conv_algo_search) and [`cudnn_conv1d_pad_to_nc1d`](https://onnxruntime.ai/docs/performance/tune-performance.html#convolution-input-padding-in-the-cuda-ep) to see what's change in memory usage and performance.\r\n\r\nBTW, your model is very simple, which means you are able to use larger batch size in T4 than K80.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474411351/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1475577015",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15098#issuecomment-1475577015",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15098",
        "id": 1475577015,
        "node_id": "IC_kwDOCVq1mM5X84S3",
        "user": {
            "login": "knitvoger",
            "id": 19744544,
            "node_id": "MDQ6VXNlcjE5NzQ0NTQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/19744544?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/knitvoger",
            "html_url": "https://github.com/knitvoger",
            "followers_url": "https://api.github.com/users/knitvoger/followers",
            "following_url": "https://api.github.com/users/knitvoger/following{/other_user}",
            "gists_url": "https://api.github.com/users/knitvoger/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/knitvoger/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/knitvoger/subscriptions",
            "organizations_url": "https://api.github.com/users/knitvoger/orgs",
            "repos_url": "https://api.github.com/users/knitvoger/repos",
            "events_url": "https://api.github.com/users/knitvoger/events{/privacy}",
            "received_events_url": "https://api.github.com/users/knitvoger/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-20T03:48:07Z",
        "updated_at": "2023-03-20T03:48:07Z",
        "author_association": "NONE",
        "body": "> T4 has tensor core, so it has more choices of convolution algorithms in cuDNN. Different convolution algorithm uses different size of workspace.\r\n> \r\n> You can try tune a few parameters: `gpu_mem_limit`, [`cudnn_conv_algo_search`](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cudnn_conv_algo_search) and [`cudnn_conv1d_pad_to_nc1d`](https://onnxruntime.ai/docs/performance/tune-performance.html#convolution-input-padding-in-the-cuda-ep) to see what's change in memory usage and performance.\r\n> \r\n> BTW, your model is very simple, which means you are able to use larger batch size in T4 than K80.\r\n\r\nThanks @tianleiwu. I have tried the parameters. But they don't bring any change to the memory. Are there any other parameters I can try?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1475577015/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]