[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1074397486",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10957#issuecomment-1074397486",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10957",
        "id": 1074397486,
        "node_id": "IC_kwDOCVq1mM5ACgEu",
        "user": {
            "login": "hanbitmyths",
            "id": 35605090,
            "node_id": "MDQ6VXNlcjM1NjA1MDkw",
            "avatar_url": "https://avatars.githubusercontent.com/u/35605090?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hanbitmyths",
            "html_url": "https://github.com/hanbitmyths",
            "followers_url": "https://api.github.com/users/hanbitmyths/followers",
            "following_url": "https://api.github.com/users/hanbitmyths/following{/other_user}",
            "gists_url": "https://api.github.com/users/hanbitmyths/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hanbitmyths/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hanbitmyths/subscriptions",
            "organizations_url": "https://api.github.com/users/hanbitmyths/orgs",
            "repos_url": "https://api.github.com/users/hanbitmyths/repos",
            "events_url": "https://api.github.com/users/hanbitmyths/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hanbitmyths/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-21T20:40:33Z",
        "updated_at": "2022-03-21T20:40:33Z",
        "author_association": "MEMBER",
        "body": "From below [comment](https://github.com/emscripten-core/emscripten/blob/main/src/settings.js), the maximum wasm memory will be 2GB by default unless it's built with MAXIMUM_MEMORY option. ONNX Runtime Web is not built with that option, so it's limited to 2GB memory.\r\n\r\n// Set the maximum size of memory in the wasm module (in bytes). This is only\r\n// relevant when ALLOW_MEMORY_GROWTH is set, as without growth, the size of\r\n// INITIAL_MEMORY is the final size of memory anyhow.\r\n//\r\n// Note that the default value here is 2GB, which means that by default if you\r\n// enable memory growth then we can grow up to 2GB but no higher. 2GB is a\r\n// natural limit for several reasons:\r\n//\r\n//   * If the maximum heap size is over 2GB, then pointers must be unsigned in\r\n//     JavaScript, which increases code size. We don't want memory growth builds\r\n//     to be larger unless someone explicitly opts in to >2GB+ heaps.\r\n//   * Historically no VM has supported more >2GB+, and only recently (Mar 2020)\r\n//     has support started to appear. As support is limited, it's safer for\r\n//     people to opt into >2GB+ heaps rather than get a build that may not\r\n//     work on all VMs.\r\n//\r\n// To use more than 2GB, set this to something higher, like 4GB.\r\n//\r\n// (This option was formerly called WASM_MEM_MAX and BINARYEN_MEM_MAX.)\r\n// [link]\r\nvar MAXIMUM_MEMORY = 2147483648;",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1074397486/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1074911590",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10957#issuecomment-1074911590",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10957",
        "id": 1074911590,
        "node_id": "IC_kwDOCVq1mM5AEdlm",
        "user": {
            "login": "josephrocca",
            "id": 1167575,
            "node_id": "MDQ6VXNlcjExNjc1NzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1167575?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/josephrocca",
            "html_url": "https://github.com/josephrocca",
            "followers_url": "https://api.github.com/users/josephrocca/followers",
            "following_url": "https://api.github.com/users/josephrocca/following{/other_user}",
            "gists_url": "https://api.github.com/users/josephrocca/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/josephrocca/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/josephrocca/subscriptions",
            "organizations_url": "https://api.github.com/users/josephrocca/orgs",
            "repos_url": "https://api.github.com/users/josephrocca/repos",
            "events_url": "https://api.github.com/users/josephrocca/events{/privacy}",
            "received_events_url": "https://api.github.com/users/josephrocca/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-22T09:10:01Z",
        "updated_at": "2022-03-22T09:10:01Z",
        "author_association": "CONTRIBUTOR",
        "body": "@hanbitmyths Thanks for looking into this! A few questions:\r\n\r\n1. Is it possible to improve the error message here so it's not confusing for other/future users of the web runtime?\r\n2. Since the default 2GB choice is based on Emscripten glue code size, and since users of ONNX Runtime Web will load vastly more data over the network due to the `.onnx` model compared to Emscripten's glue code, can this be switched to 4GB by default? (Note: There is some discussion [here](https://github.com/emscripten-core/emscripten/issues/16197) on setting the memory limit at load time, but I think in this case it would make more sense to just set 4GB as the max at build time.)\r\n3. Is it possible to load each session as a completely separate wasm module (with its own set of workers)? If not, how hard would this be to implement? This would be ideal because it would basically remove the memory limit for most practical usage.\r\n    *  Would this also improve parallelization if I'm running several inference sessions at once, each with only one thread? In my initial tests of this it doesn't seems to scale very well, and I figure that this could be due to all the inference sessions all sharing the same wasm module?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1074911590/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1081409523",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10957#issuecomment-1081409523",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10957",
        "id": 1081409523,
        "node_id": "IC_kwDOCVq1mM5AdP_z",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-29T05:09:41Z",
        "updated_at": "2022-03-29T05:09:41Z",
        "author_association": "MEMBER",
        "body": "1> probably this can be improved by using a \"debug\" web assembly build ( with -s ASSERTIONS=1 ). However, including another 4 .wasm files into the NPM package will significantly increase the package size. This need to be think of carefully.\r\n\r\n2> I see no harm to increase max_size to 4GB.\r\n\r\n3> this cannot be done based on current code, so need code changes to make it happen. However, using a singleton wasm instance in one JS context is a design decision which is based on the requirement of saving memory usage, as there is no need to duplicate the memory used by ORT itself (and also simply the implementation of status management). The only benefit seems to be allowing bypassing the 4GB limit, which may be supported by wasm64 in future.\r\n     > so far ort.min.js cannot be loaded in web-worker directly, so there is no parallelization would happen for different models. It needs a change to allow the ORT Web js to be loaded in a web worker.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1081409523/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1081567635",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10957#issuecomment-1081567635",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10957",
        "id": 1081567635,
        "node_id": "IC_kwDOCVq1mM5Ad2mT",
        "user": {
            "login": "josephrocca",
            "id": 1167575,
            "node_id": "MDQ6VXNlcjExNjc1NzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1167575?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/josephrocca",
            "html_url": "https://github.com/josephrocca",
            "followers_url": "https://api.github.com/users/josephrocca/followers",
            "following_url": "https://api.github.com/users/josephrocca/following{/other_user}",
            "gists_url": "https://api.github.com/users/josephrocca/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/josephrocca/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/josephrocca/subscriptions",
            "organizations_url": "https://api.github.com/users/josephrocca/orgs",
            "repos_url": "https://api.github.com/users/josephrocca/repos",
            "events_url": "https://api.github.com/users/josephrocca/events{/privacy}",
            "received_events_url": "https://api.github.com/users/josephrocca/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-29T08:16:21Z",
        "updated_at": "2022-03-29T08:17:50Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks for looking into this! Would the changes required to support parallel execution of several sessions also allow those individual sessions to have their own separate 4 GB limits?\r\n\r\nIn [this OpenAI CLIP demo](https://josephrocca.github.io/clip-image-sorter) I'm using ONNX Runtime Web to get the embeddings for a user-provided directory of images, and if the user's machine has 16 GB of RAM and 16 threads, then I'd love it if it were possible to process images at up to ~16x the speed (the models takes ~400 MB of RAM IIRC, so there'd be leftover RAM for the OS and other processes).\r\n\r\nAs an example, ideally it would be as simple as something like:\r\n```js\r\nlet session1 = await ort.InferenceSession.create(imageModelUrl, { resourceGroup: \"foo\" });\r\nlet session2 = await ort.InferenceSession.create(imageModelUrl, { resourceGroup: \"foo\" });\r\nlet session3 = await ort.InferenceSession.create(imageModelUrl, { resourceGroup: \"bar\" });\r\n```\r\nSo in this case `session1` and `session2` share workers and RAM, and `session3` has its own workers and RAM. Each resource group would (if I understand correctly) only be able to execute a single model at a time, and would have up to 4GB of memory (until wasm64, at least). There'll likely be a better name than `resourceGroup` - this is just to illustrate.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1081567635/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]