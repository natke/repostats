[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334195038",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334195038",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334195038,
        "node_id": "IC_kwDOCVq1mM5PhjNe",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T18:44:42Z",
        "updated_at": "2022-12-01T23:02:42Z",
        "author_association": "MEMBER",
        "body": "Updated:\r\n\r\nActually, I can reproduce the issue. Add profiling:\r\n```\r\nimport copy\r\nimport time\r\n\r\nimport onnxruntime as ort\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\ndef test(use_ort_in_loop):\r\n\tbatch_size = 4\r\n\tsequence_length = 32\r\n\r\n\ttokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\r\n\r\n\ttotal_infsession = []\r\n\ttotal_no_infsession = []\r\n\tn_loop = 100\r\n\r\n\tinp = {\r\n\t    \"input_ids\": torch.randint(tokenizer.vocab_size - 1, (batch_size, 5), dtype=torch.int64),\r\n\t    \"encoder_attention_mask\": torch.ones(batch_size, sequence_length, dtype=torch.int64),\r\n\t    \"encoder_hidden_states\": torch.rand((batch_size, sequence_length, 512), dtype=torch.float32)\r\n\t}\r\n\r\n\tort_inp = {}\r\n\tfor key, value in inp.items():\r\n\t    ort_inp[key] = inp[key].detach().cpu().numpy()\r\n\r\n\tsession = ort.InferenceSession(\"decoder_model.onnx\", providers=[\"CPUExecutionProvider\"])\r\n\r\n\tM = torch.zeros(32128, 2)\r\n\r\n\tdef get_next_token_logits(ort_inp):\r\n\t    res = session.run(None, ort_inp)\r\n\t    out = torch.from_numpy(res[0])\r\n\t    next_token_logits = out[:, -1, :]\r\n\r\n\t    # forcing contiguous does not help :-(\r\n\t    # next_token_logits = next_token_logits.contiguous()\r\n\r\n\t    return next_token_logits\r\n\r\n\tnext_token_logits = get_next_token_logits(ort_inp)\r\n\ttime.sleep(2)\r\n\twith torch.profiler.profile(\r\n\t    activities=[\r\n\t        torch.profiler.ProfilerActivity.CPU,\r\n\t        torch.profiler.ProfilerActivity.CUDA,\r\n\t    ]\r\n\t) as p:\r\n\t\tfor i in range(n_loop):\r\n\t\t\tif use_ort_in_loop:\r\n\t\t\t\tnext_token_logits = get_next_token_logits(ort_inp)\r\n\t\t\tstart = time.time()\r\n\t\t\ttorch.matmul(next_token_logits, M)\r\n\t\t\ttotal_no_infsession += [time.time() - start]\r\n\tprint(f\"use_ort_in_loop={use_ort_in_loop}, operation took {sum(total_no_infsession) / n_loop * 1000} ms over {n_loop} runs\")\r\n\tprint(p.key_averages().table( sort_by=\"self_cpu_time_total\", row_limit=-1))\r\n\r\nwith torch.no_grad():\r\n    test(True)\r\n    test(False)\r\n```\r\n\r\nThe output:\r\n```\r\nuse_ort_in_loop=True, operation took 32.16004133224487 ms over 100 runs\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                   aten::mm        93.75%        3.214s        93.75%        3.214s      32.140ms           100\r\n      cudaDeviceSynchronize         6.07%     208.037ms         6.07%     208.037ms     208.037ms             1\r\n    cudaGetDeviceProperties         0.06%       2.213ms         0.06%       2.213ms       2.213ms             1\r\n                aten::slice         0.05%       1.772ms         0.06%       2.084ms      10.420us           200\r\n               aten::select         0.02%     723.000us         0.02%     823.000us       8.230us           100\r\n               aten::matmul         0.02%     515.000us        93.76%        3.215s      32.145ms           100\r\n           aten::lift_fresh         0.01%     441.000us         0.01%     441.000us       4.410us           100\r\n           aten::as_strided         0.01%     412.000us         0.01%     412.000us       1.373us           300\r\n         cudaGetDeviceCount         0.01%     189.000us         0.01%     189.000us     189.000us             1\r\n         aten::resolve_conj         0.00%       2.000us         0.00%       2.000us       0.007us           300\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 3.428s\r\n\r\nSTAGE:2022-12-01 22:37:36 2951695:2951695 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 22:37:36 2951695:2951695 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=False, operation took 0.2479982376098633 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        94.07%      24.398ms        94.07%      24.399ms     243.990us           100\r\n             aten::matmul         5.88%       1.525ms        94.46%      24.498ms     244.980us           100\r\n    cudaDeviceSynchronize         0.05%      12.000us         0.05%      12.000us      12.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 25.936ms\r\n\r\n```\r\n\r\nRight now, I am not sure why there are extra code called (like aten::slice).\r\n\r\nOne possible cause is ORT session has thread pool, and torch uses threads for matmul. In some way, ORT thread that slows down torch thread.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334195038/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334203162",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334203162",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334203162,
        "node_id": "IC_kwDOCVq1mM5PhlMa",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T18:53:17Z",
        "updated_at": "2022-12-01T19:21:04Z",
        "author_association": "CONTRIBUTOR",
        "body": "I'll try again tomorrow, I had the issue with contiguous as well. Edit: interestingly on my home laptop I can't reproduce the x50 slowdown, only x2 (both contiguous and non contiguous).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334203162/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334579176",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334579176",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334579176,
        "node_id": "IC_kwDOCVq1mM5PjA_o",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-01T23:32:15Z",
        "updated_at": "2022-12-03T19:11:23Z",
        "author_association": "MEMBER",
        "body": "I did an experiments which shows that reducing ORT threads help: 31ms => 0.3ms. It confirms that the cause is thread conflictions between ORT and Torch.\r\n\r\n```\r\nimport time\r\n\r\nimport onnxruntime as ort\r\nimport psutil\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\ndef test(\r\n    use_ort_in_loop,\r\n    num_thread_ort,\r\n    num_thread_torch,\r\n    contiguous=False,\r\n    n_loop=100,\r\n    disable_ort_spinning=False,\r\n    profiling=False,\r\n):\r\n    batch_size = 4\r\n    sequence_length = 32\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\r\n\r\n    inp = {\r\n        \"input_ids\": torch.randint(tokenizer.vocab_size - 1, (batch_size, 5), dtype=torch.int64),\r\n        \"encoder_attention_mask\": torch.ones(batch_size, sequence_length, dtype=torch.int64),\r\n        \"encoder_hidden_states\": torch.rand((batch_size, sequence_length, 512), dtype=torch.float32),\r\n    }\r\n\r\n    ort_inp = {}\r\n    for key in inp:\r\n        ort_inp[key] = inp[key].detach().cpu().numpy()\r\n\r\n    sess_options = ort.SessionOptions()\r\n    if disable_ort_spinning:\r\n        sess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\r\n\r\n    session = ort.InferenceSession(\"decoder_model.onnx\", sess_options=sess_options, providers=[\"CPUExecutionProvider\"])\r\n\r\n    M = torch.zeros(32128, 2)\r\n\r\n    def get_next_token_logits(ort_inp):\r\n        res = session.run(None, ort_inp)\r\n        out = torch.from_numpy(res[0])\r\n        next_token_logits = out[:, -1, :]\r\n        return next_token_logits.contiguous() if contiguous else next_token_logits\r\n\r\n    next_token_logits = get_next_token_logits(ort_inp)\r\n    time.sleep(15)\r\n\r\n    torch.set_num_threads(num_thread_torch)\r\n\r\n    def run_perf_test(next_token_logits):\r\n        latency = []\r\n        for _ in range(n_loop):\r\n            if use_ort_in_loop:\r\n                next_token_logits = get_next_token_logits(ort_inp)\r\n            start = time.time()\r\n            torch.matmul(next_token_logits, M)\r\n            latency += [time.time() - start]\r\n        return latency\r\n\r\n    if profiling:\r\n        with torch.profiler.profile(\r\n            activities=[\r\n                torch.profiler.ProfilerActivity.CPU,\r\n                torch.profiler.ProfilerActivity.CUDA,\r\n            ]\r\n        ) as p:\r\n            latency = run_perf_test(next_token_logits)\r\n        print(p.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))\r\n    else:\r\n        latency = run_perf_test(next_token_logits)\r\n\r\n    print(\r\n        f\"use_ort_in_loop={use_ort_in_loop}, disable_ort_spinning={disable_ort_spinning} ort_threads={num_thread_ort}, torch_threads={num_thread_torch}, operation took {sum(latency) / n_loop * 1000:.1f} ms over {n_loop} runs\"\r\n    )\r\n\r\n\r\nwith torch.no_grad():\r\n    cpu_count = psutil.cpu_count(logical=False)\r\n    for disable_spinning in [False, True]:\r\n        test(True, int(cpu_count), int(cpu_count), disable_ort_spinning=disable_spinning)\r\n        test(True, int(cpu_count / 2), int(cpu_count / 2), disable_ort_spinning=disable_spinning)\r\n        test(False, int(cpu_count), int(cpu_count), disable_ort_spinning=disable_spinning)\r\n        test(False, int(cpu_count / 2), int(cpu_count / 2), disable_ort_spinning=disable_spinning)\r\n```\r\n\r\nThe output:\r\n```\r\nuse_ort_in_loop=True, ort_threads=4, torch_threads=4, operation took 21.24567747116089 ms over 100 runs\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                   aten::mm        90.60%        2.123s        90.60%        2.123s      21.227ms           100\r\n      cudaDeviceSynchronize         9.13%     214.017ms         9.13%     214.017ms     214.017ms             1\r\n    cudaGetDeviceProperties         0.09%       2.170ms         0.09%       2.170ms       2.170ms             1\r\n                aten::slice         0.07%       1.638ms         0.08%       1.962ms       9.810us           200\r\n               aten::select         0.03%     737.000us         0.04%     837.000us       8.370us           100\r\n               aten::matmul         0.03%     621.000us        90.62%        2.123s      21.233ms           100\r\n           aten::lift_fresh         0.02%     504.000us         0.02%     504.000us       5.040us           100\r\n           aten::as_strided         0.02%     424.000us         0.02%     424.000us       1.413us           300\r\n         cudaGetDeviceCount         0.01%     197.000us         0.01%     197.000us     197.000us             1\r\n         aten::resolve_conj         0.00%       6.000us         0.00%       6.000us       0.020us           300\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 2.343s\r\n\r\nSTAGE:2022-12-01 23:38:03 2962600:2962600 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 23:38:05 2962600:2962600 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=True, ort_threads=2, torch_threads=2, operation took 0.32085180282592773 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        89.22%      30.525ms        89.22%      30.526ms     305.260us           100\r\n              aten::slice         4.70%       1.609ms         5.64%       1.928ms       9.640us           200\r\n             aten::select         2.00%     685.000us         2.31%     790.000us       7.900us           100\r\n             aten::matmul         1.48%     507.000us        90.71%      31.033ms     310.330us           100\r\n         aten::lift_fresh         1.31%     447.000us         1.31%     447.000us       4.470us           100\r\n         aten::as_strided         1.24%     424.000us         1.24%     424.000us       1.413us           300\r\n    cudaDeviceSynchronize         0.04%      15.000us         0.04%      15.000us      15.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 34.213ms\r\n\r\nSTAGE:2022-12-01 23:38:21 2962600:2962600 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 23:38:21 2962600:2962600 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=False, ort_threads=4, torch_threads=4, operation took 0.8952760696411133 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        99.33%      89.091ms        99.33%      89.092ms     890.920us           100\r\n             aten::matmul         0.65%     587.000us        99.46%      89.203ms     892.030us           100\r\n    cudaDeviceSynchronize         0.01%      12.000us         0.01%      12.000us      12.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 89.691ms\r\n\r\nSTAGE:2022-12-01 23:38:38 2962600:2962600 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-01 23:38:38 2962600:2962600 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\nuse_ort_in_loop=False, ort_threads=2, torch_threads=2, operation took 0.2782249450683594 ms over 100 runs\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                 aten::mm        96.15%      27.365ms        96.15%      27.366ms     273.660us           100\r\n             aten::matmul         3.81%       1.084ms        96.57%      27.485ms     274.850us           100\r\n    cudaDeviceSynchronize         0.04%      12.000us         0.04%      12.000us      12.000us             1\r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.003us           300\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\nSelf CPU time total: 28.462ms\r\n```\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334579176/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334895190",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1334895190",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1334895190,
        "node_id": "IC_kwDOCVq1mM5PkOJW",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-02T08:19:20Z",
        "updated_at": "2022-12-02T08:19:20Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks a lot for the followup and better profiling! Do you see it as being a bug/issue in ONNX Runtime?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1334895190/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1335867352",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1335867352",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1335867352,
        "node_id": "IC_kwDOCVq1mM5Pn7fY",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-02T21:37:55Z",
        "updated_at": "2022-12-02T21:37:55Z",
        "author_association": "MEMBER",
        "body": "I do not think it is a bug. It is expected that when total threads of a process are larger than number of cpu cores, the performance will be impacted due to thread context switch.\r\n\r\nTo achieve better performance, I suggest experiments on thread setting of ORT and PyTorch & Numpy.\r\nhttps://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html\r\nhttps://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy\r\n\r\nORT python API has two settings (inter_op_num_threads and intra_op_num_threads):\r\nhttps://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.SessionOptions.inter_op_num_threads\r\n\r\n@pranavsharma, any suggestions on this topic?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1335867352/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336007732",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336007732",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336007732,
        "node_id": "IC_kwDOCVq1mM5Podw0",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T01:51:55Z",
        "updated_at": "2022-12-03T05:12:08Z",
        "author_association": "MEMBER",
        "body": "Couple of options:\r\n\r\n1. Is it possible to delete the session object once you're done with inferencing by calling ```del session```? That'll cause the session and hence the ORT threadpool to be destructed.\r\n2. Another option is to pin pytorch and ORT threads to different cores, but we don't support that in ORT today (it's work in progress).\r\n3. You can also try by turning off spinning using [this config](https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/session/onnxruntime_session_options_config_keys.h#L88).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336007732/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336109177",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336109177",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336109177,
        "node_id": "IC_kwDOCVq1mM5Po2h5",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T08:01:38Z",
        "updated_at": "2022-12-03T16:24:53Z",
        "author_association": "CONTRIBUTOR",
        "body": "Thanks for your answers and suggestion! Will try it out. Will check if PyTorch + Numpy have the same issue or not.\r\n\r\nDoes this mean that torchdynamo + ort as execution backend will be very bad (by default) on CPU in case the whole graph is not traced (edit: spoiler: IMO yes ðŸ˜…)? Or does dynamo works differently than spawning InferenceSessions?\r\n\r\n```\r\nimport torch._dynamo as dynamo\r\n\r\ndynamo_model = dynamo.optimize(\"onnxrt\")(model)\r\n```\r\n\r\nRelevant torchdynamo backend: https://github.com/pytorch/pytorch/blob/1ee189ce8ebf392b4a9c026f040b14f7145ca5e6/torch/_dynamo/optimizations/backends.py#L135-L176 (looks weird, there's IOBinding even on CPU @ezyang)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336109177/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219034",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336219034",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336219034,
        "node_id": "IC_kwDOCVq1mM5PpRWa",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T18:58:46Z",
        "updated_at": "2022-12-03T19:12:17Z",
        "author_association": "MEMBER",
        "body": "@fxmarty,  add the following setting seems help:\r\n```\r\nsess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\r\n```\r\n\r\nUpdated the previous script inline. Here is the result:\r\n```\r\nuse_ort_in_loop=True, disable_ort_spinning=False ort_threads=4, torch_threads=4, operation took 23.15556049346924 ms over 100 runs\r\nuse_ort_in_loop=True, disable_ort_spinning=False ort_threads=2, torch_threads=2, operation took 4.832360744476318 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=False ort_threads=4, torch_threads=4, operation took 0.7350826263427734 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=False ort_threads=2, torch_threads=2, operation took 0.24795770645141602 ms over 100 runs\r\nuse_ort_in_loop=True, disable_ort_spinning=True ort_threads=4, torch_threads=4, operation took 0.8697772026062012 ms over 100 runs\r\nuse_ort_in_loop=True, disable_ort_spinning=True ort_threads=2, torch_threads=2, operation took 0.33701181411743164 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=True ort_threads=4, torch_threads=4, operation took 0.7239651679992676 ms over 100 runs\r\nuse_ort_in_loop=False, disable_ort_spinning=True ort_threads=2, torch_threads=2, operation took 0.24130821228027344 ms over 100 runs\r\n\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219034/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219493",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1336219493",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1336219493,
        "node_id": "IC_kwDOCVq1mM5PpRdl",
        "user": {
            "login": "ezyang",
            "id": 13564,
            "node_id": "MDQ6VXNlcjEzNTY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/13564?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ezyang",
            "html_url": "https://github.com/ezyang",
            "followers_url": "https://api.github.com/users/ezyang/followers",
            "following_url": "https://api.github.com/users/ezyang/following{/other_user}",
            "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions",
            "organizations_url": "https://api.github.com/users/ezyang/orgs",
            "repos_url": "https://api.github.com/users/ezyang/repos",
            "events_url": "https://api.github.com/users/ezyang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ezyang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-03T19:01:10Z",
        "updated_at": "2022-12-03T19:01:10Z",
        "author_association": "NONE",
        "body": "We haven't really heavily tested the ORT backend so there may be flagrant performance problems for no good reason. One thing is that you should imagine what the performance of your program would be if you replaced your model with several calls into ORT backend. That should give a sense for what the perf is",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1336219493/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1337177148",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1337177148",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1337177148,
        "node_id": "IC_kwDOCVq1mM5Ps7Q8",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-05T11:28:59Z",
        "updated_at": "2022-12-05T11:33:38Z",
        "author_association": "CONTRIBUTOR",
        "body": "Here's an updated script:\r\n\r\n<details>\r\n  <summary>Script</summary>\r\n\r\n```python\r\nimport time\r\n\r\nimport onnxruntime as ort\r\nimport psutil\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\ndef test(\r\n    use_ort_in_loop,\r\n    use_pt_in_loop,\r\n    num_thread_ort,\r\n    num_thread_torch,\r\n    contiguous=True,\r\n    n_loop=200,\r\n    disable_ort_spinning=False,\r\n    profiling=True,\r\n):\r\n    batch_size = 4\r\n    sequence_length = 32\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\r\n\r\n    inp = {\r\n        \"input_ids\": torch.randint(tokenizer.vocab_size - 1, (batch_size, 5), dtype=torch.int64),\r\n        \"encoder_attention_mask\": torch.ones(batch_size, sequence_length, dtype=torch.int64),\r\n        \"encoder_hidden_states\": torch.rand((batch_size, sequence_length, 512), dtype=torch.float32),\r\n    }\r\n\r\n    ort_inp = {}\r\n    for key in inp:\r\n        ort_inp[key] = inp[key].detach().cpu().numpy()\r\n\r\n    sess_options = ort.SessionOptions()\r\n    sess_options.intra_op_num_threads = num_thread_ort\r\n    if disable_ort_spinning:\r\n        sess_options.add_session_config_entry(\"session.intra_op.allow_spinning\", \"0\")\r\n\r\n    session = ort.InferenceSession(\"decoder_model.onnx\", sess_options=sess_options, providers=[\"CPUExecutionProvider\"])\r\n\r\n    M = torch.zeros(32128, 100)\r\n\r\n    def get_next_token_logits(ort_inp):\r\n        res = session.run(None, ort_inp)\r\n        next_token_logits = torch.from_numpy(res[0])\r\n        return next_token_logits.contiguous() if contiguous else next_token_logits\r\n\r\n    next_token_logits = get_next_token_logits(ort_inp)\r\n    time.sleep(10)\r\n\r\n    torch.set_num_threads(num_thread_torch)\r\n\r\n    def run_perf_test(next_token_logits):\r\n        latency = [0]\r\n        ort_latency = [0]\r\n        for _ in range(n_loop):\r\n            if use_ort_in_loop:\r\n                start = time.time()\r\n                next_token_logits = get_next_token_logits(ort_inp)\r\n                ort_latency += [time.time() - start]\r\n            if use_pt_in_loop:\r\n                start = time.time()\r\n                torch.matmul(next_token_logits, M)\r\n                latency += [time.time() - start]\r\n        return latency, ort_latency\r\n\r\n    if profiling:\r\n        with torch.profiler.profile(\r\n            activities=[\r\n                torch.profiler.ProfilerActivity.CPU,\r\n                torch.profiler.ProfilerActivity.CUDA,\r\n            ]\r\n        ) as p:\r\n            latency, ort_latency = run_perf_test(next_token_logits)\r\n        print(p.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))\r\n    else:\r\n        latency, ort_latency = run_perf_test(next_token_logits)\r\n\r\n    print(\r\n        f\"use_ort={use_ort_in_loop}, use_pt={use_pt_in_loop}, disable_ort_spinning={disable_ort_spinning} ort_threads={num_thread_ort}, torch_threads={num_thread_torch}, PT operation took {sum(latency) * 1e3:.1f} ms, ORT operation took {sum(ort_latency) * 1e3:.1f} ms over {n_loop} runs\"\r\n    )\r\n    print(\"------------\")\r\n\r\n\r\nwith torch.no_grad():\r\n    cpu_count = psutil.cpu_count(logical=False)\r\n\r\n    for disable_spinning in [False, True]:\r\n        test(True, True, int(cpu_count), int(cpu_count), disable_ort_spinning=disable_spinning)\r\n        test(True, True, int(cpu_count) // 2, int(cpu_count) // 2, disable_ort_spinning=disable_spinning)\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n  <summary>Results on AWS EC2 c6i instance (with a piece of Platinum 8375C CPU, 4 physical cores)</summary>\r\n\r\n```\r\nSTAGE:2022-12-05 11:21:41 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:21:43 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        97.13%     242.705ms        97.13%     242.709ms       1.214ms           200  \r\n            aten::matmul         1.04%       2.611ms        99.72%     249.184ms       1.246ms           200  \r\n           aten::reshape         0.64%       1.597ms         1.08%       2.700ms      13.500us           200  \r\n    aten::_reshape_alias         0.49%       1.228ms         0.49%       1.228ms       6.140us           200  \r\n      aten::_unsafe_view         0.42%       1.039ms         0.42%       1.039ms       5.195us           200  \r\n        aten::lift_fresh         0.28%     701.000us         0.28%     701.000us       3.505us           200  \r\n      aten::resolve_conj         0.00%       4.000us         0.00%       4.000us       0.007us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 249.885ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=4, torch_threads=4, PT operation took 251.7 ms, ORT operation took 2005.3 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 11:21:53 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:21:57 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        98.37%     459.505ms        98.37%     459.507ms       2.298ms           200  \r\n            aten::matmul         0.60%       2.815ms        99.83%     466.335ms       2.332ms           200  \r\n           aten::reshape         0.35%       1.627ms         0.61%       2.843ms      14.215us           200  \r\n    aten::_reshape_alias         0.29%       1.333ms         0.29%       1.333ms       6.665us           200  \r\n      aten::_unsafe_view         0.23%       1.053ms         0.23%       1.053ms       5.265us           200  \r\n        aten::lift_fresh         0.17%     776.000us         0.17%     776.000us       3.880us           200  \r\n      aten::resolve_conj         0.00%       2.000us         0.00%       2.000us       0.003us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 467.111ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=2, torch_threads=2, PT operation took 469.0 ms, ORT operation took 3139.4 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 11:22:08 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:22:11 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        96.50%     199.427ms        96.50%     199.431ms     997.155us           200  \r\n            aten::matmul         1.22%       2.513ms        99.59%     205.809ms       1.029ms           200  \r\n           aten::reshape         0.77%       1.592ms         1.41%       2.905ms      14.525us           200  \r\n    aten::_reshape_alias         0.66%       1.369ms         0.66%       1.369ms       6.845us           200  \r\n      aten::_unsafe_view         0.44%     904.000us         0.44%     904.000us       4.520us           200  \r\n        aten::lift_fresh         0.41%     846.000us         0.41%     846.000us       4.230us           200  \r\n      aten::resolve_conj         0.00%       4.000us         0.00%       4.000us       0.007us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 206.655ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=4, torch_threads=4, PT operation took 208.3 ms, ORT operation took 2891.3 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 11:22:21 5513:5513 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 11:22:25 5513:5513 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                aten::mm        97.57%     282.228ms        97.57%     282.231ms       1.411ms           200  \r\n            aten::matmul         0.88%       2.538ms        99.74%     288.486ms       1.442ms           200  \r\n           aten::reshape         0.54%       1.575ms         0.94%       2.722ms      13.610us           200  \r\n    aten::_reshape_alias         0.42%       1.227ms         0.42%       1.227ms       6.135us           200  \r\n      aten::_unsafe_view         0.32%     915.000us         0.32%     915.000us       4.575us           200  \r\n        aten::lift_fresh         0.26%     765.000us         0.26%     765.000us       3.825us           200  \r\n      aten::resolve_conj         0.00%       3.000us         0.00%       3.000us       0.005us           600  \r\n------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 289.251ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=2, torch_threads=2, PT operation took 291.0 ms, ORT operation took 3259.3 ms over 200 runs\r\n------------\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n  <summary>Results on my laptop (with i7-1280P, that has both E-cores and P-cores)</summary>\r\n\r\n```\r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                   aten::mm        97.80%        6.796s        97.80%        6.796s      33.979ms           200  \r\n      cudaDeviceSynchronize         1.74%     121.127ms         1.74%     121.127ms     121.127ms             1  \r\n               aten::matmul         0.36%      25.196ms        98.24%        6.826s      34.132ms           200  \r\n              aten::reshape         0.03%       2.187ms         0.06%       3.918ms      19.590us           200  \r\n       aten::_reshape_alias         0.03%       1.744ms         0.03%       1.744ms       8.720us           200  \r\n         aten::_unsafe_view         0.02%       1.420ms         0.02%       1.420ms       7.100us           200  \r\n           aten::lift_fresh         0.01%       1.006ms         0.01%       1.006ms       5.030us           200  \r\n         cudaGetDeviceCount         0.00%     163.000us         0.00%     163.000us     163.000us             1  \r\n    cudaGetDeviceProperties         0.00%      95.000us         0.00%      95.000us      95.000us             1  \r\n         aten::resolve_conj         0.00%      36.000us         0.00%      36.000us       0.060us           600  \r\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 6.949s\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=14, torch_threads=14, PT operation took 6830.3 ms, ORT operation took 2642.2 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 12:19:48 22172:22172 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 12:19:51 22172:22172 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                 aten::mm        95.21%     193.453ms        95.22%     193.474ms     967.370us           200  \r\n             aten::matmul         1.85%       3.758ms        99.47%     202.094ms       1.010ms           200  \r\n            aten::reshape         0.99%       2.003ms         1.81%       3.674ms      18.370us           200  \r\n     aten::_reshape_alias         0.84%       1.698ms         0.84%       1.698ms       8.490us           200  \r\n       aten::_unsafe_view         0.57%       1.161ms         0.57%       1.161ms       5.805us           200  \r\n         aten::lift_fresh         0.53%       1.072ms         0.53%       1.072ms       5.360us           200  \r\n       aten::resolve_conj         0.01%      21.000us         0.01%      21.000us       0.035us           600  \r\n    cudaDeviceSynchronize         0.01%      14.000us         0.01%      14.000us      14.000us             1  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 203.180ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=False ort_threads=7, torch_threads=7, PT operation took 205.2 ms, ORT operation took 2778.5 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 12:20:03 22172:22172 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 12:20:05 22172:22172 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                 aten::mm        98.37%     244.519ms        98.37%     244.522ms       1.223ms           200  \r\n             aten::matmul         0.64%       1.580ms        99.85%     248.196ms       1.241ms           200  \r\n            aten::reshape         0.41%       1.018ms         0.68%       1.680ms       8.400us           200  \r\n     aten::_reshape_alias         0.28%     696.000us         0.28%     696.000us       3.480us           200  \r\n       aten::_unsafe_view         0.15%     380.000us         0.15%     380.000us       1.900us           200  \r\n         aten::lift_fresh         0.15%     361.000us         0.15%     361.000us       1.805us           200  \r\n    cudaDeviceSynchronize         0.00%      11.000us         0.00%      11.000us      11.000us             1  \r\n       aten::resolve_conj         0.00%       3.000us         0.00%       3.000us       0.005us           600  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 248.568ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=14, torch_threads=14, PT operation took 249.8 ms, ORT operation took 2117.6 ms over 200 runs\r\n------------\r\nSTAGE:2022-12-05 12:20:17 22172:22172 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\r\nSTAGE:2022-12-05 12:20:20 22172:22172 ActivityProfilerController.cpp:300] Completed Stage: Collection\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                 aten::mm        98.12%     175.821ms        98.12%     175.822ms     879.110us           200  \r\n             aten::matmul         0.83%       1.492ms        99.87%     178.952ms     894.760us           200  \r\n            aten::reshape         0.44%     787.000us         0.70%       1.263ms       6.315us           200  \r\n     aten::_reshape_alias         0.30%     529.000us         0.30%     529.000us       2.645us           200  \r\n       aten::_unsafe_view         0.18%     322.000us         0.18%     322.000us       1.610us           200  \r\n         aten::lift_fresh         0.13%     229.000us         0.13%     229.000us       1.145us           200  \r\n    cudaDeviceSynchronize         0.01%      11.000us         0.01%      11.000us      11.000us             1  \r\n       aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       0.002us           600  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 179.192ms\r\n\r\nuse_ort=True, use_pt=True, disable_ort_spinning=True ort_threads=7, torch_threads=7, PT operation took 180.4 ms, ORT operation took 2496.5 ms over 200 runs\r\n------------\r\n```\r\n</details>\r\n\r\nTL;DR, for this specific model and specific pytorch operation:\r\n* On the c6i instance, I have no issue of 50x slowdown when mixing ORT + PyTorch to start with\r\n* Disabling ORT thread spinning hurts ORT latency on the c6i instance, but not on my laptop\r\n* On the c6i instance, passing from 4 threads to 2 threads hurt both ORT and PT perfs \r\n* On my laptop, passing from 4 threads to 2 threads does not hurt ORT perfs",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1337177148/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1339040093",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13808#issuecomment-1339040093",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13808",
        "id": 1339040093,
        "node_id": "IC_kwDOCVq1mM5P0CFd",
        "user": {
            "login": "fxmarty",
            "id": 9808326,
            "node_id": "MDQ6VXNlcjk4MDgzMjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxmarty",
            "html_url": "https://github.com/fxmarty",
            "followers_url": "https://api.github.com/users/fxmarty/followers",
            "following_url": "https://api.github.com/users/fxmarty/following{/other_user}",
            "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions",
            "organizations_url": "https://api.github.com/users/fxmarty/orgs",
            "repos_url": "https://api.github.com/users/fxmarty/repos",
            "events_url": "https://api.github.com/users/fxmarty/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fxmarty/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-06T09:39:54Z",
        "updated_at": "2022-12-08T19:06:09Z",
        "author_association": "CONTRIBUTOR",
        "body": "@tianleiwu On which hardware/CPU did you reproduce?\r\n\r\nNote: the issue can be reproduced on Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1339040093/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]