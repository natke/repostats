[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178114456",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1178114456",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1178114456,
        "node_id": "IC_kwDOCVq1mM5GOJmY",
        "user": {
            "login": "YUNQIUGUO",
            "id": 35738743,
            "node_id": "MDQ6VXNlcjM1NzM4NzQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/35738743?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YUNQIUGUO",
            "html_url": "https://github.com/YUNQIUGUO",
            "followers_url": "https://api.github.com/users/YUNQIUGUO/followers",
            "following_url": "https://api.github.com/users/YUNQIUGUO/following{/other_user}",
            "gists_url": "https://api.github.com/users/YUNQIUGUO/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/YUNQIUGUO/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/YUNQIUGUO/subscriptions",
            "organizations_url": "https://api.github.com/users/YUNQIUGUO/orgs",
            "repos_url": "https://api.github.com/users/YUNQIUGUO/repos",
            "events_url": "https://api.github.com/users/YUNQIUGUO/events{/privacy}",
            "received_events_url": "https://api.github.com/users/YUNQIUGUO/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-07T19:21:21Z",
        "updated_at": "2022-07-07T19:21:21Z",
        "author_association": "MEMBER",
        "body": "I am not sure if beam search support large t5 model yet. @tianleiwu Any insights?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178114456/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178189858",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1178189858",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1178189858,
        "node_id": "IC_kwDOCVq1mM5GOcAi",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-07T20:30:00Z",
        "updated_at": "2022-07-07T20:34:28Z",
        "author_association": "MEMBER",
        "body": "@[NouamaneTazi](https://github.com/NouamaneTazi), the bottleneck is memory size of hardware. You might notice that the pytorch model itself is 42GB. Convert the model need space (both memory and disk) of multiple times of model size. Even A100 might not have enough GPU memory for this task.\r\n\r\nCould you remove `--use_gpu` and use a machine with enough CPU memory (like 256GB)? Also, please use nightly package instead of ORT 1.11.1, since some change in BeamSearch interface is not backward compatible.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178189858/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178202871",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1178202871",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1178202871,
        "node_id": "IC_kwDOCVq1mM5GOfL3",
        "user": {
            "login": "NouamaneTazi",
            "id": 29777165,
            "node_id": "MDQ6VXNlcjI5Nzc3MTY1",
            "avatar_url": "https://avatars.githubusercontent.com/u/29777165?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/NouamaneTazi",
            "html_url": "https://github.com/NouamaneTazi",
            "followers_url": "https://api.github.com/users/NouamaneTazi/followers",
            "following_url": "https://api.github.com/users/NouamaneTazi/following{/other_user}",
            "gists_url": "https://api.github.com/users/NouamaneTazi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/NouamaneTazi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/NouamaneTazi/subscriptions",
            "organizations_url": "https://api.github.com/users/NouamaneTazi/orgs",
            "repos_url": "https://api.github.com/users/NouamaneTazi/repos",
            "events_url": "https://api.github.com/users/NouamaneTazi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/NouamaneTazi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-07T20:41:45Z",
        "updated_at": "2022-07-07T20:41:45Z",
        "author_association": "NONE",
        "body": "Thank you @tianleiwu. How can I run inference for such large models on a multi GPU setup? Can you point me to some example?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178202871/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178268597",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1178268597",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1178268597,
        "node_id": "IC_kwDOCVq1mM5GOvO1",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-07-07T21:57:41Z",
        "updated_at": "2022-07-07T23:59:45Z",
        "author_association": "MEMBER",
        "body": "I guess one A100 with 80GB might be able to inference t5-11b model in float32. \r\n![image](https://user-images.githubusercontent.com/30328909/177890544-cbc8d6b8-637e-4156-b225-a177b71d98f8.png)\r\n\r\nIf you convert the ONNX model to mixed precision (with potential loss in accuracy), it might be able to run in 40GB A100 (I have not tried it so it is not sure).\r\n\r\n@NouamaneTazi, currently ORT supports one session in one GPU (That also means BeamSearch operator can only run in one GPU).\r\n\r\nFor t5-11b, you might try create one session for encoder in one GPU, and a session for decoder in another GPU.\r\n\r\nFor larger model, a possible walkaround is to split a large model to multiple models (like split 24 layers to 2 models with 12 layers in each model), then create one inference session for each model, and each session is running in different GPU. You will need implement a custom version of beam search for this purpose.\r\n \r\n@viboga, could you share prototype of beam search custom operator when it is ready. Other users could modify it to support multiple models.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1178268597/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1305251661",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1305251661",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1305251661,
        "node_id": "IC_kwDOCVq1mM5NzI9N",
        "user": {
            "login": "YifanWang-22",
            "id": 108775848,
            "node_id": "U_kgDOBnvJqA",
            "avatar_url": "https://avatars.githubusercontent.com/u/108775848?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YifanWang-22",
            "html_url": "https://github.com/YifanWang-22",
            "followers_url": "https://api.github.com/users/YifanWang-22/followers",
            "following_url": "https://api.github.com/users/YifanWang-22/following{/other_user}",
            "gists_url": "https://api.github.com/users/YifanWang-22/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/YifanWang-22/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/YifanWang-22/subscriptions",
            "organizations_url": "https://api.github.com/users/YifanWang-22/orgs",
            "repos_url": "https://api.github.com/users/YifanWang-22/repos",
            "events_url": "https://api.github.com/users/YifanWang-22/events{/privacy}",
            "received_events_url": "https://api.github.com/users/YifanWang-22/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-07T08:27:37Z",
        "updated_at": "2022-11-07T08:31:35Z",
        "author_association": "NONE",
        "body": "@tianleiwu Hello there, I encountered the same CUDA out of memory error when converting a large transformer model. However I want to convert my model in FP16, which cpu does not support. Thus disabling `--use_gpu` cannot help me.\r\nI wonder if there's any other workaround to convert the large model to onnx with gpu? Really appreciate your help!\r\n\r\n**System Information**\r\npytorch: 1.13\r\nonnxruntime: latest version",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1305251661/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1324020545",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1324020545",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1324020545,
        "node_id": "IC_kwDOCVq1mM5O6vNB",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-22T17:29:59Z",
        "updated_at": "2022-11-22T17:39:17Z",
        "author_association": "MEMBER",
        "body": "@YifanWang-22, you can export the model to fp32 in CPU with large memory using PyTorch.\r\n\r\nThere is separated python script that could convert ONNX model from FP32 to FP16. The script can run in CPU only: https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py\r\n\r\nIf you want to export huge model in CUDA, you will need some extra work: export your model in several parts.  For example, you can export layer by layer: embedding layer, transformer layer and pooling layer separately. Then have another script to optimize each layer and convert it to FP16, and finally concatenate all layers into one graph.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1324020545/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1324568542",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/12124#issuecomment-1324568542",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/12124",
        "id": 1324568542,
        "node_id": "IC_kwDOCVq1mM5O80_e",
        "user": {
            "login": "YifanWang-22",
            "id": 108775848,
            "node_id": "U_kgDOBnvJqA",
            "avatar_url": "https://avatars.githubusercontent.com/u/108775848?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YifanWang-22",
            "html_url": "https://github.com/YifanWang-22",
            "followers_url": "https://api.github.com/users/YifanWang-22/followers",
            "following_url": "https://api.github.com/users/YifanWang-22/following{/other_user}",
            "gists_url": "https://api.github.com/users/YifanWang-22/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/YifanWang-22/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/YifanWang-22/subscriptions",
            "organizations_url": "https://api.github.com/users/YifanWang-22/orgs",
            "repos_url": "https://api.github.com/users/YifanWang-22/repos",
            "events_url": "https://api.github.com/users/YifanWang-22/events{/privacy}",
            "received_events_url": "https://api.github.com/users/YifanWang-22/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-23T05:24:11Z",
        "updated_at": "2022-11-23T05:24:11Z",
        "author_association": "NONE",
        "body": "Thanks for your advice @tianleiwu, but the problem I had encountered was the model I wanted to convert has mixed-precision where the cpu doesn't support fp16 part and CUDA lacked enough memory. We later solved such problem by just using a larger memory of CUDA.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1324568542/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]