[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1095938429",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1095938429",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1095938429,
        "node_id": "IC_kwDOCVq1mM5BUrF9",
        "user": {
            "login": "faxu",
            "id": 20780999,
            "node_id": "MDQ6VXNlcjIwNzgwOTk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/20780999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/faxu",
            "html_url": "https://github.com/faxu",
            "followers_url": "https://api.github.com/users/faxu/followers",
            "following_url": "https://api.github.com/users/faxu/following{/other_user}",
            "gists_url": "https://api.github.com/users/faxu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/faxu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/faxu/subscriptions",
            "organizations_url": "https://api.github.com/users/faxu/orgs",
            "repos_url": "https://api.github.com/users/faxu/repos",
            "events_url": "https://api.github.com/users/faxu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/faxu/received_events",
            "type": "User",
            "site_admin": true
        },
        "created_at": "2022-04-12T03:40:31Z",
        "updated_at": "2022-04-12T03:40:31Z",
        "author_association": "MEMBER",
        "body": "Are you using onnxmltools to convert fp32 to fp16?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1095938429/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1140309860",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1140309860",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1140309860,
        "node_id": "IC_kwDOCVq1mM5D979k",
        "user": {
            "login": "prashantskit",
            "id": 101550102,
            "node_id": "U_kgDOBg2IFg",
            "avatar_url": "https://avatars.githubusercontent.com/u/101550102?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/prashantskit",
            "html_url": "https://github.com/prashantskit",
            "followers_url": "https://api.github.com/users/prashantskit/followers",
            "following_url": "https://api.github.com/users/prashantskit/following{/other_user}",
            "gists_url": "https://api.github.com/users/prashantskit/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/prashantskit/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/prashantskit/subscriptions",
            "organizations_url": "https://api.github.com/users/prashantskit/orgs",
            "repos_url": "https://api.github.com/users/prashantskit/repos",
            "events_url": "https://api.github.com/users/prashantskit/events{/privacy}",
            "received_events_url": "https://api.github.com/users/prashantskit/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-28T18:26:24Z",
        "updated_at": "2022-05-30T06:20:05Z",
        "author_association": "NONE",
        "body": "@soundarthiaga , The solution you mention is specific to transformers. Can you suggest what I should do rectify my problem?\r\n\r\nHi, I am also experiencing the same issue. The performance of fp16 is slower in comparison to fp32 by significant amount on CPU. \r\n\r\nDescription:\r\nI am converting tacotron2 model to onnx using this link [onnx-export](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/exports/export_tacotron2_onnx.py)\r\nFramework - Pytorch version 1.10.2\r\nExporing method - torch.onnx.export\r\nopset - 11\r\nonnx version - 1.11.0\r\nOS - Ubuntu 20.04\r\nProcessor - Intel Xeon\r\nInference is done using onnxruntime\r\nonnx runtime version - 1.11.1\r\n\r\nI used convert_float_to_float16 function from onnxconverter_common [quantisation](https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py) input model is onnx float 32 and expected output is float16.\r\n\r\n```\r\nimport onnx\r\nfrom onnxconverter_common.float16 import convert_float_to_float16\r\n\r\nmodel_fp32_decoder = 'tacotron2/outdir/working-onnx/decoder_iter.onnx'\r\nmodel_quant_decoder = 'tacotron2/outdir/working-onnx/decoder_iter.fp16.onnx'\r\nmodel = onnx.load_model(model_fp32_decoder)\r\nnew_model = convert_float_to_float16(model)\r\n\r\nonnx.save_model(new_model, model_quant_decoder) \r\n```\r\n\r\nIn one of the issue I read fp16 is not supported in Arm but I am using Intel. Can somebody help with the issue? Let me know  if more information is needed.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1140309860/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1350659683",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1350659683",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1350659683,
        "node_id": "IC_kwDOCVq1mM5QgW5j",
        "user": {
            "login": "EmreOzkose",
            "id": 17765576,
            "node_id": "MDQ6VXNlcjE3NzY1NTc2",
            "avatar_url": "https://avatars.githubusercontent.com/u/17765576?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/EmreOzkose",
            "html_url": "https://github.com/EmreOzkose",
            "followers_url": "https://api.github.com/users/EmreOzkose/followers",
            "following_url": "https://api.github.com/users/EmreOzkose/following{/other_user}",
            "gists_url": "https://api.github.com/users/EmreOzkose/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/EmreOzkose/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/EmreOzkose/subscriptions",
            "organizations_url": "https://api.github.com/users/EmreOzkose/orgs",
            "repos_url": "https://api.github.com/users/EmreOzkose/repos",
            "events_url": "https://api.github.com/users/EmreOzkose/events{/privacy}",
            "received_events_url": "https://api.github.com/users/EmreOzkose/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-14T08:53:26Z",
        "updated_at": "2022-12-14T08:53:26Z",
        "author_association": "NONE",
        "body": "Any improvement on this issue?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1350659683/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1351889802",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1351889802",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1351889802,
        "node_id": "IC_kwDOCVq1mM5QlDOK",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-14T18:11:37Z",
        "updated_at": "2022-12-14T18:11:37Z",
        "author_association": "MEMBER",
        "body": "It is expected. ORT computes fp16 with fp32 operators on CPU now. Intel CPUs doesn't support fp16 actually. It introduces avx512-fp16 in SPR microarchitecture, which will be released to public early next year. We are investigating to add support for it.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1351889802/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1352865829",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1352865829",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1352865829,
        "node_id": "IC_kwDOCVq1mM5Qoxgl",
        "user": {
            "login": "EmreOzkose",
            "id": 17765576,
            "node_id": "MDQ6VXNlcjE3NzY1NTc2",
            "avatar_url": "https://avatars.githubusercontent.com/u/17765576?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/EmreOzkose",
            "html_url": "https://github.com/EmreOzkose",
            "followers_url": "https://api.github.com/users/EmreOzkose/followers",
            "following_url": "https://api.github.com/users/EmreOzkose/following{/other_user}",
            "gists_url": "https://api.github.com/users/EmreOzkose/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/EmreOzkose/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/EmreOzkose/subscriptions",
            "organizations_url": "https://api.github.com/users/EmreOzkose/orgs",
            "repos_url": "https://api.github.com/users/EmreOzkose/repos",
            "events_url": "https://api.github.com/users/EmreOzkose/events{/privacy}",
            "received_events_url": "https://api.github.com/users/EmreOzkose/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-12-15T10:34:51Z",
        "updated_at": "2022-12-15T10:34:51Z",
        "author_association": "NONE",
        "body": "Thank you for detailed explanation ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1352865829/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1405599998",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1405599998",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1405599998,
        "node_id": "IC_kwDOCVq1mM5Tx8D-",
        "user": {
            "login": "ShimaShahfar",
            "id": 60238747,
            "node_id": "MDQ6VXNlcjYwMjM4NzQ3",
            "avatar_url": "https://avatars.githubusercontent.com/u/60238747?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ShimaShahfar",
            "html_url": "https://github.com/ShimaShahfar",
            "followers_url": "https://api.github.com/users/ShimaShahfar/followers",
            "following_url": "https://api.github.com/users/ShimaShahfar/following{/other_user}",
            "gists_url": "https://api.github.com/users/ShimaShahfar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ShimaShahfar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ShimaShahfar/subscriptions",
            "organizations_url": "https://api.github.com/users/ShimaShahfar/orgs",
            "repos_url": "https://api.github.com/users/ShimaShahfar/repos",
            "events_url": "https://api.github.com/users/ShimaShahfar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ShimaShahfar/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-26T20:18:34Z",
        "updated_at": "2023-01-26T20:18:45Z",
        "author_association": "NONE",
        "body": "> It is expected. ORT computes fp16 with fp32 operators on CPU now. Intel CPUs doesn't support fp16 actually. It introduces avx512-fp16 in SPR microarchitecture, which will be released to public early next year. We are investigating to add support for it.\r\n\r\nAny update on this issue?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1405599998/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1406963762",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1406963762",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1406963762,
        "node_id": "IC_kwDOCVq1mM5T3JAy",
        "user": {
            "login": "yufenglee",
            "id": 30486710,
            "node_id": "MDQ6VXNlcjMwNDg2NzEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/30486710?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yufenglee",
            "html_url": "https://github.com/yufenglee",
            "followers_url": "https://api.github.com/users/yufenglee/followers",
            "following_url": "https://api.github.com/users/yufenglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions",
            "organizations_url": "https://api.github.com/users/yufenglee/orgs",
            "repos_url": "https://api.github.com/users/yufenglee/repos",
            "events_url": "https://api.github.com/users/yufenglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yufenglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-27T19:13:59Z",
        "updated_at": "2023-01-27T19:13:59Z",
        "author_association": "MEMBER",
        "body": "> > It is expected. ORT computes fp16 with fp32 operators on CPU now. Intel CPUs doesn't support fp16 actually. It introduces avx512-fp16 in SPR microarchitecture, which will be released to public early next year. We are investigating to add support for it.\r\n> \r\n> Any update on this issue?\r\n\r\nWe are adding fp16 support on ARM64 and targets to support heavy operators(MatMul, Conv) and operators in popular models(mobilenet and other CNN models) in 1.15.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1406963762/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1514635846",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10919#issuecomment-1514635846",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10919",
        "id": 1514635846,
        "node_id": "IC_kwDOCVq1mM5aR4JG",
        "user": {
            "login": "lucasjinreal",
            "id": 21303438,
            "node_id": "MDQ6VXNlcjIxMzAzNDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21303438?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucasjinreal",
            "html_url": "https://github.com/lucasjinreal",
            "followers_url": "https://api.github.com/users/lucasjinreal/followers",
            "following_url": "https://api.github.com/users/lucasjinreal/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucasjinreal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucasjinreal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucasjinreal/subscriptions",
            "organizations_url": "https://api.github.com/users/lucasjinreal/orgs",
            "repos_url": "https://api.github.com/users/lucasjinreal/repos",
            "events_url": "https://api.github.com/users/lucasjinreal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucasjinreal/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-19T12:19:15Z",
        "updated_at": "2023-04-19T12:19:15Z",
        "author_association": "NONE",
        "body": "Any updates on this in 2023???? I got CUDA fp16 slower than fp32, this is really out of my expectation!!\r\n\r\n```\r\n➜ python .\\scripts\\test_ort.py .\\models\\body3d_full_fp16.onnx\r\n0.046171860694885256\r\n🐍(base) 👽 Administrator in  …\\xx on  main [?] took 34s\r\n➜ python .\\scripts\\test_ort.py .\\models\\body3d_full.onnx\r\n0.04198937177658081\r\n```\r\n\r\nthe inputs changed to according fp16 already, but the fp16 speed slower than fp32, am using latest 1.14.1 python version.\r\n \r\nSame bahavior on C++ side, Please take a look!! @microsoft !@faxu",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1514635846/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]