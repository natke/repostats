[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1262806744",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13171#issuecomment-1262806744",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13171",
        "id": 1262806744,
        "node_id": "IC_kwDOCVq1mM5LRObY",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-29T20:55:54Z",
        "updated_at": "2022-09-29T20:55:54Z",
        "author_association": "MEMBER",
        "body": "I don't see anything wrong with the JavaScript code, and the inputs looks good.\r\n\r\nThe error message shows that the input shape is invalid for a 'Expand' node in the model. probably a problem with the converter or the ort core.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1262806744/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1263269242",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13171#issuecomment-1263269242",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13171",
        "id": 1263269242,
        "node_id": "IC_kwDOCVq1mM5LS_V6",
        "user": {
            "login": "loretoparisi",
            "id": 163333,
            "node_id": "MDQ6VXNlcjE2MzMzMw==",
            "avatar_url": "https://avatars.githubusercontent.com/u/163333?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/loretoparisi",
            "html_url": "https://github.com/loretoparisi",
            "followers_url": "https://api.github.com/users/loretoparisi/followers",
            "following_url": "https://api.github.com/users/loretoparisi/following{/other_user}",
            "gists_url": "https://api.github.com/users/loretoparisi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/loretoparisi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/loretoparisi/subscriptions",
            "organizations_url": "https://api.github.com/users/loretoparisi/orgs",
            "repos_url": "https://api.github.com/users/loretoparisi/repos",
            "events_url": "https://api.github.com/users/loretoparisi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/loretoparisi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-30T08:24:57Z",
        "updated_at": "2022-09-30T08:27:58Z",
        "author_association": "NONE",
        "body": "> I don't see anything wrong with the JavaScript code, and the inputs looks good.\r\n> \r\n> The error message shows that the input shape is invalid for a 'Expand' node in the model. probably a problem with the converter or the ort core.\r\n\r\n@fs-eire thank you for the hint, @paulthemagno please let's have a look to the converted model, to exclude is the model itself. In that case it could be related to ort nodejs sdk.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1263269242/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1263641584",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13171#issuecomment-1263641584",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13171",
        "id": 1263641584,
        "node_id": "IC_kwDOCVq1mM5LUaPw",
        "user": {
            "login": "loretoparisi",
            "id": 163333,
            "node_id": "MDQ6VXNlcjE2MzMzMw==",
            "avatar_url": "https://avatars.githubusercontent.com/u/163333?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/loretoparisi",
            "html_url": "https://github.com/loretoparisi",
            "followers_url": "https://api.github.com/users/loretoparisi/followers",
            "following_url": "https://api.github.com/users/loretoparisi/following{/other_user}",
            "gists_url": "https://api.github.com/users/loretoparisi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/loretoparisi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/loretoparisi/subscriptions",
            "organizations_url": "https://api.github.com/users/loretoparisi/orgs",
            "repos_url": "https://api.github.com/users/loretoparisi/repos",
            "events_url": "https://api.github.com/users/loretoparisi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/loretoparisi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-09-30T14:21:53Z",
        "updated_at": "2022-09-30T14:22:37Z",
        "author_association": "NONE",
        "body": "> I don't see anything wrong with the JavaScript code, and the inputs looks good.\r\n> \r\n> The error message shows that the input shape is invalid for a 'Expand' node in the model. probably a problem with the converter or the ort core.\r\n\r\n@fs-eire we digged into the converted model a bit, and it may happen that  there is a difference in the special tokens of the original model tokenizer and converted one (likely the `<pad>` and the `<mask>` are missing in the converted one). In fact we see a difference in the size of the vocab (25000 against 25002 where 2 is accounted for the missing special tokens). If so, could be the `Expand` error related?\r\n\r\nThank you!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1263641584/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1267580880",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13171#issuecomment-1267580880",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13171",
        "id": 1267580880,
        "node_id": "IC_kwDOCVq1mM5Ljb_Q",
        "user": {
            "login": "fs-eire",
            "id": 7679871,
            "node_id": "MDQ6VXNlcjc2Nzk4NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7679871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fs-eire",
            "html_url": "https://github.com/fs-eire",
            "followers_url": "https://api.github.com/users/fs-eire/followers",
            "following_url": "https://api.github.com/users/fs-eire/following{/other_user}",
            "gists_url": "https://api.github.com/users/fs-eire/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fs-eire/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fs-eire/subscriptions",
            "organizations_url": "https://api.github.com/users/fs-eire/orgs",
            "repos_url": "https://api.github.com/users/fs-eire/repos",
            "events_url": "https://api.github.com/users/fs-eire/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fs-eire/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-04T21:08:43Z",
        "updated_at": "2022-10-04T21:08:43Z",
        "author_association": "MEMBER",
        "body": "It's possible.\r\n\r\nThere is one more thing can do to test: use the onnxruntime python package to perform the model inference using the same input tensor data. If it runs successfully, then there must be something wrong with Node.js binding; otherwise, it's the problem of either ORT core or the model itself. Then, need the model file for further investigation.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1267580880/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1268130163",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13171#issuecomment-1268130163",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13171",
        "id": 1268130163,
        "node_id": "IC_kwDOCVq1mM5LliFz",
        "user": {
            "login": "loretoparisi",
            "id": 163333,
            "node_id": "MDQ6VXNlcjE2MzMzMw==",
            "avatar_url": "https://avatars.githubusercontent.com/u/163333?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/loretoparisi",
            "html_url": "https://github.com/loretoparisi",
            "followers_url": "https://api.github.com/users/loretoparisi/followers",
            "following_url": "https://api.github.com/users/loretoparisi/following{/other_user}",
            "gists_url": "https://api.github.com/users/loretoparisi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/loretoparisi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/loretoparisi/subscriptions",
            "organizations_url": "https://api.github.com/users/loretoparisi/orgs",
            "repos_url": "https://api.github.com/users/loretoparisi/repos",
            "events_url": "https://api.github.com/users/loretoparisi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/loretoparisi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-05T08:40:25Z",
        "updated_at": "2022-10-05T08:40:25Z",
        "author_association": "NONE",
        "body": "> It's possible.\r\n> \r\n> There is one more thing can do to test: use the onnxruntime python package to perform the model inference using the same input tensor data. If it runs successfully, then there must be something wrong with Node.js binding; otherwise, it's the problem of either ORT core or the model itself. Then, need the model file for further investigation.\r\n\r\n@fs-eire Thanks a lot @paulthemagno let's try the python counterpart.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1268130163/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1268673090",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/13171#issuecomment-1268673090",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/13171",
        "id": 1268673090,
        "node_id": "IC_kwDOCVq1mM5LnmpC",
        "user": {
            "login": "paulthemagno",
            "id": 38130299,
            "node_id": "MDQ6VXNlcjM4MTMwMjk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/38130299?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/paulthemagno",
            "html_url": "https://github.com/paulthemagno",
            "followers_url": "https://api.github.com/users/paulthemagno/followers",
            "following_url": "https://api.github.com/users/paulthemagno/following{/other_user}",
            "gists_url": "https://api.github.com/users/paulthemagno/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/paulthemagno/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/paulthemagno/subscriptions",
            "organizations_url": "https://api.github.com/users/paulthemagno/orgs",
            "repos_url": "https://api.github.com/users/paulthemagno/repos",
            "events_url": "https://api.github.com/users/paulthemagno/events{/privacy}",
            "received_events_url": "https://api.github.com/users/paulthemagno/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-10-05T16:39:00Z",
        "updated_at": "2022-10-06T08:59:38Z",
        "author_association": "NONE",
        "body": "I tried to convert the pytorch model with this script:\r\n\r\n```python\r\nimport torch\r\nfrom transformers import AutoModelForSequenceClassification\r\n\r\ndef convert_pt_to_onnx(input_pt_path, output_onnx_path):\r\n    model = AutoModelForSequenceClassification.from_pretrained(input_pt_path)\r\n    dummy_input_ids = torch.Tensor([[0, 3293, 83, 142, 98, 19, 425, 3299, 2]]).to(torch.int64)\r\n    torch.onnx.export(model,\r\n                  (dummy_input_ids),\r\n                  output_onnx_path,\r\n                  export_params=True,        # store the trained parameter weights inside the model file\r\n                  opset_version=11,          # the ONNX version to export the model to\r\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\r\n                  input_names=['input'],\r\n                  output_names=['output'],\r\n                  dynamic_axes={'input' :{0 : 'batch_size',\r\n                                          1: 'sentence_length'},\r\n                                'output': {0: 'batch_size'}})\r\n\r\nconvert_pt_to_onnx(input_pt_path = \"MY_PT_MODEL_PATH\", output_onnx_path =\"ONNX_PATH.onnx\")\r\n```\r\n\r\nThen I load the session with this function:\r\n```python\r\nimport torch\r\nfrom os import environ\r\nfrom psutil import cpu_count\r\nfrom transformers import AutoTokenizer\r\nimport onnx\r\nfrom onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers\r\n\r\ndef create_inference_session(model_path: str, use_gpu: bool) -> InferenceSession: \r\n    # Constants from the performance optimization available in onnxruntime\r\n    # It needs to be done before importing onnxruntime\r\n    environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\r\n    environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\r\n    PROVIDERS = [\"CPUExecutionProvider\", \"CUDAExecutionProvider\"]\r\n    \r\n    if torch.cuda.is_available() and use_gpu:\r\n        provider = PROVIDERS[1] #cuda\r\n    else: \r\n        provider = PROVIDERS[0] #cpu\r\n    print(f\"Using provider {provider}\")\r\n  \r\n    assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\r\n\r\n    # Few properties that might have an impact on performances (provided by MS)\r\n    options = SessionOptions()\r\n    options.intra_op_num_threads = 1\r\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\r\n\r\n    # Load the model as a graph and prepare the CPU backend \r\n    session = InferenceSession(model_path, options, providers=[provider])\r\n    session.disable_fallback()\r\n\r\n    return session\r\n\r\nonnx_path = \"ONNX_PATH.onnx\"\r\ntokenizer = AutoTokenizer.from_pretrained('microsoft/Multilingual-MiniLM-L12-H384')\r\nort_session = create_inference_session(model_path = onnx_path, use_gpu = True)\r\n```\r\n\r\nAnd finally I defined the predict function:\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\n\r\ndef predict_batch(ort_session, tokenizer, texts, labels):\r\n    def to_numpy(tensor):\r\n        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\r\n    input_ids = tokenizer.batch_encode_plus(texts, add_special_tokens=True, return_token_type_ids=False, return_attention_mask=False)[\"input_ids\"]\r\n    \r\n    # compute ONNX Runtime output prediction\r\n    outputs = []\r\n    logits = []\r\n    for input_id in input_ids:\r\n        input_id = torch.tensor(input_id)\r\n        input_id = torch.unsqueeze(input_id, dim=0)\r\n        ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_id)}\r\n        ort_out = ort_session.run(None, ort_inputs)\r\n        sigmoid_tensor = torch.sigmoid(torch.tensor(np.array(ort_out)))\r\n        int_label = torch.argmax(sigmoid_tensor)\r\n        logits.append(np.squeeze(sigmoid_tensor.cpu().detach().numpy()))\r\n        outputs.append(labels[int_label])\r\n    return {\"logits\": logits, \"labels\": outputs}\r\n```\r\n\r\nIt's working even if I find different results between the execution of the ONNX model and the Pytorch one (labels are correct but score is slightly different)\r\n\r\n```python\r\n# with ONNX\r\nprint(predict_batch(ort_session = ort_session,\r\n                    tokenizer = tokenizer, \r\n                    texts = [\r\n                        \"Example1\",\r\n                        \"Example2\",\r\n                        \"Example3\",    \r\n                    ], \r\n                    labels = [\"LABEL_0\", \"LABEL_1\"]))\r\n\r\n# with Pytorch model\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"MY_PT_MODEL_PATH\")\r\ntokenizer = AutoTokenizer.from_pretrained('microsoft/Multilingual-MiniLM-L12-H384')\r\nclassifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device = 0)\r\nprint(classifier([\r\n    \"Example1\",\r\n    \"Example2\",\r\n    \"Example3\",    \r\n]))\r\n```\r\n\r\nand the ouptut is similar, but not the same:\r\n\r\n```\r\n//onnx\r\n{'logits': [\r\narray([0.96897584, 0.02641451], dtype=float32),\r\n array([0.97955716, 0.01729073], dtype=float32), \r\narray([0.95629483, 0.0375994 ], dtype=float32)], \r\n'labels': [\r\n'LABEL_0', \r\n'LABEL_0', \r\n'LABEL_0']\r\n}\r\n\r\n//pytorch\r\n[{'label': 'LABEL_0', 'score': 0.9991320967674255},\r\n {'label': 'LABEL_0', 'score': 0.9996329545974731},\r\n {'label': 'LABEL_0', 'score': 0.9982176423072815}]\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1268673090/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]