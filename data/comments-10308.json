[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015733866",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1015733866",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1015733866,
        "node_id": "IC_kwDOCVq1mM48it5q",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-18T19:05:31Z",
        "updated_at": "2022-01-18T19:05:31Z",
        "author_association": "MEMBER",
        "body": "Based on my experience, I suggest to export FP32 ONNX model. Then use some tools to convert it to FP16 (or mixed precision) model. You will need config which part of the model computed in FP16, and other parts computed in FP32 to preserve enough accuracy.\r\n\r\nAn example conversion tool: https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1015733866/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016113957",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1016113957",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1016113957,
        "node_id": "IC_kwDOCVq1mM48kKsl",
        "user": {
            "login": "SiChuanJay",
            "id": 52130681,
            "node_id": "MDQ6VXNlcjUyMTMwNjgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/52130681?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SiChuanJay",
            "html_url": "https://github.com/SiChuanJay",
            "followers_url": "https://api.github.com/users/SiChuanJay/followers",
            "following_url": "https://api.github.com/users/SiChuanJay/following{/other_user}",
            "gists_url": "https://api.github.com/users/SiChuanJay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SiChuanJay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SiChuanJay/subscriptions",
            "organizations_url": "https://api.github.com/users/SiChuanJay/orgs",
            "repos_url": "https://api.github.com/users/SiChuanJay/repos",
            "events_url": "https://api.github.com/users/SiChuanJay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SiChuanJay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-19T06:04:40Z",
        "updated_at": "2022-01-19T06:04:40Z",
        "author_association": "NONE",
        "body": "hello, I tried using onnxconverter_common to transform the model, The code is shown below\r\n\r\nimport onnxmltools\r\nfrom onnxconverter_common.float16 import convert_float_to_float16\r\ninput_onnx_model='automatic_test.onnx'\r\nout_onnx_model='automatic_16.onnx'\r\nonnx_model = onnxmltools.utils.load_model(input_onnx_model)\r\n# Convert tensor float type from your input ONNX model to tensor float16\r\nonnx_model = convert_float_to_float16(onnx_model)\r\n# Save as protobuf\r\nonnxmltools.utils.save_model(onnx_model, out_onnx_model)\r\n\r\nbut got an error\r\n![图片](https://user-images.githubusercontent.com/52130681/150072505-c36f0408-b120-499b-ab4a-861210289aa7.png)\r\n\r\nthe onnx model of float32\r\n![图片](https://user-images.githubusercontent.com/52130681/150073109-e13b60d2-0076-4799-b33a-f99a8eeb06cb.png)\r\nthe onnx model of float16\r\n![图片](https://user-images.githubusercontent.com/52130681/150073230-ca14f1a3-38cd-4729-8ba7-c42258d074a8.png)\r\n\r\nI don't know why some new operations are introduced, how to solve it? Looking forward to your help",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016113957/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016117378",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1016117378",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1016117378,
        "node_id": "IC_kwDOCVq1mM48kLiC",
        "user": {
            "login": "SiChuanJay",
            "id": 52130681,
            "node_id": "MDQ6VXNlcjUyMTMwNjgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/52130681?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SiChuanJay",
            "html_url": "https://github.com/SiChuanJay",
            "followers_url": "https://api.github.com/users/SiChuanJay/followers",
            "following_url": "https://api.github.com/users/SiChuanJay/following{/other_user}",
            "gists_url": "https://api.github.com/users/SiChuanJay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SiChuanJay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SiChuanJay/subscriptions",
            "organizations_url": "https://api.github.com/users/SiChuanJay/orgs",
            "repos_url": "https://api.github.com/users/SiChuanJay/repos",
            "events_url": "https://api.github.com/users/SiChuanJay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SiChuanJay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-19T06:11:47Z",
        "updated_at": "2022-01-19T06:11:47Z",
        "author_association": "NONE",
        "body": "Besides, when I use onnxruntime.quantization to quantify the float32 model.\r\n![图片](https://user-images.githubusercontent.com/52130681/150073873-42fe2699-0d35-4d7b-a687-090784aeff1b.png)\r\nI can get the model right, but the INT8 model is running much slower than float32's model on both CPU and GPU, I wonder why?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016117378/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016448467",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1016448467",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1016448467,
        "node_id": "IC_kwDOCVq1mM48lcXT",
        "user": {
            "login": "Alwin4Zhang",
            "id": 33918902,
            "node_id": "MDQ6VXNlcjMzOTE4OTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/33918902?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Alwin4Zhang",
            "html_url": "https://github.com/Alwin4Zhang",
            "followers_url": "https://api.github.com/users/Alwin4Zhang/followers",
            "following_url": "https://api.github.com/users/Alwin4Zhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/Alwin4Zhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Alwin4Zhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Alwin4Zhang/subscriptions",
            "organizations_url": "https://api.github.com/users/Alwin4Zhang/orgs",
            "repos_url": "https://api.github.com/users/Alwin4Zhang/repos",
            "events_url": "https://api.github.com/users/Alwin4Zhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Alwin4Zhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-19T13:07:29Z",
        "updated_at": "2022-01-19T13:08:54Z",
        "author_association": "NONE",
        "body": "I also get some errors when I use benchmark_t5 to see the performance between cpu/gpu io-binding by fp16，but fp32 is normal.Some errors occur like below:\r\n\r\nException\r\nTraceback (most recent call last):\r\n  File \"/home/studio-lab-user/onnxruntime/onnxruntime/python/tools/transformers/t5/benchmark_t5.py\", line 254, in main\r\n    ort_outputs, ort_latency = T5DecoderHelper.onnxruntime_inference(decoder_session, decoder_inputs, args.test_times)\r\n  File \"/home/studio-lab-user/onnxruntime/onnxruntime/python/tools/transformers/t5/t5_helper.py\", line 552, in onnxruntime_inference\r\n    ort_outputs = ort_session.run(None, ort_inputs)\r\n  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 188, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))\r\nException\r\nTraceback (most recent call last):\r\n  File \"/home/studio-lab-user/onnxruntime/onnxruntime/python/tools/transformers/t5/benchmark_t5.py\", line 254, in main\r\n    ort_outputs, ort_latency = T5DecoderHelper.onnxruntime_inference(decoder_session, decoder_inputs, args.test_times)\r\n  File \"/home/studio-lab-user/onnxruntime/onnxruntime/python/tools/transformers/t5/t5_helper.py\", line 552, in onnxruntime_inference\r\n    ort_outputs = ort_session.run(None, ort_inputs)\r\n  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 188, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))\r\n\r\nWhen I change the code of t5_helper.py,\"pytorch_inference\" ,convert the inputs to float16，but got some other errors...",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016448467/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016896234",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1016896234",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1016896234,
        "node_id": "IC_kwDOCVq1mM48nJrq",
        "user": {
            "login": "Ki6an",
            "id": 63173962,
            "node_id": "MDQ6VXNlcjYzMTczOTYy",
            "avatar_url": "https://avatars.githubusercontent.com/u/63173962?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Ki6an",
            "html_url": "https://github.com/Ki6an",
            "followers_url": "https://api.github.com/users/Ki6an/followers",
            "following_url": "https://api.github.com/users/Ki6an/following{/other_user}",
            "gists_url": "https://api.github.com/users/Ki6an/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Ki6an/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Ki6an/subscriptions",
            "organizations_url": "https://api.github.com/users/Ki6an/orgs",
            "repos_url": "https://api.github.com/users/Ki6an/repos",
            "events_url": "https://api.github.com/users/Ki6an/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Ki6an/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-19T21:46:38Z",
        "updated_at": "2022-01-19T21:46:38Z",
        "author_association": "NONE",
        "body": "@zhangAlwin if you want to quantize t5 models to int8 please refer to [fastT5 ](https://github.com/Ki6an/fastT5)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016896234/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016898367",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1016898367",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1016898367,
        "node_id": "IC_kwDOCVq1mM48nKM_",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-19T21:49:45Z",
        "updated_at": "2022-01-19T21:49:45Z",
        "author_association": "MEMBER",
        "body": "@SiChuanJay, it is expected that there are extra \"Cast\" nodes. The process of converting model from FP32 to FP16: add some Cast nodes to let some nodes to compute in FP16 instead of FP32. \r\n\r\nconvert_float_to_float16 has parameters that you can specify whether input/output or some node shall be kept in FP32.\r\n\r\nINT8 quantization does not have GPU implementation so current quantization is for CPU only. I believe that there are some options you can configure which part of the model to be quantized. You can tune options (like only quantize some operators) to see whether performance can be improved.\r\n\r\n@zhangAlwin, the cause might be convert_float_to_float16 has been updated after the T5 benchmark. Since the script complains input type not matched, you can either change the model to let the input to be in FP16 using convert_float_to_float16, or change your input data type in your script. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1016898367/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017007603",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1017007603",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1017007603,
        "node_id": "IC_kwDOCVq1mM48nk3z",
        "user": {
            "login": "Alwin4Zhang",
            "id": 33918902,
            "node_id": "MDQ6VXNlcjMzOTE4OTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/33918902?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Alwin4Zhang",
            "html_url": "https://github.com/Alwin4Zhang",
            "followers_url": "https://api.github.com/users/Alwin4Zhang/followers",
            "following_url": "https://api.github.com/users/Alwin4Zhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/Alwin4Zhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Alwin4Zhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Alwin4Zhang/subscriptions",
            "organizations_url": "https://api.github.com/users/Alwin4Zhang/orgs",
            "repos_url": "https://api.github.com/users/Alwin4Zhang/repos",
            "events_url": "https://api.github.com/users/Alwin4Zhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Alwin4Zhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-20T00:52:52Z",
        "updated_at": "2022-01-20T00:54:19Z",
        "author_association": "NONE",
        "body": "> @zhangAlwin if you want to quantize t5 models to int8 please refer to [fastT5 ](https://github.com/Ki6an/fastT5)\r\n\r\n@Ki6an  I come from fastt5's git issues(this one -> https://github.com/Ki6an/fastT5/issues/34)。First,INT8 quantization does not have GPU implementation so current quantization is for CPU only. Second,I'm not sure whether fastt5's fp16 precision can use GPU accelerate.In conclusion,fastt5 is a good job to accelerate t5.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017007603/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017016819",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1017016819",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1017016819,
        "node_id": "IC_kwDOCVq1mM48nnHz",
        "user": {
            "login": "Ki6an",
            "id": 63173962,
            "node_id": "MDQ6VXNlcjYzMTczOTYy",
            "avatar_url": "https://avatars.githubusercontent.com/u/63173962?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Ki6an",
            "html_url": "https://github.com/Ki6an",
            "followers_url": "https://api.github.com/users/Ki6an/followers",
            "following_url": "https://api.github.com/users/Ki6an/following{/other_user}",
            "gists_url": "https://api.github.com/users/Ki6an/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Ki6an/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Ki6an/subscriptions",
            "organizations_url": "https://api.github.com/users/Ki6an/orgs",
            "repos_url": "https://api.github.com/users/Ki6an/repos",
            "events_url": "https://api.github.com/users/Ki6an/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Ki6an/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-20T01:07:01Z",
        "updated_at": "2022-01-20T01:07:01Z",
        "author_association": "NONE",
        "body": "@zhangAlwin  fastt5 supports only CPU inference with quantization, currently working on implementing GPU support with Cuda and tensorrt. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017016819/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017040671",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1017040671",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1017040671,
        "node_id": "IC_kwDOCVq1mM48ns8f",
        "user": {
            "login": "Alwin4Zhang",
            "id": 33918902,
            "node_id": "MDQ6VXNlcjMzOTE4OTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/33918902?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Alwin4Zhang",
            "html_url": "https://github.com/Alwin4Zhang",
            "followers_url": "https://api.github.com/users/Alwin4Zhang/followers",
            "following_url": "https://api.github.com/users/Alwin4Zhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/Alwin4Zhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Alwin4Zhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Alwin4Zhang/subscriptions",
            "organizations_url": "https://api.github.com/users/Alwin4Zhang/orgs",
            "repos_url": "https://api.github.com/users/Alwin4Zhang/repos",
            "events_url": "https://api.github.com/users/Alwin4Zhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Alwin4Zhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-20T01:55:35Z",
        "updated_at": "2022-01-20T05:38:24Z",
        "author_association": "NONE",
        "body": "> @zhangAlwin fastt5 supports only CPU inference with quantization, currently working on implementing GPU support with Cuda and tensorrt.\r\n\r\n@Ki6an Come on man.I'm confused that how much can be imporved that the performance of seq2seq(or encoder-decoder) model using tensorflow serving/torchserve/onnx serving with gpu.For example,so far,in my test,batch size 32,max_seq_length 200, don't use beam search, the fasted speed of using t5-small to generate summarization is about 150ms. I'm not sure whether fp16 precision can accelerate it.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017040671/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017080925",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1017080925",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1017080925,
        "node_id": "IC_kwDOCVq1mM48n2xd",
        "user": {
            "login": "Alwin4Zhang",
            "id": 33918902,
            "node_id": "MDQ6VXNlcjMzOTE4OTAy",
            "avatar_url": "https://avatars.githubusercontent.com/u/33918902?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Alwin4Zhang",
            "html_url": "https://github.com/Alwin4Zhang",
            "followers_url": "https://api.github.com/users/Alwin4Zhang/followers",
            "following_url": "https://api.github.com/users/Alwin4Zhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/Alwin4Zhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Alwin4Zhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Alwin4Zhang/subscriptions",
            "organizations_url": "https://api.github.com/users/Alwin4Zhang/orgs",
            "repos_url": "https://api.github.com/users/Alwin4Zhang/repos",
            "events_url": "https://api.github.com/users/Alwin4Zhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Alwin4Zhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-20T03:20:40Z",
        "updated_at": "2022-01-20T03:20:40Z",
        "author_association": "NONE",
        "body": "> @SiChuanJay, it is expected that there are extra \"Cast\" nodes. The process of converting model from FP32 to FP16: add some Cast nodes to let some nodes to compute in FP16 instead of FP32.\r\n> \r\n> convert_float_to_float16 has parameters that you can specify whether input/output or some node shall be kept in FP32.\r\n> \r\n> INT8 quantization does not have GPU implementation so current quantization is for CPU only. I believe that there are some options you can configure which part of the model to be quantized. You can tune options (like only quantize some operators) to see whether performance can be improved.\r\n> \r\n> @zhangAlwin, the cause might be convert_float_to_float16 has been updated after the T5 benchmark. Since the script complains input type not matched, you can either change the model to let the input to be in FP16 using convert_float_to_float16, or change your input data type in your script.\r\n\r\n@tianleiwu  When i change the inputs to float16,got some errors like \"RuntimeError: expected scalar type Half but found Float\"，dame it。I hope that you can fix it if you have time.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1017080925/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1023733125",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1023733125",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1023733125,
        "node_id": "IC_kwDOCVq1mM49BO2F",
        "user": {
            "login": "garymm",
            "id": 421339,
            "node_id": "MDQ6VXNlcjQyMTMzOQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/421339?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/garymm",
            "html_url": "https://github.com/garymm",
            "followers_url": "https://api.github.com/users/garymm/followers",
            "following_url": "https://api.github.com/users/garymm/following{/other_user}",
            "gists_url": "https://api.github.com/users/garymm/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/garymm/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/garymm/subscriptions",
            "organizations_url": "https://api.github.com/users/garymm/orgs",
            "repos_url": "https://api.github.com/users/garymm/repos",
            "events_url": "https://api.github.com/users/garymm/events{/privacy}",
            "received_events_url": "https://api.github.com/users/garymm/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-01-27T23:23:59Z",
        "updated_at": "2022-01-27T23:23:59Z",
        "author_association": "CONTRIBUTOR",
        "body": "@SiChuanJay can you confirm that the PyTorch model outputs float16 tensors (before conversion to ONNX)?\r\n\r\nIf so then this may be a bug in torch.onnx.export.\r\nPlease:\r\n1. Try with the latest nightly PyTorch.\r\n2. If you still have a problem, file a bug with reproduction instructions at github.com/pytorch/pytorch",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1023733125/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1054818918",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1054818918",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1054818918,
        "node_id": "IC_kwDOCVq1mM4-30Jm",
        "user": {
            "login": "thiagocrepaldi",
            "id": 5469809,
            "node_id": "MDQ6VXNlcjU0Njk4MDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5469809?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thiagocrepaldi",
            "html_url": "https://github.com/thiagocrepaldi",
            "followers_url": "https://api.github.com/users/thiagocrepaldi/followers",
            "following_url": "https://api.github.com/users/thiagocrepaldi/following{/other_user}",
            "gists_url": "https://api.github.com/users/thiagocrepaldi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thiagocrepaldi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thiagocrepaldi/subscriptions",
            "organizations_url": "https://api.github.com/users/thiagocrepaldi/orgs",
            "repos_url": "https://api.github.com/users/thiagocrepaldi/repos",
            "events_url": "https://api.github.com/users/thiagocrepaldi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thiagocrepaldi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-02-28T23:39:13Z",
        "updated_at": "2022-02-28T23:39:13Z",
        "author_association": "MEMBER",
        "body": "> > @SiChuanJay, it is expected that there are extra \"Cast\" nodes. The process of converting model from FP32 to FP16: add some Cast nodes to let some nodes to compute in FP16 instead of FP32.\r\n> > convert_float_to_float16 has parameters that you can specify whether input/output or some node shall be kept in FP32.\r\n> > INT8 quantization does not have GPU implementation so current quantization is for CPU only. I believe that there are some options you can configure which part of the model to be quantized. You can tune options (like only quantize some operators) to see whether performance can be improved.\r\n> > @zhangAlwin, the cause might be convert_float_to_float16 has been updated after the T5 benchmark. Since the script complains input type not matched, you can either change the model to let the input to be in FP16 using convert_float_to_float16, or change your input data type in your script.\r\n> \r\n> @tianleiwu Tianlei Wu FTE When i change the inputs to float16,got some errors like \"RuntimeError: expected scalar type Half but found Float\"，dame it。I hope that you can fix it if you have time.\r\n\r\n@Alwin4Zhang ideally you should use float16 input to get an float16 output from pytorch, which would then be exported to onnx using float16 too. However, many pytorch operators do not allow mixing fp16 and fp32 in the input and will raise something like `\"RuntimeError: expected scalar type Half but found Float\"` as you described. It might be a bug or by design\r\n\r\nFrom one of your screenshots, it seems the operator in question is the `Resize-11`. Is it the case? If it is, the issue could be for for opset 11 ([and 13 actually](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Resize-11)), `scales` is always a `tensor(float)` whereas `X` input can be any of `tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double), tensor(string), tensor(bool), tensor(complex64), tensor(complex128)`. Multiplying `X.half()` by `scales.float()` would fail with type mismatch.\r\n\r\nTo test this theory, first, try to change this\r\n\r\n```\r\n    def export(self):\r\n        rec = (torch.zeros([1, 1, 1, 1]).to(self.args.device, self.precision),) * 4\r\n        src = torch.randn(1, 3, 1080, 1920).to(self.args.device, self.precision)\r\n        downsample_ratio = torch.tensor([0.25]).to(self.args.device, dtype=self.precision)\r\n        \r\n        dynamic_spatial = {0: 'batch_size', 2: 'height', 3: 'width'}\r\n        dynamic_everything = {0: 'batch_size', 1: 'channels', 2: 'height', 3: 'width'}\r\n        \r\n        torch.onnx.export(\r\n            self.model,\r\n            (src, *rec, downsample_ratio),\r\n            self.args.output,\r\n            export_params=True,\r\n            opset_version=self.args.opset,\r\n            do_constant_folding=True,\r\n            input_names=['src', 'r1i', 'r2i', 'r3i', 'r4i', 'downsample_ratio'],\r\n            output_names=['fgr', 'pha', 'r1o', 'r2o', 'r3o', 'r4o'],\r\n            dynamic_axes={\r\n                'src': dynamic_spatial,\r\n                'fgr': dynamic_spatial,\r\n                'pha': dynamic_spatial,\r\n                'r1i': dynamic_everything,\r\n                'r2i': dynamic_everything,\r\n                'r3i': dynamic_everything,\r\n                'r4i': dynamic_everything,\r\n                'r1o': dynamic_spatial,\r\n                'r2o': dynamic_spatial,\r\n                'r3o': dynamic_spatial,\r\n                'r4o': dynamic_spatial,\r\n            })\r\n```\r\n\r\n\r\nto\r\n\r\n```\r\n    def export(self):\r\n        rec = (torch.zeros([1, 1, 1, 1]).to(self.args.device, self.precision),) * 4\r\n        src = torch.randn(1, 3, 1080, 1920).to(self.args.device, self.precision)\r\n        downsample_ratio = torch.tensor([0.25]).to(self.args.device, dtype=self.precision)\r\n        \r\n        dynamic_spatial = {0: 'batch_size', 2: 'height', 3: 'width'}\r\n        dynamic_everything = {0: 'batch_size', 1: 'channels', 2: 'height', 3: 'width'}\r\n        \r\n        with torch.autocast(device_type=str(self.args.device), enabled=True):\r\n                torch.onnx.export(\r\n                    self.model,\r\n                    (src, *rec, downsample_ratio),\r\n                    self.args.output,\r\n                    export_params=True,\r\n                    opset_version=self.args.opset,\r\n                    do_constant_folding=True,\r\n                    input_names=['src', 'r1i', 'r2i', 'r3i', 'r4i', 'downsample_ratio'],\r\n                    output_names=['fgr', 'pha', 'r1o', 'r2o', 'r3o', 'r4o'],\r\n                    dynamic_axes={\r\n                        'src': dynamic_spatial,\r\n                        'fgr': dynamic_spatial,\r\n                        'pha': dynamic_spatial,\r\n                        'r1i': dynamic_everything,\r\n                        'r2i': dynamic_everything,\r\n                        'r3i': dynamic_everything,\r\n                        'r4i': dynamic_everything,\r\n                        'r1o': dynamic_spatial,\r\n                        'r2o': dynamic_spatial,\r\n                        'r3o': dynamic_spatial,\r\n                        'r4o': dynamic_spatial,\r\n                    })\r\n```\r\n\r\nWith this change (wrapping `export` call on a `torch.autocast()` context manager), the mismatch between fp16 and fp32 should go away. However, the output can be either fp16 or fp32 depending on the actual implementation of the operator in question. Some ops create the output node with `dtype` matching the first input dtype. So passing `input.half()` as input would create a `float16` output. Other ops just hard code the type.\r\n\r\nTry the code above and provides an give us an actual model that reproduces the issue. `from model.model import MattingNetwork` does not help us reproduce the issue and provide an accurate answer.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1054818918/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1317208275",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10308#issuecomment-1317208275",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10308",
        "id": 1317208275,
        "node_id": "IC_kwDOCVq1mM5OgwDT",
        "user": {
            "login": "thiagocrepaldi",
            "id": 5469809,
            "node_id": "MDQ6VXNlcjU0Njk4MDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5469809?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thiagocrepaldi",
            "html_url": "https://github.com/thiagocrepaldi",
            "followers_url": "https://api.github.com/users/thiagocrepaldi/followers",
            "following_url": "https://api.github.com/users/thiagocrepaldi/following{/other_user}",
            "gists_url": "https://api.github.com/users/thiagocrepaldi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thiagocrepaldi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thiagocrepaldi/subscriptions",
            "organizations_url": "https://api.github.com/users/thiagocrepaldi/orgs",
            "repos_url": "https://api.github.com/users/thiagocrepaldi/repos",
            "events_url": "https://api.github.com/users/thiagocrepaldi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thiagocrepaldi/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-11-16T15:33:17Z",
        "updated_at": "2022-11-16T15:33:17Z",
        "author_association": "MEMBER",
        "body": "I am assuming 1) my suggestion above worked or 2) the issue is not relevant anymore because there was no response from author after 9 months. Thus, will close it. Feel free to reopen it if that is relevant with instructions on where t get the `model` which is imported by the script",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1317208275/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]