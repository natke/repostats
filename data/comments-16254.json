[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579381298",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16254#issuecomment-1579381298",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16254",
        "id": 1579381298,
        "node_id": "IC_kwDOCVq1mM5eI3Iy",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-06T20:11:45Z",
        "updated_at": "2023-06-06T20:11:45Z",
        "author_association": "MEMBER",
        "body": "80GB GPU might be able to export 13B model. \r\n\r\nFor 30B model, it could be a little tricky to export to ONNX since it will need multiple GPUs, and ONNX Runtime does not have good support for multiple GPU inference right now. Could you try export using cpu device instead of cuda device?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579381298/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579388692",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16254#issuecomment-1579388692",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16254",
        "id": 1579388692,
        "node_id": "IC_kwDOCVq1mM5eI48U",
        "user": {
            "login": "aleph65",
            "id": 124602977,
            "node_id": "U_kgDOB21KYQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/124602977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aleph65",
            "html_url": "https://github.com/aleph65",
            "followers_url": "https://api.github.com/users/aleph65/followers",
            "following_url": "https://api.github.com/users/aleph65/following{/other_user}",
            "gists_url": "https://api.github.com/users/aleph65/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aleph65/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aleph65/subscriptions",
            "organizations_url": "https://api.github.com/users/aleph65/orgs",
            "repos_url": "https://api.github.com/users/aleph65/repos",
            "events_url": "https://api.github.com/users/aleph65/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aleph65/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-06T20:18:26Z",
        "updated_at": "2023-06-06T20:18:26Z",
        "author_association": "NONE",
        "body": "@tianleiwu  Thanks so much for the reply.\r\n\r\nI will try cpu export instead\r\n\r\n1) I heard that for O4 optimization, cuda / GPU is required?\r\n2) You said \"onnx runtime does not have good support for multip gpu inference\" does that mean that onnx models would not be able to run on multiple GPUs at once?\r\n3) What is the best way to optimize a large model like 30b or 40b to get about 100-200x performance gains? Or at least the 17x inference which appears possible from ONNX, what needs to happen to get that?\r\n\r\nOnce again thank you, and I'll update when I'm done trying the CPU conversion. I will run this:\r\n\r\n```\r\noptimum-cli export onnx --task text-generation-with-past --model WizardLM-30B-Uncensored WizardLM-30B-ONNX/\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579388692/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579611054",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16254#issuecomment-1579611054",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16254",
        "id": 1579611054,
        "node_id": "IC_kwDOCVq1mM5eJvOu",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-06T23:50:37Z",
        "updated_at": "2023-06-06T23:50:37Z",
        "author_association": "MEMBER",
        "body": "> 1. I heard that for O4 optimization, cuda / GPU is required?\r\n> 2. You said \"onnx runtime does not have good support for multip gpu inference\" does that mean that onnx models would not be able to run on multiple GPUs at once?\r\n> 3. What is the best way to optimize a large model like 30b or 40b to get about 100-200x performance gains? Or at least the 17x inference which appears possible from ONNX, what needs to happen to get that?\r\n\r\nONNX export can be done in CPU. For inference, it is recommended to use GPU for latency consideration.\r\n\r\nFor FP16, you will need partition 30B model to multiple sub-models to fit GPU memory. Right now, you can create one inference session per sub-model, and write code to call sub-models one by one. It is not a simple task considering you might also need integrate beam search optimization etc.\r\n\r\nFor best performance, you might need quantize model to 4 bits to fit in one GPU. However, ONNX Runtime does not support 4 bit natively so you might need implement some custom operators for this.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579611054/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579617195",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16254#issuecomment-1579617195",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16254",
        "id": 1579617195,
        "node_id": "IC_kwDOCVq1mM5eJwur",
        "user": {
            "login": "aleph65",
            "id": 124602977,
            "node_id": "U_kgDOB21KYQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/124602977?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aleph65",
            "html_url": "https://github.com/aleph65",
            "followers_url": "https://api.github.com/users/aleph65/followers",
            "following_url": "https://api.github.com/users/aleph65/following{/other_user}",
            "gists_url": "https://api.github.com/users/aleph65/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aleph65/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aleph65/subscriptions",
            "organizations_url": "https://api.github.com/users/aleph65/orgs",
            "repos_url": "https://api.github.com/users/aleph65/repos",
            "events_url": "https://api.github.com/users/aleph65/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aleph65/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-06T23:58:32Z",
        "updated_at": "2023-06-07T01:14:46Z",
        "author_association": "NONE",
        "body": "Hey @tianleiwu, some good news! I was able to run the export on my CPU, and run inference!\r\n\r\nHowever, the inference is very slow... and the model folder is massive, 360 GB\r\n\r\n<img width=\"1049\" alt=\"image\" src=\"https://github.com/microsoft/onnxruntime/assets/124602977/021d969a-15d5-4548-af22-59fc72baf37a\">\r\n\r\n\r\nI exported in O1 I believe which is the default, just ran a basic command (took about 80 minutes)\r\n\r\n1. For a Llama based 30B model, what is the suggested quantization (O3?) and configuration? Is --arm64 best? I will re-export and quantize if so\r\n<img width=\"875\" alt=\"image\" src=\"https://github.com/microsoft/onnxruntime/assets/124602977/1744f5a1-77d2-4279-b941-a2cd307feab8\">\r\n\r\n2. How do I get rid of these warnings that \"13/self_attn/Constant_ _43 _output_0'. It is not used by any node and should be removed from the model.\" etc.? Will additional quantization take care of it?\r\n\r\nKind regards and thanks\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1579617195/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1585303988",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/16254#issuecomment-1585303988",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/16254",
        "id": 1585303988,
        "node_id": "IC_kwDOCVq1mM5efdG0",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-06-10T00:24:03Z",
        "updated_at": "2023-06-10T00:24:03Z",
        "author_association": "MEMBER",
        "body": "For #1, you may ask in [optimum](https://github.com/huggingface/optimum).\r\nFor #2, you can ignore the warning since ONNX Runtime will remove those internally. If you want to remove unused constant in model, you can try python script like the following (only works when the model does not have sub-graph):\r\n```\r\nimport onnx\r\nfrom onnxruntime.transformers.onnx_model import OnnxModel\r\nm= OnnxModel(onnx.load(\"input.onnx\"))\r\nm.update_graph()\r\nonnx.save(m.model, \"output.onnx\", save_as_external_data=True)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1585303988/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]