[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/999148551",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10100#issuecomment-999148551",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10100",
        "id": 999148551,
        "node_id": "IC_kwDOCVq1mM47jcwH",
        "user": {
            "login": "chausner",
            "id": 15180557,
            "node_id": "MDQ6VXNlcjE1MTgwNTU3",
            "avatar_url": "https://avatars.githubusercontent.com/u/15180557?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chausner",
            "html_url": "https://github.com/chausner",
            "followers_url": "https://api.github.com/users/chausner/followers",
            "following_url": "https://api.github.com/users/chausner/following{/other_user}",
            "gists_url": "https://api.github.com/users/chausner/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chausner/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chausner/subscriptions",
            "organizations_url": "https://api.github.com/users/chausner/orgs",
            "repos_url": "https://api.github.com/users/chausner/repos",
            "events_url": "https://api.github.com/users/chausner/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chausner/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-21T22:55:43Z",
        "updated_at": "2021-12-21T22:55:43Z",
        "author_association": "CONTRIBUTOR",
        "body": "I think quantization only changes the data types internally for weights and activations but keeps the original data type for input and output nodes. This means, even after quantizing your model to int8 you are supposed to provide float32 data as input if that was the original data type.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/999148551/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/999157518",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10100#issuecomment-999157518",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10100",
        "id": 999157518,
        "node_id": "IC_kwDOCVq1mM47je8O",
        "user": {
            "login": "Hamptonjc",
            "id": 41594631,
            "node_id": "MDQ6VXNlcjQxNTk0NjMx",
            "avatar_url": "https://avatars.githubusercontent.com/u/41594631?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Hamptonjc",
            "html_url": "https://github.com/Hamptonjc",
            "followers_url": "https://api.github.com/users/Hamptonjc/followers",
            "following_url": "https://api.github.com/users/Hamptonjc/following{/other_user}",
            "gists_url": "https://api.github.com/users/Hamptonjc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Hamptonjc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Hamptonjc/subscriptions",
            "organizations_url": "https://api.github.com/users/Hamptonjc/orgs",
            "repos_url": "https://api.github.com/users/Hamptonjc/repos",
            "events_url": "https://api.github.com/users/Hamptonjc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Hamptonjc/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-21T23:16:25Z",
        "updated_at": "2021-12-21T23:16:25Z",
        "author_association": "NONE",
        "body": "> I think quantization only changes the data types internally for weights and activations but keeps the original data type for input and output nodes. This means, even after quantizing your model to int8 you are supposed to provide float32 data as input if that was the original data type.\r\n\r\nCorrect. Is there a way I can change the input data type to int8? To use int8 data in PyTorch I must quantize the model using their quantization. However to my knowledge PyTorch quantized models can't be exported to onnx.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/999157518/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/999201460",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10100#issuecomment-999201460",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10100",
        "id": 999201460,
        "node_id": "IC_kwDOCVq1mM47jpq0",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2021-12-22T01:03:36Z",
        "updated_at": "2021-12-22T01:03:36Z",
        "author_association": "MEMBER",
        "body": "Usually the input would be fp32 otherwise you need zero point and scale information. \r\n\r\nIn the pytorch quantization [documentation](https://pytorch.org/docs/stable/quantization.html) the input to the quantized model is still fp32.\r\n\r\n```python\r\n# Convert the observed model to a quantized model. This does several things:\r\n# quantizes the weights, computes and stores the scale and bias value to be\r\n# used with each activation tensor, and replaces key operators with quantized\r\n# implementations.\r\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\r\n\r\n# run the model, relevant calculations will happen in int8\r\nres = model_int8(input_fp32)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/999201460/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]