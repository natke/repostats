[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1067112968",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10770#issuecomment-1067112968",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10770",
        "id": 1067112968,
        "node_id": "IC_kwDOCVq1mM4_mtoI",
        "user": {
            "login": "stevenlix",
            "id": 38092805,
            "node_id": "MDQ6VXNlcjM4MDkyODA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/38092805?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stevenlix",
            "html_url": "https://github.com/stevenlix",
            "followers_url": "https://api.github.com/users/stevenlix/followers",
            "following_url": "https://api.github.com/users/stevenlix/following{/other_user}",
            "gists_url": "https://api.github.com/users/stevenlix/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stevenlix/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stevenlix/subscriptions",
            "organizations_url": "https://api.github.com/users/stevenlix/orgs",
            "repos_url": "https://api.github.com/users/stevenlix/repos",
            "events_url": "https://api.github.com/users/stevenlix/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stevenlix/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-14T17:47:35Z",
        "updated_at": "2022-03-14T17:47:35Z",
        "author_association": "MEMBER",
        "body": "Could you check if the GPU memory is running out during inference?\r\nIf not, you may try to increase TRT workspace by setting environment variable ORT_TENSORRT_MAX_WORKSPACE_SIZE to bigger value, i.e. 4294967296 (4GB).",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1067112968/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1072040911",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/10770#issuecomment-1072040911",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/10770",
        "id": 1072040911,
        "node_id": "IC_kwDOCVq1mM4_5gvP",
        "user": {
            "login": "pycoco",
            "id": 11053628,
            "node_id": "MDQ6VXNlcjExMDUzNjI4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11053628?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pycoco",
            "html_url": "https://github.com/pycoco",
            "followers_url": "https://api.github.com/users/pycoco/followers",
            "following_url": "https://api.github.com/users/pycoco/following{/other_user}",
            "gists_url": "https://api.github.com/users/pycoco/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pycoco/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pycoco/subscriptions",
            "organizations_url": "https://api.github.com/users/pycoco/orgs",
            "repos_url": "https://api.github.com/users/pycoco/repos",
            "events_url": "https://api.github.com/users/pycoco/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pycoco/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-03-18T05:16:00Z",
        "updated_at": "2022-03-18T05:16:00Z",
        "author_association": "NONE",
        "body": "> Could you check if the GPU memory is running out during inference? If not, you may try to increase TRT workspace by setting environment variable ORT_TENSORRT_MAX_WORKSPACE_SIZE to bigger value, i.e. 4294967296 (4GB).\r\n\r\nThank u very much, this issue has fixed.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1072040911/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]