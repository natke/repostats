[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142334708",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11688#issuecomment-1142334708",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11688",
        "id": 1142334708,
        "node_id": "IC_kwDOCVq1mM5EFqT0",
        "user": {
            "login": "haydn-jones",
            "id": 9584084,
            "node_id": "MDQ6VXNlcjk1ODQwODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9584084?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/haydn-jones",
            "html_url": "https://github.com/haydn-jones",
            "followers_url": "https://api.github.com/users/haydn-jones/followers",
            "following_url": "https://api.github.com/users/haydn-jones/following{/other_user}",
            "gists_url": "https://api.github.com/users/haydn-jones/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/haydn-jones/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/haydn-jones/subscriptions",
            "organizations_url": "https://api.github.com/users/haydn-jones/orgs",
            "repos_url": "https://api.github.com/users/haydn-jones/repos",
            "events_url": "https://api.github.com/users/haydn-jones/events{/privacy}",
            "received_events_url": "https://api.github.com/users/haydn-jones/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T16:11:46Z",
        "updated_at": "2022-05-31T16:11:46Z",
        "author_association": "NONE",
        "body": "I've tried this arch in ONNX and I've noticed that ONNX is faster than PyTorch on CPU, have you tried enabling all optimizations on your inference session?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142334708/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142354107",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11688#issuecomment-1142354107",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11688",
        "id": 1142354107,
        "node_id": "IC_kwDOCVq1mM5EFvC7",
        "user": {
            "login": "rogachevai",
            "id": 56756391,
            "node_id": "MDQ6VXNlcjU2NzU2Mzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56756391?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rogachevai",
            "html_url": "https://github.com/rogachevai",
            "followers_url": "https://api.github.com/users/rogachevai/followers",
            "following_url": "https://api.github.com/users/rogachevai/following{/other_user}",
            "gists_url": "https://api.github.com/users/rogachevai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rogachevai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rogachevai/subscriptions",
            "organizations_url": "https://api.github.com/users/rogachevai/orgs",
            "repos_url": "https://api.github.com/users/rogachevai/repos",
            "events_url": "https://api.github.com/users/rogachevai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rogachevai/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T16:29:47Z",
        "updated_at": "2022-05-31T16:29:47Z",
        "author_association": "NONE",
        "body": "> have you tried enabling all optimizations on your inference session?\r\n\r\nDo you mean passing `rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED` as an option to InferenceSession? In this case it doesn't seem to help.\r\n\r\n\r\n> ONNX is faster than PyTorch on CPU\r\n\r\ntbh, I didn't compare CPU versions. Is it faster in GPU case as well?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142354107/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142393001",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11688#issuecomment-1142393001",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11688",
        "id": 1142393001,
        "node_id": "IC_kwDOCVq1mM5EF4ip",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T17:04:43Z",
        "updated_at": "2022-05-31T17:04:43Z",
        "author_association": "MEMBER",
        "body": ">Maybe I should somehow update my code in order to avoid moving input to cpu in order to provide sess with numpy input?\r\n\r\nYes. Please try IO Binding. See sections of [IO Binding](https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding) and [Data on device](https://onnxruntime.ai/docs/api/python/api_summary.html). That could avoid copying input data from CPU to GPU.\r\n\r\nThe following code copied in_mat from GPU to CPU, and CPU to GPU (in ORT), and output from GPU to CPU (in ORT). It is known that data copy is slow.\r\n```\r\nin_mat = torch.tensor(in_mat).cuda()\r\nout_mat = sess.run([output_name], {input_name: in_mat.cpu().numpy()})[0]\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142393001/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142404585",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11688#issuecomment-1142404585",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11688",
        "id": 1142404585,
        "node_id": "IC_kwDOCVq1mM5EF7Xp",
        "user": {
            "login": "rogachevai",
            "id": 56756391,
            "node_id": "MDQ6VXNlcjU2NzU2Mzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56756391?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rogachevai",
            "html_url": "https://github.com/rogachevai",
            "followers_url": "https://api.github.com/users/rogachevai/followers",
            "following_url": "https://api.github.com/users/rogachevai/following{/other_user}",
            "gists_url": "https://api.github.com/users/rogachevai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rogachevai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rogachevai/subscriptions",
            "organizations_url": "https://api.github.com/users/rogachevai/orgs",
            "repos_url": "https://api.github.com/users/rogachevai/repos",
            "events_url": "https://api.github.com/users/rogachevai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rogachevai/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T17:17:27Z",
        "updated_at": "2022-05-31T17:17:27Z",
        "author_association": "NONE",
        "body": "Well, I replaced this part as follows:\r\n```\r\nio_binding.bind_cpu_input(input_name, in_mat)\r\nio_binding.bind_output(output_name)\r\nsess.run_with_iobinding(io_binding)\r\nout_mat = io_binding.copy_outputs_to_cpu()[0][0]\r\n```\r\nHowever it doesn't improve the situation. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142404585/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142429948",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11688#issuecomment-1142429948",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11688",
        "id": 1142429948,
        "node_id": "IC_kwDOCVq1mM5EGBj8",
        "user": {
            "login": "rogachevai",
            "id": 56756391,
            "node_id": "MDQ6VXNlcjU2NzU2Mzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56756391?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rogachevai",
            "html_url": "https://github.com/rogachevai",
            "followers_url": "https://api.github.com/users/rogachevai/followers",
            "following_url": "https://api.github.com/users/rogachevai/following{/other_user}",
            "gists_url": "https://api.github.com/users/rogachevai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rogachevai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rogachevai/subscriptions",
            "organizations_url": "https://api.github.com/users/rogachevai/orgs",
            "repos_url": "https://api.github.com/users/rogachevai/repos",
            "events_url": "https://api.github.com/users/rogachevai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rogachevai/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-05-31T17:45:32Z",
        "updated_at": "2022-05-31T17:45:32Z",
        "author_association": "NONE",
        "body": "> Please try IO Binding.\r\n\r\nI even tried to rewrite the code and test it in terms of time, however it still doesn't work for me.\r\n\r\n```\r\nsess = rt.InferenceSession('realesrgan-x4-default-cuda-one-batch.onnx', providers=['CUDAExecutionProvider'])\r\nio_binding = sess.io_binding()\r\ninput_name = sess.get_inputs()[0].name\r\noutput_name = sess.get_outputs()[0].name\r\n\r\ndef model(input_img, sess = sess, io_binding = io_binding, input_name = input_name, output_name = output_name):\r\n    \r\n    input_img = input_img.contiguous()\r\n\r\n    io_binding.bind_input(\r\n                name=input_name,\r\n                device_type='cuda',\r\n                device_id=0,\r\n                element_type=np.float32,\r\n                shape=tuple(input_img.shape),\r\n                buffer_ptr=input_img.data_ptr(),\r\n                )\r\n    out_mat = torch.empty((1, 3,input_img.shape[2] * 4, input_img.shape[3] * 4 ), dtype=torch.float32, device='cuda:0').contiguous()\r\n\r\n    io_binding.bind_output(\r\n        name=output_name,\r\n        device_type='cuda',\r\n        device_id=0,\r\n        element_type=np.float32,\r\n        shape=tuple(out_mat.shape),\r\n        buffer_ptr=out_mat.data_ptr(),\r\n    )\r\n    start_time = time.time()\r\n    sess.run_with_iobinding(io_binding)\r\n    elapsed_time = time.time() - start_time\r\n    print(elapsed_time)\r\n    \r\n    return out_mat\r\n\r\nfor _ in range(10):\r\n    h = np.random.randint(256, 512)\r\n    w = np.random.randint(256, 512)\r\n    temp_i = torch.rand(1,3,h,w).cuda()\r\n    print(h,w)\r\n    temp_r = model(temp_i) \r\n\r\n410 485\r\n11.116924047470093\r\n324 500\r\n8.948107242584229\r\n319 373\r\n6.92399787902832\r\n353 487\r\n9.696300029754639\r\n271 490\r\n7.827823877334595\r\n418 395\r\n9.345899820327759\r\n358 470\r\n9.476377725601196\r\n347 279\r\n7.469595193862915\r\n297 487\r\n8.367061376571655\r\n504 454\r\n12.829151630401611\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1142429948/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1143409747",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/11688#issuecomment-1143409747",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/11688",
        "id": 1143409747,
        "node_id": "IC_kwDOCVq1mM5EJwxT",
        "user": {
            "login": "rogachevai",
            "id": 56756391,
            "node_id": "MDQ6VXNlcjU2NzU2Mzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56756391?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rogachevai",
            "html_url": "https://github.com/rogachevai",
            "followers_url": "https://api.github.com/users/rogachevai/followers",
            "following_url": "https://api.github.com/users/rogachevai/following{/other_user}",
            "gists_url": "https://api.github.com/users/rogachevai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rogachevai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rogachevai/subscriptions",
            "organizations_url": "https://api.github.com/users/rogachevai/orgs",
            "repos_url": "https://api.github.com/users/rogachevai/repos",
            "events_url": "https://api.github.com/users/rogachevai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rogachevai/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2022-06-01T10:16:28Z",
        "updated_at": "2022-06-01T10:16:28Z",
        "author_association": "NONE",
        "body": "So, I tried to change the idea of warmup. \r\nIn case I firstly iterate over images in a directory, generate random input of the same shape as current real image has, pass it to the model and only after all the image-like inputs were passed, I apply the model to the real images, the inference speed increases and becomes nearly equal to the Pytorch one.\r\n\r\n```\r\nsess = rt.InferenceSession('realesrgan-x4-default-cuda-one-batch.onnx', providers=['CUDAExecutionProvider'])\r\n\r\nio_binding = sess.io_binding()\r\ninput_name = sess.get_inputs()[0].name\r\noutput_name = sess.get_outputs()[0].name\r\n\r\ndef inference(p, sess, io_binding, input_name, output_name, warmup = False):\r\n    \r\n    in_image = cv2.imread(p, cv2.IMREAD_UNCHANGED)\r\n    in_mat = cv2.cvtColor(in_image, cv2.COLOR_BGR2RGB)\r\n    in_mat = np.transpose(in_mat, (2, 1, 0))[np.newaxis]\r\n    in_mat = in_mat.astype(np.float32)\r\n    in_mat = in_mat/255\r\n    \r\n    if warmup:\r\n        in_mat = np.random.rand(*in_mat.shape)  \r\n\r\n    \r\n    X_ortvalue = rt.OrtValue.ortvalue_from_numpy(in_mat, 'cuda', 0)\r\n    io_binding.bind_input(name=input_name, device_type=X_ortvalue.device_name(),\r\n                          device_id=0, element_type=np.float32, shape=X_ortvalue.shape(), buffer_ptr=X_ortvalue.data_ptr())\r\n    io_binding.bind_output(output_name, 'cuda')\r\n    sess.run_with_iobinding(io_binding)\r\n    out_mat = io_binding.copy_outputs_to_cpu()[0][0]\r\n    \r\n    return out_mat\r\n\r\nstart_time = time.time()\r\nfor p in (img_list[:50]):\r\n    _ = inference(p, sess, io_binding, input_name, output_name, True)    \r\nelapsed_time = time.time() - start_time\r\nprint('Warmup : ', elapsed_time)\r\n\r\nstart_time = time.time()\r\nfor p in (img_list[:50]):\r\n    _ = inference(p, sess, io_binding, input_name, output_name, False)    \r\nelapsed_time = time.time() - start_time\r\nprint('Inference : ', elapsed_time)\r\n\r\nWarmup :  201.89860439300537\r\nInference :  19.002933740615845\r\n```\r\n\r\nThus it looks like in case of different input and output shapes it is needed to firstly warmup using all the possible shapes. However it is still not clear why it doesn't improve the speed significantly compared to Torch model.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1143409747/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]