[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467029841",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15021#issuecomment-1467029841",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15021",
        "id": 1467029841,
        "node_id": "IC_kwDOCVq1mM5XcRlR",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-13T22:04:28Z",
        "updated_at": "2023-03-13T22:09:41Z",
        "author_association": "MEMBER",
        "body": "LayerNormalization was added to the ONNX spec in opset 17. Prior to that it was an internal ORT operator.\r\n\r\nThe opset 17 CUDA implementation of LayerNormalization supports 16-bit floats\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/538d64891ac8e43c1faf7846635c3a1bf7b6b6c5/onnxruntime/core/providers/cuda/nn/layer_norm.cc#L12-L22\r\n\r\nAssuming you want to run on GPU, can you instead update the model to opset 17 and use the official [LayerNormalization](https://github.com/onnx/onnx/blob/main/docs/Operators.md#LayerNormalization) operator? \r\n\r\nYou can update the opset using a tool in the ORT python package. \r\n\r\n`python -m onnxruntime.tools.update_onnx_opset --opset 17 model.onnx model.opset17.onnx`\r\n\r\nNote that the CPU EP implementation for LayerNormalization does not support fp16. You need to enable the CUDA EP when creating the InferenceSession to hit the GPU implementation.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467029841/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467709199",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15021#issuecomment-1467709199",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15021",
        "id": 1467709199,
        "node_id": "IC_kwDOCVq1mM5Xe3cP",
        "user": {
            "login": "nzmora-nvidia",
            "id": 96238833,
            "node_id": "U_kgDOBbx88Q",
            "avatar_url": "https://avatars.githubusercontent.com/u/96238833?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nzmora-nvidia",
            "html_url": "https://github.com/nzmora-nvidia",
            "followers_url": "https://api.github.com/users/nzmora-nvidia/followers",
            "following_url": "https://api.github.com/users/nzmora-nvidia/following{/other_user}",
            "gists_url": "https://api.github.com/users/nzmora-nvidia/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nzmora-nvidia/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nzmora-nvidia/subscriptions",
            "organizations_url": "https://api.github.com/users/nzmora-nvidia/orgs",
            "repos_url": "https://api.github.com/users/nzmora-nvidia/repos",
            "events_url": "https://api.github.com/users/nzmora-nvidia/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nzmora-nvidia/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-14T09:20:24Z",
        "updated_at": "2023-03-14T09:20:24Z",
        "author_association": "NONE",
        "body": "@skottmckay thank you. I'm familiar with LN in opset 17 but I cannot use it because of dependencies in the SW stack I'm using.\r\n\r\nIn any case the problem occurs also when I use opset 17 - ORT seems to be trying to do pattern matching with an LN kernel and something is not to its liking so it aborts the build. \r\nBut there is nothing wrong with the subgraph I provided to it, as can be seen when you look at the second ONNX file  (`almost.layernorm_fp16.onnx`) which makes a tiny change in the graph, thus breaking the template matching (I'm guessing here), and therefore ORT manages the subgraph.\r\n\r\n**ORT should be able to build the subgraph `layernorm_fp16.onnx`. If he subgraph is illegal, please let me know what is wrong with it. If it is correct, please look into this issue.**",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1467709199/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1469181436",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15021#issuecomment-1469181436",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15021",
        "id": 1469181436,
        "node_id": "IC_kwDOCVq1mM5Xke38",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-15T02:02:21Z",
        "updated_at": "2023-03-15T02:02:21Z",
        "author_association": "MEMBER",
        "body": "It has converted the nodes to a LayerNormalization node as per the error message so the pattern matching appears to be working:\r\n\r\n> Op with name (LayerNormalization) and type (LayerNormalization) Version mismatch. node_version: 1 kernel start version: 17 \r\nkernel_end_version: 2147483647\r\n\r\nI believe the general expectation is that a GPU based EP (e.g. CUDA EP) is enabled if you're using fp16 data. The problem is the optimizer isn't skipping the insertion of the internal LayerNormalization operator that uses fp16 data when only the CPU EP is enabled, which results in a node that we have no kernel that can execute it.\r\n\r\nIf the CUDA or ROCm EP was enabled there is an fp16 kernel for the internal LayerNormalization operator\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/538d64891ac8e43c1faf7846635c3a1bf7b6b6c5/onnxruntime/core/providers/cuda/nn/layer_norm.cc#L105-L118\r\n\r\nIs there a reason for trying to run an fp16 model on CPU? ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1469181436/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1471888571",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15021#issuecomment-1471888571",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15021",
        "id": 1471888571,
        "node_id": "IC_kwDOCVq1mM5Xuzy7",
        "user": {
            "login": "nzmora-nvidia",
            "id": 96238833,
            "node_id": "U_kgDOBbx88Q",
            "avatar_url": "https://avatars.githubusercontent.com/u/96238833?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nzmora-nvidia",
            "html_url": "https://github.com/nzmora-nvidia",
            "followers_url": "https://api.github.com/users/nzmora-nvidia/followers",
            "following_url": "https://api.github.com/users/nzmora-nvidia/following{/other_user}",
            "gists_url": "https://api.github.com/users/nzmora-nvidia/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nzmora-nvidia/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nzmora-nvidia/subscriptions",
            "organizations_url": "https://api.github.com/users/nzmora-nvidia/orgs",
            "repos_url": "https://api.github.com/users/nzmora-nvidia/repos",
            "events_url": "https://api.github.com/users/nzmora-nvidia/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nzmora-nvidia/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-16T12:43:24Z",
        "updated_at": "2023-03-16T12:43:24Z",
        "author_association": "NONE",
        "body": "Thanks @skottmckay for the quick fix!\r\n\r\nFor anyone interested, Scott gave me this tip to work-around the issue I describe above: \r\n>  You can disable an optimizer if running from python. Add this to the InferenceSession constructor arguments:\r\n       `, disabled_optimizers=[\"LayerNormFusion\"]`\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1471888571/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]