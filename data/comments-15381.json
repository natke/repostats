[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498060534",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1498060534",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1498060534,
        "node_id": "IC_kwDOCVq1mM5ZSpb2",
        "user": {
            "login": "fdwr",
            "id": 1809166,
            "node_id": "MDQ6VXNlcjE4MDkxNjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1809166?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fdwr",
            "html_url": "https://github.com/fdwr",
            "followers_url": "https://api.github.com/users/fdwr/followers",
            "following_url": "https://api.github.com/users/fdwr/following{/other_user}",
            "gists_url": "https://api.github.com/users/fdwr/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fdwr/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fdwr/subscriptions",
            "organizations_url": "https://api.github.com/users/fdwr/orgs",
            "repos_url": "https://api.github.com/users/fdwr/repos",
            "events_url": "https://api.github.com/users/fdwr/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fdwr/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-05T20:06:49Z",
        "updated_at": "2023-04-05T20:46:34Z",
        "author_association": "MEMBER",
        "body": ">In other words I just want to copy the outputs from the GPU to the CPU to read them.\r\n\r\nI'm not that familiar with C# (mainly a C++ dev), but can you just use the output returned from `Run` and read the first tensor rather than rather than using the `IOBinding`? e.g.\r\n\r\n```\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.IO;\r\nusing System.Linq;\r\nusing Microsoft.ML.OnnxRuntime;\r\nusing Microsoft.ML.OnnxRuntime.Tensors;\r\n\r\nnamespace OrtTestApp\r\n{\r\n    class Program\r\n    {\r\n        static void Main(string[] args)\r\n        {\r\n            Console.WriteLine(\"Begin...\");\r\n\r\n            SessionOptions sessionOptions = new SessionOptions();\r\n            sessionOptions.LogSeverityLevel = OrtLoggingLevel.ORT_LOGGING_LEVEL_INFO;\r\n            sessionOptions.GraphOptimizationLevel = GraphOptimizationLevel.ORT_ENABLE_ALL; //  ORT_DISABLE_ALL;\r\n\r\n            sessionOptions.AppendExecutionProvider_DML(0);\r\n            sessionOptions.EnableMemoryPattern = false;\r\n            InferenceSession onnxSession = new InferenceSession(\"squeezenet1.1-7.onnx\", sessionOptions);\r\n\r\n            var tensor = new DenseTensor<float>(new int[] {1,3,224,224});\r\n            var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor<float>(\"data\", tensor) };\r\n            var output = onnxSession.Run(inputs);\r\n\r\n            Console.WriteLine(\"Done\");\r\n            Console.WriteLine(output);\r\n            var values = (output.ToList().First().Value as IEnumerable<float>).ToArray();\r\n            foreach (var item in values)\r\n            {\r\n                Console.Write(item.ToString() + \",\");\r\n            }\r\n        }\r\n    }\r\n}\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498060534/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498075555",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1498075555",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1498075555,
        "node_id": "IC_kwDOCVq1mM5ZStGj",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-05T20:16:03Z",
        "updated_at": "2023-04-05T21:58:15Z",
        "author_association": "NONE",
        "body": "Yes that is what I had before, but I am trying to speed up my code. [And the documentation says](https://fs-eire.github.io/onnxruntime/docs/performance/tune-performance.html), if you are running on GPU, it is best to map the inputs and outputs to GPU memory using IOBinding. Especially if I'm running the session recursively. \r\n\r\nBut then at the end I want to get the outputs back from the GPU.\r\n\r\n[ In C++ there ](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/test/shared_lib/test_inference.cc)is a method on OrtValue called `GetTensorData<float>()` which gets the data from the ortvalue. But in C# it doesn't seem this has been implemented? In C# the [OrtValue](https://onnxruntime.ai/docs/api/csharp/api/Microsoft.ML.OnnxRuntime.OrtValue.html) class is quite barron.\r\n\r\nAm I just thinking about this all the wrong way? ðŸ¤” I am running a GPT2 type model and other people have reported getting speed ups using IOBinding. \r\n\r\nAlso in the [documentation](https://fs-eire.github.io/onnxruntime/docs/performance/tune-performance.html) it says \"for c# please refer to https://github.com/microsoft/onnxruntime/blob/master/csharp/test/Microsoft.ML.OnnxRuntime.Tests/OrtIoBindingAllocationTest.cs\"  but this file doesn't exit. So the documentation needs updating at the very least.\r\n\r\nThey probably mean this file:\r\nhttps://github.com/microsoft/onnxruntime/blob/main/csharp/test/Microsoft.ML.OnnxRuntime.Tests.Common/OrtIoBindingAllocationTest.cs\r\n\r\n\r\nIn fact what I would _really_ like to do is take my output tensor of shape (1,500,50257) and only copy the last 50257 values to the CPU. The rest of it is junk and can stay on the GPU. Having to copy all 500x501257 values is causing a big CPU bottleneck. (As this is a language model so I'm only interested in the last token). ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498075555/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498248803",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1498248803",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1498248803,
        "node_id": "IC_kwDOCVq1mM5ZTXZj",
        "user": {
            "login": "baijumeswani",
            "id": 12852605,
            "node_id": "MDQ6VXNlcjEyODUyNjA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/12852605?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/baijumeswani",
            "html_url": "https://github.com/baijumeswani",
            "followers_url": "https://api.github.com/users/baijumeswani/followers",
            "following_url": "https://api.github.com/users/baijumeswani/following{/other_user}",
            "gists_url": "https://api.github.com/users/baijumeswani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/baijumeswani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/baijumeswani/subscriptions",
            "organizations_url": "https://api.github.com/users/baijumeswani/orgs",
            "repos_url": "https://api.github.com/users/baijumeswani/repos",
            "events_url": "https://api.github.com/users/baijumeswani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/baijumeswani/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-05T22:44:36Z",
        "updated_at": "2023-04-05T22:44:36Z",
        "author_association": "MEMBER",
        "body": "I guess you could use the `AsTensor<float>()` method. [Here](https://github.com/microsoft/onnxruntime/blob/a7d321e9dc8eded098d8267162d84f24b14a7dad/csharp/test/Microsoft.ML.OnnxRuntime.Tests.Common/InferenceTest.cs#L1707-L1715) is an example usage from our C# tests.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498248803/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498256163",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1498256163",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1498256163,
        "node_id": "IC_kwDOCVq1mM5ZTZMj",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-05T22:56:06Z",
        "updated_at": "2023-04-06T02:13:28Z",
        "author_association": "NONE",
        "body": "I solved my bottleneck a different way... or so I thought.\r\n\r\nI just modified the LLM by adding output=output[:,-1,:], so now the output is truncated on the GPU. And it only has to send the last token to the CPU. Instead of sending the last 1000 ðŸ˜‚ \r\n\r\nThis has sped things up considerably. ðŸš€ \r\n\r\n(Yes, I did ask chat gpt how to do that!)\r\n\r\nWell this speeds things up for float16 onnx because then I have to do less float16->float32 conversions on the CPU. (which I haven't found a fast library for yet). I think I was stupidly converting all 100x50257 values from float16->float32 which is pointless. ðŸ™„ When taking that into account, the moving of data from GPU to CPU isn't really much of a bottleneck at all. My other bottleneck is just that the dyanmic_quantized onnx is just slower than the float16 onnx.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1498256163/reactions",
            "total_count": 4,
            "+1": 2,
            "-1": 0,
            "laugh": 2,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499578761",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1499578761",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1499578761,
        "node_id": "IC_kwDOCVq1mM5ZYcGJ",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-06T20:23:00Z",
        "updated_at": "2023-04-06T20:23:00Z",
        "author_association": "NONE",
        "body": "I don't think this binding will help speed up my inferences... but maybe it will help someone else.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499578761/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1505706639",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1505706639",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1505706639,
        "node_id": "IC_kwDOCVq1mM5Zv0KP",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-12T18:01:38Z",
        "updated_at": "2023-04-12T18:10:42Z",
        "author_association": "MEMBER",
        "body": "Binding does not help speeding up the inference. It helps to avoid copies when the data is already on device, so one does not have to copy it to a separate piece of memory.\r\n\r\nAnother scenario is a circular feeding of data. This when someone wants to feed output to input, they would quickly rebound output buffer to input buffer and vice versa.\r\n\r\nIf you just want to run inference on GPU and then get results on CPU, onnxruntime would copy both inputs and outputs as appropriate, no need for any special actions.\r\n\r\nJust use either `NamedOnnxValue` or `FixedBufferOnnxValue` for inputs. For tensors, those would pin the managed memory and feed it into onnxruntime, no copy made here.\r\n\r\nIf GPU copy is required, onnxruntime would do it.\r\n\r\nThe same thing on return, unless otherwise specified, the output would be copied back to CPU and the native memory would be exposed via `DisposableNamedOnnxValue` instances.\r\n\r\nNote, that most of the Onnxruntime C# classes are disposable, because they hold on to native memory or instances of native classes. This includes options, sessions and just about anything else.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1505706639/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1505903257",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1505903257",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1505903257,
        "node_id": "IC_kwDOCVq1mM5ZwkKZ",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-12T20:37:57Z",
        "updated_at": "2023-04-12T20:38:47Z",
        "author_association": "NONE",
        "body": "Thanks yes, I realised that for a language model because I need the tokens on the CPU at each step, binding wouldn't give me any speed up. But for other cases I would like to keep the result on the GPU.\r\n\r\nBTW are there any samples/documentation for taking the output of one model (bound to the GPU) and feeding it to the input of a second model? In c#?\r\n\r\n(Although to be honest, I think the CPU to GPU transfer is not really such a bottleneck. It is quite fast compared to doing the inference.)",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1505903257/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1507347367",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1507347367",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1507347367,
        "node_id": "IC_kwDOCVq1mM5Z2Eun",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-13T17:27:09Z",
        "updated_at": "2023-04-13T17:27:09Z",
        "author_association": "MEMBER",
        "body": "> BTW are there any samples/documentation for taking the output of one model (bound to the GPU) and feeding it to the input of a second model? In c#?\r\n> \r\n\r\nI do not think there is a documentation. The original use case dictated that both inputs and outputs shapes are known. Thus, both input and output memory are pre-allocated and the user have native (GPU in this case) pointers in their possession. So using output pointers, one can create a binding for another model and feed it as input.\r\n\r\nYour use case, when the output shape is not known (you only specify the device), and you cannot pre-allocate is not covered at the moment, but it should be.\r\n\r\nOur C# API needs work and classes with the name starting with `Ort*` represent an effort to make it at least as usable as other APIs. So far, there was no good reason to expose `OrtValue` although in our other APis it the first class citizen.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1507347367/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1548212368",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15381#issuecomment-1548212368",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15381",
        "id": 1548212368,
        "node_id": "IC_kwDOCVq1mM5cR9iQ",
        "user": {
            "login": "yuslepukhin",
            "id": 11303988,
            "node_id": "MDQ6VXNlcjExMzAzOTg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/11303988?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yuslepukhin",
            "html_url": "https://github.com/yuslepukhin",
            "followers_url": "https://api.github.com/users/yuslepukhin/followers",
            "following_url": "https://api.github.com/users/yuslepukhin/following{/other_user}",
            "gists_url": "https://api.github.com/users/yuslepukhin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yuslepukhin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yuslepukhin/subscriptions",
            "organizations_url": "https://api.github.com/users/yuslepukhin/orgs",
            "repos_url": "https://api.github.com/users/yuslepukhin/repos",
            "events_url": "https://api.github.com/users/yuslepukhin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yuslepukhin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-05-15T16:54:54Z",
        "updated_at": "2023-05-15T16:54:54Z",
        "author_association": "MEMBER",
        "body": "I will be adding some new API, that is `bound` to address the issue.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1548212368/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]