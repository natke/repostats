[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1378098154",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14043#issuecomment-1378098154",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14043",
        "id": 1378098154,
        "node_id": "IC_kwDOCVq1mM5SJBvq",
        "user": {
            "login": "adrianlizarraga",
            "id": 19691973,
            "node_id": "MDQ6VXNlcjE5NjkxOTcz",
            "avatar_url": "https://avatars.githubusercontent.com/u/19691973?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adrianlizarraga",
            "html_url": "https://github.com/adrianlizarraga",
            "followers_url": "https://api.github.com/users/adrianlizarraga/followers",
            "following_url": "https://api.github.com/users/adrianlizarraga/following{/other_user}",
            "gists_url": "https://api.github.com/users/adrianlizarraga/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adrianlizarraga/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adrianlizarraga/subscriptions",
            "organizations_url": "https://api.github.com/users/adrianlizarraga/orgs",
            "repos_url": "https://api.github.com/users/adrianlizarraga/repos",
            "events_url": "https://api.github.com/users/adrianlizarraga/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adrianlizarraga/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-11T00:59:42Z",
        "updated_at": "2023-01-11T00:59:42Z",
        "author_association": "MEMBER",
        "body": "Hi @krishung5,\r\n\r\nI ran the following NVIDIA sample programs with valgrind and I can see similar memory leaks whenever `nvinfer1::createInferBuilder` is called.\r\n\r\n- [TensorRT/samples/sampleINT8API at main · NVIDIA/TensorRT (github.com)](https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleINT8API)\r\n- [TensorRT/samples/sampleOnnxMNIST at main · NVIDIA/TensorRT (github.com)](https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleOnnxMNIST)\r\n\r\nHere's a snippet of the valgrind output when running sampleINT8API:\r\n\r\n`/usr/bin/valgrind --leak-check=full --show-leak-kinds=definite --max-threads=3000 --num-callers=20 --keep-debuginfo=yes --log-file=./valgrind_int8.log ./sample_int8_api --model=../data/resnet50/ResNet50.onnx`\r\n\r\n```shell\r\n==3995== 260 bytes in 1 blocks are definitely lost in loss record 2,537 of 3,724\r\n==3995==    at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==3995==    by 0x452471CA: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.1)\r\n==3995==    by 0x22D414DE: __pthread_once_slow (pthread_once.c:116)\r\n==3995==    by 0x45258468: cuGetProcAddress (in /usr/lib/x86_64-linux-gnu/libcuda.so.1)\r\n==3995==    by 0xA624B4F: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0xA62DAA9: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0xA62DC17: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0x22D414DE: __pthread_once_slow (pthread_once.c:116)\r\n==3995==    by 0xA67D5E8: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0xA624546: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0xA658365: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0x59220E4: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0x58BD8C1: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0x58C7149: createInferBuilder_INTERNAL (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3995==    by 0x111D14: nvinfer1::(anonymous namespace)::createInferBuilder(nvinfer1::ILogger&) (NvInfer.h:9677)\r\n==3995==    by 0x1140C7: SampleINT8API::build() (sampleINT8API.cpp:493)\r\n==3995==    by 0x115CD3: main (sampleINT8API.cpp:866)\r\n```\r\nHere's a similar valgrind snippet for sampleOnnxMNIST:\r\n\r\n`/usr/bin/valgrind --leak-check=full --show-leak-kinds=definite --max-threads=3000 --num-callers=20 --keep-debuginfo=yes --log-file=./valgrind.log ./sample_onnx_mnist`\r\n\r\n```shell\r\n==3628== 260 bytes in 1 blocks are definitely lost in loss record 2,545 of 3,745\r\n==3628==    at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==3628==    by 0x452471CA: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.1)\r\n==3628==    by 0x22D414DE: __pthread_once_slow (pthread_once.c:116)\r\n==3628==    by 0x45258468: cuGetProcAddress (in /usr/lib/x86_64-linux-gnu/libcuda.so.1)\r\n==3628==    by 0xA624B4F: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0xA62DAA9: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0xA62DC17: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0x22D414DE: __pthread_once_slow (pthread_once.c:116)\r\n==3628==    by 0xA67D5E8: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0xA624546: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0xA658365: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0x59220E4: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0x58BD8C1: ??? (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0x58C7149: createInferBuilder_INTERNAL (in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8.5.1)\r\n==3628==    by 0x111CD4: nvinfer1::(anonymous namespace)::createInferBuilder(nvinfer1::ILogger&) (NvInfer.h:9677)\r\n==3628==    by 0x111E2C: SampleOnnxMNIST::build() (sampleOnnxMNIST.cpp:105)\r\n==3628==    by 0x113545: main (sampleOnnxMNIST.cpp:394)\r\n```\r\n\r\nThis leads me to believe that the leak is not specific to onnxruntime, but to libnvinfer. Note that the above programs were run on Ubuntu 20.04 with TensorRT version 8.5.1 and CUDA 11.6.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1378098154/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1379224486",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14043#issuecomment-1379224486",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14043",
        "id": 1379224486,
        "node_id": "IC_kwDOCVq1mM5SNUum",
        "user": {
            "login": "jywu-msft",
            "id": 43355415,
            "node_id": "MDQ6VXNlcjQzMzU1NDE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/43355415?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jywu-msft",
            "html_url": "https://github.com/jywu-msft",
            "followers_url": "https://api.github.com/users/jywu-msft/followers",
            "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}",
            "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions",
            "organizations_url": "https://api.github.com/users/jywu-msft/orgs",
            "repos_url": "https://api.github.com/users/jywu-msft/repos",
            "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jywu-msft/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-11T17:22:22Z",
        "updated_at": "2023-01-11T17:22:22Z",
        "author_association": "MEMBER",
        "body": "to elaborate on this, that local IBuilder object is managed by a unique_ptr \r\nhttps://github.com/microsoft/onnxruntime/blob/main/onnxruntime/core/providers/tensorrt/tensorrt_execution_provider.cc#L925\r\nso destroy() should get called when it goes out of scope. as @adrianlizarraga points out, it's the same pattern used in the tensorrt code samples which valgrind also reports the same memory leak.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1379224486/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1381065909",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14043#issuecomment-1381065909",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14043",
        "id": 1381065909,
        "node_id": "IC_kwDOCVq1mM5SUWS1",
        "user": {
            "login": "krishung5",
            "id": 43719498,
            "node_id": "MDQ6VXNlcjQzNzE5NDk4",
            "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/krishung5",
            "html_url": "https://github.com/krishung5",
            "followers_url": "https://api.github.com/users/krishung5/followers",
            "following_url": "https://api.github.com/users/krishung5/following{/other_user}",
            "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions",
            "organizations_url": "https://api.github.com/users/krishung5/orgs",
            "repos_url": "https://api.github.com/users/krishung5/repos",
            "events_url": "https://api.github.com/users/krishung5/events{/privacy}",
            "received_events_url": "https://api.github.com/users/krishung5/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-12T22:30:20Z",
        "updated_at": "2023-01-12T22:30:20Z",
        "author_association": "NONE",
        "body": "Thank you @adrianlizarraga and @jywu-msft for looking into it! Two questions I would like to ask:\r\n1. Just wanted to confirm that the ORT version @adrianlizarraga used in the above sample was 1.13.1, right? Besides, may I ask which build version of TRT was used to reproduce the memory leak? I cannot reproduce the memory leak using TRT NGC image with 8.5.1.7.\r\n2. In the above sample we are leaking 260 bytes whereas in my valgrind output 77,852 bytes were reported leaking. Is it possible that they have different root cause? It's ~300 times larger in bytes and in my example we didn't create the `nvinfer1::createInferBuilder` for 300 times.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1381065909/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1381618686",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14043#issuecomment-1381618686",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14043",
        "id": 1381618686,
        "node_id": "IC_kwDOCVq1mM5SWdP-",
        "user": {
            "login": "adrianlizarraga",
            "id": 19691973,
            "node_id": "MDQ6VXNlcjE5NjkxOTcz",
            "avatar_url": "https://avatars.githubusercontent.com/u/19691973?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adrianlizarraga",
            "html_url": "https://github.com/adrianlizarraga",
            "followers_url": "https://api.github.com/users/adrianlizarraga/followers",
            "following_url": "https://api.github.com/users/adrianlizarraga/following{/other_user}",
            "gists_url": "https://api.github.com/users/adrianlizarraga/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adrianlizarraga/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adrianlizarraga/subscriptions",
            "organizations_url": "https://api.github.com/users/adrianlizarraga/orgs",
            "repos_url": "https://api.github.com/users/adrianlizarraga/repos",
            "events_url": "https://api.github.com/users/adrianlizarraga/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adrianlizarraga/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-13T10:22:24Z",
        "updated_at": "2023-01-13T10:22:24Z",
        "author_association": "MEMBER",
        "body": "Hi @krishung5,\r\n\r\n> Just wanted to confirm that the ORT version @adrianlizarraga used in the above sample was 1.13.1, right? Besides, may I ask which build version of TRT was used to reproduce the memory leak? I cannot reproduce the memory leak using TRT NGC image with 8.5.1.7.\r\n\r\nI ran your original example program with both ORT@master and ORT 1.13.1. Valgrind showed memory leaks when calling `nvinfer1::createInferBuilder`.\r\n\r\nTo my knowledge, the above NVIDIA samples do not use ORT. Therefore, a leak in the samples would lead me to believe that the leak is not necessarily coming from ORT. Here is the sample docker file I used to install and run the samples with TensorRT 8.5.1:\r\n\r\n```\r\n# Dockerfile that installs TensorRT 8.5.1 sample programs on Ubuntu 20.04\r\n\r\nFROM nvidia/cuda:11.6.1-cudnn8-devel-ubuntu20.04\r\n\r\nENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/src/tensorrt/bin:${PATH}\r\nENV DEBIAN_FRONTEND=noninteractive\r\n\r\nRUN apt-get update &&\\\r\n    apt-get install -y sudo bash unattended-upgrades wget\r\nRUN unattended-upgrade\r\n\r\n# Install valgrind\r\nRUN apt-get install -y --no-install-recommends valgrind\r\n\r\n# Install TensorRT\r\nRUN v=\"8.5.1-1+cuda11.8\" &&\\\r\n    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub &&\\\r\n    apt-get update &&\\\r\n    sudo apt-get install -y libnvinfer8=${v} libnvonnxparsers8=${v} libnvparsers8=${v} libnvinfer-plugin8=${v} \\\r\n        libnvinfer-dev=${v} libnvonnxparsers-dev=${v} libnvparsers-dev=${v} libnvinfer-plugin-dev=${v} \\\r\n        python3-libnvinfer=${v} libnvinfer-samples=${v}\r\n\r\n# Compile sampleINT8API\r\nRUN cd /usr/src/tensorrt/samples/sampleINT8API && make\r\n\r\n# Compile sampleOnnxMNIST\r\nRUN cd /usr/src/tensorrt/samples/sampleOnnxMNIST && make\r\n\r\nWORKDIR /usr/src/tensorrt/bin\r\n```\r\n\r\nBuild image: `docker build -t nv_samples -f Dockerfile.ubuntu_cuda11_6_tensorrt8_5 .`\r\nRun container interactively: `docker run --gpus all -it nv_samples bash`\r\nRun valgrind on one of the samples:\r\n```shell\r\n$ cd /usr/tensorrt/bin\r\n$ /usr/bin/valgrind --leak-check=full --show-leak-kinds=definite --max-threads=3000 --num-callers=20 --keep-debuginfo=yes --log-file=./valgrind.log ./sample_onnx_mnist_debug\r\n```\r\n\r\n\r\n> In the above sample we are leaking 260 bytes whereas in my valgrind output 77,852 bytes were reported leaking. Is it possible that they have different root cause? It's ~300 times larger in bytes and in my example we didn't create the nvinfer1::createInferBuilder for 300 times.\r\n\r\nOur ORT code calls `nvinfer1::createInferBuilder` in a loop, so the amount leaked by a single call would end up multiplied by the number of iterations/calls. Here's the code in question: https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/core/providers/tensorrt/tensorrt_execution_provider.cc#L925\r\n\r\nAs @jywu-msft mentioned, we are wrapping the returned `nvinfer1::IBuilder*` into a custom unique pointer that calls `nvinfer1::IBuilder::destroy()` on destruction. This is the same strategy used by the NVIDIA samples, which I would expect not to leak, but does according to my tests.\r\n\r\nI should note that `nvinfer1::IBuilder::destroy()` has been deprecated with the expectation to use `delete` going forward. I've tried using `std::unique_ptr<nvinfer1::IBuilder>`, which calls `delete` on the pointer, but I still observe the same memory leaks.\r\n\r\nI will try to run the samples with the TRT NGC image next. Thanks for your help in getting to the bottom of this!",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1381618686/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1385963173",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14043#issuecomment-1385963173",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14043",
        "id": 1385963173,
        "node_id": "IC_kwDOCVq1mM5SnB6l",
        "user": {
            "login": "kevinch-nv",
            "id": 45886021,
            "node_id": "MDQ6VXNlcjQ1ODg2MDIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/45886021?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kevinch-nv",
            "html_url": "https://github.com/kevinch-nv",
            "followers_url": "https://api.github.com/users/kevinch-nv/followers",
            "following_url": "https://api.github.com/users/kevinch-nv/following{/other_user}",
            "gists_url": "https://api.github.com/users/kevinch-nv/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kevinch-nv/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kevinch-nv/subscriptions",
            "organizations_url": "https://api.github.com/users/kevinch-nv/orgs",
            "repos_url": "https://api.github.com/users/kevinch-nv/repos",
            "events_url": "https://api.github.com/users/kevinch-nv/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kevinch-nv/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-17T19:49:15Z",
        "updated_at": "2023-01-17T19:49:15Z",
        "author_association": "CONTRIBUTOR",
        "body": "Hi @adrianlizarraga,\r\n\r\nI'm on the TensorRT team and am looking into this issue. I do not see the memleak in standalone TensorRT (I've followed your above instructions and I'm not seeing any leaks reported by Valgrind, nor am I seeing the leaks running the same test in the NGC container)\r\n\r\nWhat GPU, cuda toolkit, and CUDA driver version are you using?",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1385963173/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1396251398",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14043#issuecomment-1396251398",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14043",
        "id": 1396251398,
        "node_id": "IC_kwDOCVq1mM5TORsG",
        "user": {
            "login": "adrianlizarraga",
            "id": 19691973,
            "node_id": "MDQ6VXNlcjE5NjkxOTcz",
            "avatar_url": "https://avatars.githubusercontent.com/u/19691973?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adrianlizarraga",
            "html_url": "https://github.com/adrianlizarraga",
            "followers_url": "https://api.github.com/users/adrianlizarraga/followers",
            "following_url": "https://api.github.com/users/adrianlizarraga/following{/other_user}",
            "gists_url": "https://api.github.com/users/adrianlizarraga/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adrianlizarraga/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adrianlizarraga/subscriptions",
            "organizations_url": "https://api.github.com/users/adrianlizarraga/orgs",
            "repos_url": "https://api.github.com/users/adrianlizarraga/repos",
            "events_url": "https://api.github.com/users/adrianlizarraga/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adrianlizarraga/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-01-19T00:00:38Z",
        "updated_at": "2023-01-19T00:00:38Z",
        "author_association": "MEMBER",
        "body": "Hi @kevinch-nv,\r\n\r\nGPU: quadro p2200\r\nGPU Driver: 517.37\r\nSupported CUDA version: 11.7\r\nCUDA toolkit: 11.6\r\n\r\n```shell\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Tue_Mar__8_18:18:20_PST_2022\r\nCuda compilation tools, release 11.6, V11.6.124\r\nBuild cuda_11.6.r11.6/compiler.31057947_0\r\n```\r\n\r\n```shell\r\n$ nvidia-smi\r\nWed Jan 18 23:53:25 2023\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 515.75       Driver Version: 517.37       CUDA Version: 11.7     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P2200        On   | 00000000:65:00.0  On |                  N/A |\r\n| 44%   30C    P8     7W /  75W |   1197MiB /  5120MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1396251398/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]