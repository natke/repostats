[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1473306312",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15080#issuecomment-1473306312",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15080",
        "id": 1473306312,
        "node_id": "IC_kwDOCVq1mM5X0N7I",
        "user": {
            "login": "skottmckay",
            "id": 979079,
            "node_id": "MDQ6VXNlcjk3OTA3OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/979079?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/skottmckay",
            "html_url": "https://github.com/skottmckay",
            "followers_url": "https://api.github.com/users/skottmckay/followers",
            "following_url": "https://api.github.com/users/skottmckay/following{/other_user}",
            "gists_url": "https://api.github.com/users/skottmckay/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/skottmckay/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/skottmckay/subscriptions",
            "organizations_url": "https://api.github.com/users/skottmckay/orgs",
            "repos_url": "https://api.github.com/users/skottmckay/repos",
            "events_url": "https://api.github.com/users/skottmckay/events{/privacy}",
            "received_events_url": "https://api.github.com/users/skottmckay/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-17T07:43:17Z",
        "updated_at": "2023-03-17T07:43:17Z",
        "author_association": "MEMBER",
        "body": "It's not easily overcome with the current implementation.\r\n\r\nORT loads the model from a protobuf format .onnx file. It's up to protobuf to decide what arenas etc. are used to load that into memory. \r\n\r\n* The bulk of the memory is the initializer/weights data which may be stored in a packed format. \r\n  * i.e. we can't treat it as a simple array of the data type so it needs to be in a different format to actually run the model. \r\n  * a copy is required to do that, which leads to 2x the memory usage for each initializer\r\n* We try and free individual things as we go along where possible, but that may not make a difference given protobuf controls what memory arenas etc. are used and a chunk of memory it owns cannot be freed until everything using that chunk is no longer in use.\r\n\r\nTheoretically if you saved the model in some different format that when loaded you could directly use it for the initializer data you could avoid this copy, however a _lot_ of the code is written to operate on the protobuf types, such as all the optimizers, and rewriting that would be a significant undertaking.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1473306312/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 1,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1473558255",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15080#issuecomment-1473558255",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15080",
        "id": 1473558255,
        "node_id": "IC_kwDOCVq1mM5X1Lbv",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-17T09:52:36Z",
        "updated_at": "2023-03-17T10:26:31Z",
        "author_association": "NONE",
        "body": " > Theoretically if you saved the model in some different format that when loaded you could directly use it for the initializer data you could avoid this copy, however a _lot_ of the code is written to operate on the protobuf types, such as all the optimizers, and rewriting **that would be a significant undertaking**.\r\n\r\nMicrosoft is a ~~multi-billion~~ trillion dollar company with very intelligent people. Just hire someone to do it.\r\n\r\nWhat I'm hearing is \"we agree Onnxruntime is inefficient and we don't want to fix it.\" Is that really the Microsoft attitude? ðŸ˜\r\n\r\nI jest, of course. I'm sure you could all fix it in a weekend of hacking if you put your minds to it. \r\n\r\nUsing double the amount of RAM needed is the very definition of inefficient code.\r\n\r\nWhy not just load the Onnx file layer by layer and convert it into the desired type layer by layer? Then delete from RAM as we go along. Doesn't seem impossible, just seems like different departments need to work together.\r\n\r\n\r\nOn the other hand, it seems like you're saying ONNX itself is flawed because it uses protobuf format. So maybe we shouldn't be using Onnxruntime at all?\r\n\r\nAs I say, pytorch manages to not uses double the amount of memory or VRAM so it's definitely possible.\r\n\r\n\"The bulk of the memory is the initializer/weights data which may be stored in a packed format\". OK, well once a layer has been converted into the format needed to run the model, delete the packet. What's so hard about that? Do it layer by layer to avoid the spike.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1473558255/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474220948",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15080#issuecomment-1474220948",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15080",
        "id": 1474220948,
        "node_id": "IC_kwDOCVq1mM5X3tOU",
        "user": {
            "login": "faxu",
            "id": 20780999,
            "node_id": "MDQ6VXNlcjIwNzgwOTk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/20780999?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/faxu",
            "html_url": "https://github.com/faxu",
            "followers_url": "https://api.github.com/users/faxu/followers",
            "following_url": "https://api.github.com/users/faxu/following{/other_user}",
            "gists_url": "https://api.github.com/users/faxu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/faxu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/faxu/subscriptions",
            "organizations_url": "https://api.github.com/users/faxu/orgs",
            "repos_url": "https://api.github.com/users/faxu/repos",
            "events_url": "https://api.github.com/users/faxu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/faxu/received_events",
            "type": "User",
            "site_admin": true
        },
        "created_at": "2023-03-17T18:08:05Z",
        "updated_at": "2023-03-17T18:08:05Z",
        "author_association": "MEMBER",
        "body": "Hi @pauldog, please take a look at our [Code of Conduct](https://opensource.microsoft.com/codeofconduct/), which outlines our expectations for Microsoft open source community engagement.\r\n\r\nWe do our best to monitor and support community product feedback, and we expect community members to use respectful language when discussing issues. ",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474220948/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474265023",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15080#issuecomment-1474265023",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15080",
        "id": 1474265023,
        "node_id": "IC_kwDOCVq1mM5X33-_",
        "user": {
            "login": "pranavsharma",
            "id": 2732907,
            "node_id": "MDQ6VXNlcjI3MzI5MDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2732907?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavsharma",
            "html_url": "https://github.com/pranavsharma",
            "followers_url": "https://api.github.com/users/pranavsharma/followers",
            "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavsharma/orgs",
            "repos_url": "https://api.github.com/users/pranavsharma/repos",
            "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavsharma/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-17T18:46:11Z",
        "updated_at": "2023-03-17T18:46:11Z",
        "author_association": "MEMBER",
        "body": "@pauldog If memory is a concern you can solve it the following way.\r\n1. Create a session with the model by setting the optimized_file_path to serialize the optimized model file.\r\n2. Externalize all weights from the optimized model.\r\n3. Create OrtValues for each of the weights.\r\n4. Feed them to ORT using [this API](https://github.com/microsoft/onnxruntime/blob/c6074f3a4b09cb44b46ca4422717deb067015bfa/include/onnxruntime/core/session/onnxruntime_c_api.h#L2440-L2441). Even though this API was originally developed to share weights between multiple models (sessions), it can still be used with a single session. It'll ensure the weights are allocated **only once** (by the user). Here's a [test](https://github.com/microsoft/onnxruntime/blob/59dfcfdce712a5d527c7b986fc41a8e4e14c702a/onnxruntime/test/shared_lib/test_inference.cc#L2261) that shows its usage.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474265023/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474289821",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15080#issuecomment-1474289821",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15080",
        "id": 1474289821,
        "node_id": "IC_kwDOCVq1mM5X3-Cd",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-03-17T19:09:34Z",
        "updated_at": "2023-03-17T22:43:05Z",
        "author_association": "NONE",
        "body": "> @pauldog If memory is a concern you can solve it the following way.\r\n> \r\n> 1. Create a session with the model by setting the optimized_file_path to serialize the optimized model file.\r\n> 2. Externalize all weights from the optimized model.\r\n> 3. Create OrtValues for each of the weights.\r\n> 4. Feed them to ORT using [this API](https://github.com/microsoft/onnxruntime/blob/c6074f3a4b09cb44b46ca4422717deb067015bfa/include/onnxruntime/core/session/onnxruntime_c_api.h#L2440-L2441). Even though this API was originally developed to share weights between multiple models (sessions), it can still be used with a single session. It'll ensure the weights are allocated **only once** (by the user). Here's a [test](https://github.com/microsoft/onnxruntime/blob/59dfcfdce712a5d527c7b986fc41a8e4e14c702a/onnxruntime/test/shared_lib/test_inference.cc#L2261) that shows its usage.\r\n\r\nThanks. I'll give it a go!ðŸ˜ƒ It looks ~~a little~~ very tricky but I'll give it a try.\r\nSorry if my previous words offended. ðŸ˜”\r\n\r\n\r\nI assume [this](https://github.com/microsoft/onnxruntime/blob/main/csharp/test/Microsoft.ML.OnnxRuntime.Tests.Common/InferenceTest.cs) is the c# version.\r\n\r\nWill all this really load the model with less memory? If so, it would be great as a tutorial as I'm sure lots of people would find it useful.\r\n\r\nThis is really tricky :( I'm stuck on \"Externalize all weights\"\r\n\r\n\r\n\r\nOn second thoughts I don't think this is the problem. Since the spike only occurs when I have DirectML mode not CPU mode.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1474289821/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499935230",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15080#issuecomment-1499935230",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15080",
        "id": 1499935230,
        "node_id": "IC_kwDOCVq1mM5ZZzH-",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-07T04:58:20Z",
        "updated_at": "2023-04-07T04:58:20Z",
        "author_association": "NONE",
        "body": "> @pauldog If memory is a concern you can solve it the following way.\r\n> \r\n> 1. Create a session with the model by setting the optimized_file_path to serialize the optimized model file.\r\n> 2. Externalize all weights from the optimized model.\r\n> 3. Create OrtValues for each of the weights.\r\n> 4. Feed them to ORT using [this API](https://github.com/microsoft/onnxruntime/blob/c6074f3a4b09cb44b46ca4422717deb067015bfa/include/onnxruntime/core/session/onnxruntime_c_api.h#L2440-L2441). Even though this API was originally developed to share weights between multiple models (sessions), it can still be used with a single session. It'll ensure the weights are allocated **only once** (by the user). Here's a [test](https://github.com/microsoft/onnxruntime/blob/59dfcfdce712a5d527c7b986fc41a8e4e14c702a/onnxruntime/test/shared_lib/test_inference.cc#L2261) that shows its usage.\r\n\r\nHi can you clarify what you mean by \"externalize all weights\"? I'm not sure I understand thanks.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1499935230/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]