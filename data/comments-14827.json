[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1445189754",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14827#issuecomment-1445189754",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14827",
        "id": 1445189754,
        "node_id": "IC_kwDOCVq1mM5WI9h6",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-25T19:41:02Z",
        "updated_at": "2023-02-25T19:41:02Z",
        "author_association": "MEMBER",
        "body": "Try nightly package: https://aiinfra.visualstudio.com/PublicPackages/_artifacts/feed/ORT-Nightly/PyPI/ort-nightly-gpu/.\r\nI think the issue is fixed in main branch.\r\n\r\nBTW, you can try our stable diffusion optimizations:\r\nhttps://github.com/microsoft/onnxruntime/blob/780054900994b704dc37c9f1a4f7874538bfc11e/onnxruntime/python/tools/transformers/models/stable_diffusion/README.md",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1445189754/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1445223392",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14827#issuecomment-1445223392",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14827",
        "id": 1445223392,
        "node_id": "IC_kwDOCVq1mM5WJFvg",
        "user": {
            "login": "ssube",
            "id": 451959,
            "node_id": "MDQ6VXNlcjQ1MTk1OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/451959?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ssube",
            "html_url": "https://github.com/ssube",
            "followers_url": "https://api.github.com/users/ssube/followers",
            "following_url": "https://api.github.com/users/ssube/following{/other_user}",
            "gists_url": "https://api.github.com/users/ssube/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ssube/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ssube/subscriptions",
            "organizations_url": "https://api.github.com/users/ssube/orgs",
            "repos_url": "https://api.github.com/users/ssube/repos",
            "events_url": "https://api.github.com/users/ssube/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ssube/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-25T23:03:40Z",
        "updated_at": "2023-02-25T23:13:22Z",
        "author_association": "NONE",
        "body": "I'm hoping to eventually use all of the optimizations that ORT is providing, they look pretty helpful.\r\n\r\nFor now, I'm still get the same error with the nightly package and `op_block_list` in my repro script:\r\n\r\n```\r\nONNX load succeeded for optimized model\r\nORT load failed for optimized model\r\nTraceback (most recent call last):\r\n  File \"/home/ssube/onnx-repro/fp16-repro.py\", line 41, in <module>\r\n    sess = InferenceSession(opt_file, providers=[\"CPUExecutionProvider\"])\r\n  File \"/home/ssube/onnx-repro/ort_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 366, in __init__\r\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n  File \"/home/ssube/onnx-repro/ort_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 403, in _create_inference_session\r\n    sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidGraph: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from ./optimized.onnx failed:This is an invalid model. Type Error: Type 'tensor(float16)' of input parameter (onnx::Resize_967) of operator (Resize) in node (Resize_340) is invalid.\r\n\r\n(ort_env) ssube@compute-infer-1:~/onnx-repro$ pip3 list --local | grep -e onnx -e ort -e torch\r\nonnx                     1.13.1\r\nort-nightly-gpu          1.15.0.dev20230222003\r\ntorch                    1.13.1\r\ntorchaudio               0.13.1\r\ntorchvision              0.14.1\r\n```\r\n\r\nwith\r\n\r\n```python\r\noptimized = convert_float_to_float16(\r\n    model, \r\n    keep_io_types=False, \r\n    force_fp16_initializers=False, \r\n    disable_shape_infer=True, \r\n    op_block_list=[\r\n        \"RandomNormalLike\",\r\n        \"Resize\",\r\n    ]\r\n)\r\n```\r\n\r\nbased on https://github.com/microsoft/onnxruntime/blob/780054900994b704dc37c9f1a4f7874538bfc11e/onnxruntime/python/tools/transformers/models/stable_diffusion/README.md#optimize-onnx-pipeline and https://github.com/microsoft/onnxruntime/blob/e097e4e93c599d2a81346d6da5979ea5068d78e6/onnxruntime/python/tools/transformers/models/stable_diffusion/optimize_pipeline.py#L151\r\n\r\nThe optimize command shown in the docs runs to completion, and loads on the CUDA provider but not the CPU provider:\r\n\r\n```shell\r\n> python3 -m onnxruntime.transformers.models.stable_diffusion.optimize_pipeline -i ~/onnx-web/models/stable-diffusion-onnx-v1-5 -o ./sd-v1-5-fp16 --float16\r\n\r\n...\r\noptimize_sd_pipeline: Convert unet to float16 ...                                                                                                                                                \r\nget_operator_statistics: Operators:{'Constant': 192, 'Transpose': 294, 'MatMul': 112, 'Shape': 66, 'Reshape': 64, 'Gather': 65, 'NhwcConv': 98, 'Unsqueeze': 158, 'GroupNorm': 61, 'Concat': 47, \r\n'ConstantOfShape': 1, 'Mul': 20, 'Equal': 1, 'Where': 1, 'Expand': 1, 'Sin': 1, 'Cos': 1, 'Slice': 2, 'Gemm': 24, 'Sigmoid': 2, 'Add': 60, 'LayerNormalization': 16, 'MultiHeadAttention': 32, 'S\r\nkipLayerNormalization': 32, 'Resize': 3, 'BiasSplitGelu': 16, 'BiasAdd': 16, 'Cast': 3}\r\nget_fused_operator_statistics: Optimized operators:{'Attention': 0, 'MultiHeadAttention': 32, 'LayerNormalization': 16, 'SkipLayerNormalization': 32, 'BiasSplitGelu': 16, 'GroupNorm': 61, 'Nhwc\r\nConv': 98}\r\n  save_model_to_file: Sort graphs in topological order\r\n  save_model_to_file: Model saved to sd-v1-5-fp16/unet/model.onnx\r\noptimize_sd_pipeline: unet is optimized\r\n...\r\n\r\n> python3\r\n>>> import onnxruntime\r\n>>> sess = onnxruntime.InferenceSession(\"./sd-v1-5-fp16/vae_decoder/model.onnx\", providers=['CPUExecutionProvider'])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ssube/onnx-repro/ort_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 366, in __init__\r\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n  File \"/home/ssube/onnx-repro/ort_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 414, in _create_inference_session\r\n    sess.initialize_session(providers, provider_options, disabled_optimizers)\r\nonnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Failed to find kernel for NhwcConv(1) (node NhwcConv_0-/post_quant_conv/Conv). Kernel not found\r\n\r\n>>> sess = onnxruntime.InferenceSession(\"./sd-v1-5-fp16/vae_decoder/model.onnx\", providers=['CUDAExecutionProvider'])\r\n2023-02-25 23:11:07.349772352 [W:onnxruntime:, session_state.cc:1136 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\r\n2023-02-25 23:11:07.349789765 [W:onnxruntime:, session_state.cc:1138 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\r\n>>> \r\n```\r\n\r\nThe fp16 optimization is not really meant for CPU, so that's probably ok, but using `convert_float_to_float16` on its own is still causing problems.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1445223392/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1445422954",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14827#issuecomment-1445422954",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14827",
        "id": 1445422954,
        "node_id": "IC_kwDOCVq1mM5WJ2dq",
        "user": {
            "login": "tianleiwu",
            "id": 30328909,
            "node_id": "MDQ6VXNlcjMwMzI4OTA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/30328909?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tianleiwu",
            "html_url": "https://github.com/tianleiwu",
            "followers_url": "https://api.github.com/users/tianleiwu/followers",
            "following_url": "https://api.github.com/users/tianleiwu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tianleiwu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tianleiwu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tianleiwu/subscriptions",
            "organizations_url": "https://api.github.com/users/tianleiwu/orgs",
            "repos_url": "https://api.github.com/users/tianleiwu/repos",
            "events_url": "https://api.github.com/users/tianleiwu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tianleiwu/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-26T18:08:29Z",
        "updated_at": "2023-02-26T18:31:39Z",
        "author_association": "MEMBER",
        "body": "@ssube, I can reproduce the issue. Walkaround is either change `disable_shape_infer=False` or `op_block_list=['Identity']` during calling convert_float_to_float16 for this model. It seems that after disabling shape inference, data type information is not completed for the conversion. We will improve type inference later.\r\n\r\nYou are right. The optimization for stable diffusion works for CUDA only.\r\n\r\nCPUExecutionProvider does not support float16 operator. If an operator has float32 implementation in CPUExecutionProvider, ORT actually will add Cast to convert tensors back to float32 so that it can run the operator in float32. That might cause float16 model to be slower than float32 in CPUExecutionProvider.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1445422954/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1447279275",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/14827#issuecomment-1447279275",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/14827",
        "id": 1447279275,
        "node_id": "IC_kwDOCVq1mM5WQ7qr",
        "user": {
            "login": "ssube",
            "id": 451959,
            "node_id": "MDQ6VXNlcjQ1MTk1OQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/451959?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ssube",
            "html_url": "https://github.com/ssube",
            "followers_url": "https://api.github.com/users/ssube/followers",
            "following_url": "https://api.github.com/users/ssube/following{/other_user}",
            "gists_url": "https://api.github.com/users/ssube/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ssube/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ssube/subscriptions",
            "organizations_url": "https://api.github.com/users/ssube/orgs",
            "repos_url": "https://api.github.com/users/ssube/repos",
            "events_url": "https://api.github.com/users/ssube/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ssube/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-02-27T23:29:18Z",
        "updated_at": "2023-02-27T23:29:18Z",
        "author_association": "NONE",
        "body": "Thanks for testing that. I had tried setting `disable_shape_infer=False`, but that fails on very large models (> 2GB), so I set it back to `True` at some point. I can reproduce your findings as well, that seems like a viable workaround if I call `onnx.shape_inference.infer_shapes_path` first with the model path rather than the loaded proto.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1447279275/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]