[
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1503674350",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15456#issuecomment-1503674350",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15456",
        "id": 1503674350,
        "node_id": "IC_kwDOCVq1mM5ZoD_u",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-11T16:00:23Z",
        "updated_at": "2023-04-11T16:23:38Z",
        "author_association": "NONE",
        "body": "Actually I have thought of a workflow for this (not sure if it will work)\r\n\r\n1. Use quantize_dynamic(..) or quantize_static(..) on an onnx file.\r\n2. Make sure both the original onnx and the quantized onnx have external weights\r\n3. The weight files should have the same names in both cases except for some *.weight renamed as *.weight_quantized\r\n4. Load in the *.weight files and the *.weight_quantized to work out the scale and offset values (probably an easier way!)\r\n5. Store the scale/offsets in an XML file.\r\n6. Create weight-less versions of the original onnx and the quantized onnx files (see[ this script ](https://github.com/pauldog/FastOnnxLoader) for example)\r\n7. At run time the user can choose either to run the model on the GPU as int8 or choose to dequantize all the weights before inference using our XML file and run on the GPU as float16 at optimum speeds.\r\n\r\n**Advantages**\r\nNow instead of a 10GB float16 onnx file you have 5GB of files. Depending on the users' GPU, if they have lots of VRAM they could choose to run this fast as float16 or slower as int8. Or on the CPU faster as int8. All from that single set of 5GB files.\r\nThus this single app could run on a range on GPUs.\r\n\r\n**Further Improvements**\r\nThis is an example of lossy compression. It would still be nice to include some kind of lossless compression. I think you can get a 20% size improvement just by zipping the quantized weight files. Because they are not entirely random but the values cluster around zero. (Mind you the fact that you can do this compression probably means the quantization method is not the optimum - a non-uniform quantization may be better)\r\n\r\n**Questions**\r\nIs there an easier way to get the scale and offset of the quantized weights?\r\nIs there a fast function something like: `Float16[] Dequantize(uint8[] data, float offset, float scale) `? A guess this could be done in a shader.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1503674350/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1503896762",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15456#issuecomment-1503896762",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15456",
        "id": 1503896762,
        "node_id": "IC_kwDOCVq1mM5Zo6S6",
        "user": {
            "login": "wschin",
            "id": 3524474,
            "node_id": "MDQ6VXNlcjM1MjQ0NzQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3524474?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wschin",
            "html_url": "https://github.com/wschin",
            "followers_url": "https://api.github.com/users/wschin/followers",
            "following_url": "https://api.github.com/users/wschin/following{/other_user}",
            "gists_url": "https://api.github.com/users/wschin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wschin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wschin/subscriptions",
            "organizations_url": "https://api.github.com/users/wschin/orgs",
            "repos_url": "https://api.github.com/users/wschin/repos",
            "events_url": "https://api.github.com/users/wschin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wschin/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-11T18:37:54Z",
        "updated_at": "2023-04-11T18:37:54Z",
        "author_association": "MEMBER",
        "body": "`lossless compression` for neural network is almost impossible and a very difficult research direction. If we print out the digits of those model weights, we will see they don't have obvious patterns and therefore bad compression ratio is expected. On the other hand, the magnitudes of weights are much more concentrated. That's why we can do int8 or float16 compression without losing meaningful.",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1503896762/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    },
    {
        "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1504062138",
        "html_url": "https://github.com/microsoft/onnxruntime/issues/15456#issuecomment-1504062138",
        "issue_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/15456",
        "id": 1504062138,
        "node_id": "IC_kwDOCVq1mM5Zpiq6",
        "user": {
            "login": "pauldog",
            "id": 33497043,
            "node_id": "MDQ6VXNlcjMzNDk3MDQz",
            "avatar_url": "https://avatars.githubusercontent.com/u/33497043?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pauldog",
            "html_url": "https://github.com/pauldog",
            "followers_url": "https://api.github.com/users/pauldog/followers",
            "following_url": "https://api.github.com/users/pauldog/following{/other_user}",
            "gists_url": "https://api.github.com/users/pauldog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pauldog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pauldog/subscriptions",
            "organizations_url": "https://api.github.com/users/pauldog/orgs",
            "repos_url": "https://api.github.com/users/pauldog/repos",
            "events_url": "https://api.github.com/users/pauldog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pauldog/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2023-04-11T20:41:12Z",
        "updated_at": "2023-04-11T21:24:42Z",
        "author_association": "NONE",
        "body": "> `lossless compression` for neural network is almost impossible and a very difficult research direction. If we print out the digits of those model weights, we will see they don't have obvious patterns and therefore bad compression ratio is expected. On the other hand, the magnitudes of weights are much more concentrated. That's why we can do int8 or float16 compression without losing meaningful.\r\n\r\nThat is true but I'm more talking about compression for quantized models. Since the int8 values aren't random, they are grouped in a normal distribution around zero if they are uniformly quantized. And also contain a lot of zeros. A simple experiment shows that you can losslessly compress a quantized onnx file. What we can do is this pipeline:\r\n\r\nfloat16 model --> (lossy compress) --> int8 model --> (lossless compress) --> zipped model\r\n\r\nThis is my experiment with the carebras-111 large language model:\r\n\r\nFor example I take my int8-quantized ONNX file (147MB) and zip it it becomes 111MB. That is a **32% space saving.** Not so shabby. I'm not sure if there is a better or faster compression than ZIP that would work for this case. There probably is a better algorithm that could take advantage of reordering rows in the weight matrices.\r\n\r\nWhen I zip up a float16 onnx file of 290MB I get a file of 262MB which is only 10% space saving. So in this case not much is saved.\r\n\r\nFor lossy compression, I have already implemented a method which simply uses the quantized weights (from a cloned and quantized model) and decompresses them back to float16 to run on the GPU. This gives a **50%** space saving while keeping the speed of float16 on the GPU. If I also zip the quantized weights this gives a total of **65%** space saving. Leaving files of about 1/3 the size sitting on the hard drive. Very convenient if dealing with a lot of neural networks. 😀\r\n(In fact the int8 shouldn't run slower than float16 but that's another issue!)\r\n\r\nMy current bottlenecks are finding a fast float[]-->float16[] method. Which I might have to delegate to the GPU. And secondly finding a good lossless-compression algorithm. (But I might skip this part) as 50% is still quite good.\r\n\r\nOnly problem with my method is depending on how you save the onnx file it can give the weights different names... ☹\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/microsoft/onnxruntime/issues/comments/1504062138/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "performed_via_github_app": null
    }
]